#!/usr/bin/env python3
"""
í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- articles_cleaned í…Œì´ë¸”ì—ì„œ ê¸°ì‚¬ ì¡°íšŒ
- í…ìŠ¤íŠ¸ ì •ì œ + ì •ê·œí™” ìˆ˜í–‰
- title_cleaned, content_cleaned ì»¬ëŸ¼ì— ì €ì¥
"""

import sys
import os
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.supabase_manager import SupabaseManager
from preprocessing.modules.text_processor import TextProcessor

def main():
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    
    # Supabase ì—°ê²°
    supabase_manager = SupabaseManager()
    if not supabase_manager.client:
        print("âŒ Supabase ì—°ê²° ì‹¤íŒ¨")
        return
    
    # í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    text_processor = TextProcessor()
    
    try:
        # articles í…Œì´ë¸”ì—ì„œ articles_cleanedì— ì—†ëŠ” ê¸°ì‚¬ë“¤ ì¡°íšŒ
        print("ğŸ“¡ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ ì¤‘...")
        
        # ë¨¼ì € ì´ë¯¸ ì „ì²˜ë¦¬ëœ ê¸°ì‚¬ IDë“¤ì„ ê°€ì ¸ì˜´
        processed_result = supabase_manager.client.table('articles_cleaned').select('article_id').execute()
        processed_ids = set(item['article_id'] for item in processed_result.data)
        
        # articles í…Œì´ë¸”ì—ì„œ ëª¨ë“  ê¸°ì‚¬ ì¡°íšŒ (ë‚ ì§œ í•„í„° ì œê±°)
        result = supabase_manager.client.table('articles').select(
            'id, title, content, url, published_at, media_id'
        ).execute()
        
        # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ê¸°ì‚¬ ì œì™¸
        if result.data:
            result.data = [article for article in result.data if article['id'] not in processed_ids]
        
        articles = result.data if result else []
        print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
        
        if not articles:
            print("ğŸ“ ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìˆ˜í–‰
        print("ğŸ”§ í…ìŠ¤íŠ¸ ì •ì œ ë° ì •ê·œí™” ì‹œì‘...")
        processed_count = 0
        failed_count = 0
        
        for article in articles:
            try:
                article_id = article['id']
                title = article.get('title', '')
                content = article.get('content', '')
                url = article.get('url', '')
                
                # ê°„ë‹¨í•œ ì–¸ë¡ ì‚¬ ì‹ë³„
                media_outlet = 'unknown'
                if 'chosun.com' in url:
                    media_outlet = 'chosun'
                elif 'hani.co.kr' in url:
                    media_outlet = 'hani'
                elif 'yonhapnews.co.kr' in url:
                    media_outlet = 'yonhap'
                elif 'donga.com' in url:
                    media_outlet = 'donga'
                elif 'joongang.co.kr' in url:
                    media_outlet = 'joongang'
                elif 'khan.co.kr' in url:
                    media_outlet = 'khan'
                elif 'ohmynews.com' in url:
                    media_outlet = 'ohmynews'
                elif 'newsis.com' in url:
                    media_outlet = 'newsis'
                
                # ì œëª© ì •ì œ
                cleaned_title, title_patterns = text_processor.clean_title(title, media_outlet)
                
                # ë³¸ë¬¸ ì •ì œ
                cleaned_content, content_patterns = text_processor.clean_content(content, media_outlet)
                
                # ì œëª© ì •ê·œí™”
                title_result = text_processor.normalize_text(cleaned_title)
                final_title = title_result.normalized_text
                
                # ë³¸ë¬¸ ì •ê·œí™”
                content_result = text_processor.normalize_text(cleaned_content)
                final_content = content_result.normalized_text
                
                # articles_cleaned í…Œì´ë¸”ì— ìƒˆ ë ˆì½”ë“œ ìƒì„± (ì œëª© + ë¦¬ë“œë§Œ)
                data = {
                    'article_id': article_id,
                    'merged_content': f"ì œëª©: {final_title}\n\në¦¬ë“œ: {final_content}",
                    'media_id': article.get('media_id'),
                    'published_at': article.get('published_at')
                }
                
                insert_result = supabase_manager.client.table('articles_cleaned').insert(data).execute()
                
                if insert_result.data:
                    # articles í…Œì´ë¸”ì—ëŠ” preprocessing_status ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”
                    pass
                
                if insert_result.data:
                    processed_count += 1
                    if processed_count % 100 == 0:
                        print(f"âœ… {processed_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ...")
                else:
                    failed_count += 1
                    
            except Exception as e:
                print(f"âŒ ê¸°ì‚¬ {article.get('id', 'Unknown')} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                failed_count += 1
                continue
        
        print(f"\nğŸ“Š í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  ì„±ê³µ: {processed_count}ê°œ")
        print(f"  ì‹¤íŒ¨: {failed_count}ê°œ")
        print(f"  ì´ ì²˜ë¦¬: {len(articles)}ê°œ")
        
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    main()
