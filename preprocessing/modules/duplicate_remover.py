#!/usr/bin/env python3
"""
ì¤‘ë³µ ì œê±° ë©”ì¸ ëª¨ë“ˆ
- Supabaseì—ì„œ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ
- ì œëª©ê³¼ ë³¸ë¬¸ ì¤‘ë³µ ì œê±°
- ìµœì‹  ê¸°ì‚¬ë§Œ ìœ ì§€í•˜ì—¬ articles_cleanedì— ì €ì¥
"""

import sys
import os
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils.supabase_manager import SupabaseManager
from preprocessing.utils.similarity_calculator import SimilarityCalculator, SimilarityResult
# ì‚­ì œëœ ëª¨ë“ˆë“¤ - í†µí•© ëª¨ë“ˆë¡œ ëŒ€ì²´ë¨

@dataclass
class PreprocessingResult:
    """ì „ì²˜ë¦¬ ê²°ê³¼ (ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§)"""
    total_articles: int
    title_duplicates_removed: int
    content_duplicates_removed: int
    no_content_removed: int
    news_agency_removed: int
    short_articles_removed: int
    final_articles: int
    processing_time: float
    success: bool
    error_message: Optional[str] = None

class IntegratedPreprocessor:
    """í†µí•© ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§)"""
    
    def __init__(self, title_threshold: float = 1.0, content_threshold: float = 0.95, 
                 min_sentences: int = 3, min_content_length: int = 100, 
                 max_lead_sentences: int = 3, date_filter=None):
        """
        ì´ˆê¸°í™”
        
        Args:
            title_threshold: ì œëª© ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 1.0 = ì •í™•í•œ ë§¤ì¹­)
            content_threshold: ë³¸ë¬¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.95)
            min_sentences: ìµœì†Œ ë¬¸ì¥ ìˆ˜ (ê¸°ë³¸ê°’: 3)
            min_content_length: ìµœì†Œ ë³¸ë¬¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 100ì)
            max_lead_sentences: ë¦¬ë“œë¬¸ ìµœëŒ€ ë¬¸ì¥ ìˆ˜ (ê¸°ë³¸ê°’: 3)
        """
        self.supabase_manager = SupabaseManager()
        self.similarity_calculator = SimilarityCalculator(title_threshold, content_threshold)
        self.date_filter = date_filter
        
    def fetch_articles_from_supabase(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Supabaseì—ì„œ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í˜ì´ì§€ë„¤ì´ì…˜)
        
        Args:
            limit: ì¡°íšŒí•  ê¸°ì‚¬ ìˆ˜ ì œí•œ (Noneì´ë©´ ëª¨ë“  ê¸°ì‚¬)
            
        Returns:
            ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        if not self.supabase_manager.client:
            raise Exception("Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # ë¨¼ì € ì´ë¯¸ ì „ì²˜ë¦¬ëœ ê¸°ì‚¬ IDë“¤ì„ ê°€ì ¸ì˜´
            print("ğŸ” ì´ë¯¸ ì „ì²˜ë¦¬ëœ ê¸°ì‚¬ í™•ì¸ ì¤‘...")
            processed_result = self.supabase_manager.client.table('articles_cleaned').select('article_id').execute()
            processed_ids = set(item['article_id'] for item in processed_result.data)
            print(f"âœ… {len(processed_ids)}ê°œ ê¸°ì‚¬ê°€ ì´ë¯¸ ì „ì²˜ë¦¬ë¨")
            
            all_articles = []
            page_size = 500  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ í˜ì´ì§€ í¬ê¸° ê°ì†Œ
            offset = 0
            total_processed = 0
            total_filtered = 0
            
            while True:
                try:
                    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
                    query = self.supabase_manager.client.table('articles').select(
                        'id, title, content, published_at, url, media_id'
                    ).order('published_at', desc=True)
                    
                    # ë‚ ì§œ í•„í„°ë§ ì ìš© (KST 9ì›” 8ì¼ â†’ UTC 9ì›” 7ì¼ 15:00 ~ 9ì›” 8ì¼ 14:59)
                    from datetime import datetime, timedelta, timezone
                    
                    # KST 9ì›” 8ì¼ â†’ UTC ë³€í™˜
                    kst_yesterday = datetime(2025, 9, 8)
                    utc_start = kst_yesterday.replace(hour=0, minute=0, second=0, tzinfo=timezone(timedelta(hours=9))).astimezone(timezone.utc)
                    utc_end = kst_yesterday.replace(hour=23, minute=59, second=59, tzinfo=timezone(timedelta(hours=9))).astimezone(timezone.utc)
                    
                    # UTC ë¬¸ìì—´ë¡œ ë³€í™˜
                    utc_start_str = utc_start.strftime('%Y-%m-%dT%H:%M:%SZ')
                    utc_end_str = utc_end.strftime('%Y-%m-%dT%H:%M:%SZ')
                    
                    # ë‚ ì§œ í•„í„° ì ìš©
                    query = query.gte('published_at', utc_start_str).lt('published_at', utc_end_str)
                    print(f"ğŸ“… ë‚ ì§œ í•„í„° ì ìš©: {utc_start_str} ~ {utc_end_str} (KST 9ì›” 8ì¼)")
                    
                    
                    query = query.range(offset, offset + page_size - 1)
                    
                    result = query.execute()
                    
                    if not result.data:
                        break  # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    
                    # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ê¸°ì‚¬ ì œì™¸
                    filtered_articles = [
                        article for article in result.data 
                        if article['id'] not in processed_ids
                    ]
                    
                    current_batch_size = len(result.data)
                    filtered_count = len(filtered_articles)
                    total_filtered += filtered_count
                    
                    all_articles.extend(filtered_articles)
                    total_processed += current_batch_size
                    
                    print(f"ğŸ“„ {current_batch_size}ê°œ ê¸°ì‚¬ ì¡°íšŒ, {filtered_count}ê°œ ì‹ ê·œ ê¸°ì‚¬ (ì´ {total_processed}ê°œ ì¡°íšŒ, {total_filtered}ê°œ ì‹ ê·œ)")
                    
                    # limitì´ ì„¤ì •ë˜ì–´ ìˆê³  ë„ë‹¬í–ˆìœ¼ë©´ ì¤‘ë‹¨
                    if limit and total_filtered >= limit:
                        all_articles = all_articles[:limit]
                        break
                    
                    # ë§ˆì§€ë§‰ í˜ì´ì§€ì¸ ê²½ìš° (ì¡°íšŒëœ ë°ì´í„°ê°€ page_sizeë³´ë‹¤ ì ìœ¼ë©´)
                    if current_batch_size < page_size:
                        break
                    
                    offset += page_size
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (ëŒ€ëµì )
                    if total_processed % 5000 == 0:
                        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬: {total_processed}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ë¨")
                    
                except Exception as page_error:
                    print(f"âš ï¸  í˜ì´ì§€ ì¡°íšŒ ì˜¤ë¥˜ (offset {offset}): {page_error}")
                    # í˜ì´ì§€ ì˜¤ë¥˜ ì‹œ ë‹¤ìŒ í˜ì´ì§€ë¡œ ê³„ì† ì§„í–‰
                    offset += page_size
                    continue
            
            print(f"âœ… ì´ {len(all_articles)}ê°œ ì‹ ê·œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
            return all_articles
            
        except Exception as e:
            raise Exception(f"ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
    
    def remove_duplicate_titles(self, articles: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
        """
        ì œëª© ì¤‘ë³µ ì œê±° (ê°€ì¥ ìµœì‹  ê²ƒë§Œ ìœ ì§€)
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ì¤‘ë³µ ìˆ˜)
        """
        if not articles:
            return articles, 0
        
        # published_at ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_articles = sorted(
            articles, 
            key=lambda x: x.get('published_at', ''), 
            reverse=True
        )
        
        # ì œëª©ë³„ë¡œ ê·¸ë£¹í™”
        title_groups = {}
        for article in sorted_articles:
            title = article.get('title', '').strip()
            if not title:
                continue
                
            if title not in title_groups:
                title_groups[title] = []
            title_groups[title].append(article)
        
        # ê° ê·¸ë£¹ì—ì„œ ì²« ë²ˆì§¸(ìµœì‹ ) ê¸°ì‚¬ë§Œ ìœ ì§€
        unique_articles = []
        duplicates_removed = 0
        
        for title, group in title_groups.items():
            if len(group) > 1:
                duplicates_removed += len(group) - 1
            unique_articles.append(group[0])  # ìµœì‹  ê¸°ì‚¬ë§Œ ìœ ì§€
        
        return unique_articles, duplicates_removed
    
    def remove_duplicate_contents(self, articles: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
        """
        ë³¸ë¬¸ ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ 0.95 ê¸°ì¤€, ê°€ì¥ ìµœì‹  ê²ƒë§Œ ìœ ì§€)
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ì¤‘ë³µ ìˆ˜)
        """
        if not articles:
            return articles, 0
        
        # published_at ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_articles = sorted(
            articles, 
            key=lambda x: x.get('published_at', ''), 
            reverse=True
        )
        
        # ì¤‘ë³µ ë³¸ë¬¸ ì°¾ê¸°
        content_duplicates = self.similarity_calculator.find_duplicate_contents(sorted_articles)
        
        # ì œê±°í•  ê¸°ì‚¬ ì¸ë±ìŠ¤ ì§‘í•©
        indices_to_remove = set()
        
        for i, j, similarity_result in content_duplicates:
            # ë” ìµœì‹  ê¸°ì‚¬(ì¸ë±ìŠ¤ê°€ ì‘ì€ ê²ƒ)ë¥¼ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì œê±°
            indices_to_remove.add(j)  # jê°€ ë” ì˜¤ë˜ëœ ê¸°ì‚¬
        
        # ì œê±°í•  ì¸ë±ìŠ¤ê°€ ì•„ë‹Œ ê¸°ì‚¬ë§Œ ìœ ì§€
        unique_articles = [
            article for idx, article in enumerate(sorted_articles)
            if idx not in indices_to_remove
        ]
        
        return unique_articles, len(indices_to_remove)
    
    def remove_hybrid_duplicate_contents(self, articles: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ë³¸ë¬¸ ì¤‘ë³µ ì œê±° (O(n) ì‹œê°„ ë³µì¡ë„)
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ì¤‘ë³µ ìˆ˜)
        """
        if not articles:
            return articles, 0
        
        # published_at ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_articles = sorted(
            articles, 
            key=lambda x: x.get('published_at', ''), 
            reverse=True
        )
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì¤‘ë³µ ì°¾ê¸° (O(n) ë³µì¡ë„)
        content_duplicates = self.similarity_calculator.find_hybrid_duplicates(sorted_articles)
        
        # ì œê±°í•  ê¸°ì‚¬ ì¸ë±ìŠ¤ ì§‘í•©
        indices_to_remove = set()
        
        for i, j, similarity_result in content_duplicates:
            # ë” ìµœì‹  ê¸°ì‚¬(ì¸ë±ìŠ¤ê°€ ì‘ì€ ê²ƒ)ë¥¼ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì œê±°
            indices_to_remove.add(j)  # jê°€ ë” ì˜¤ë˜ëœ ê¸°ì‚¬
        
        # ì¤‘ë³µì´ ì•„ë‹Œ ê¸°ì‚¬ë“¤ë§Œ ìœ ì§€
        unique_articles = [
            article for idx, article in enumerate(sorted_articles) 
            if idx not in indices_to_remove
        ]
        
        duplicates_removed = len(indices_to_remove)
        return unique_articles, duplicates_removed
    
    def save_to_articles_cleaned(self, articles: List[Dict[str, Any]]) -> bool:
        """
        ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ë¥¼ articles_cleaned í…Œì´ë¸”ì— ì €ì¥
        
        Args:
            articles: ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        if not self.supabase_manager.client:
            return False
        
        try:
            # articles_cleaned í…Œì´ë¸”ì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
            cleaned_articles = []
            
            for article in articles:
                # ë¦¬ë“œë¬¸ ì¶”ì¶œ (ì „ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”)
                # ê°„ë‹¨í•œ ë¦¬ë“œë¬¸ ì¶”ì¶œ (ì²« 2-3ë¬¸ì¥)
                content = article.get('content', '')
                if content:
                    import re
                    sentences = re.split(r'[.!?]+', content)
                    lead_sentences = sentences[:3]  # ì²« 3ë¬¸ì¥
                    lead_paragraph = '. '.join(lead_sentences).strip()
                    if lead_paragraph and not lead_paragraph.endswith(('.', '!', '?')):
                        lead_paragraph += '.'
                else:
                    lead_paragraph = ''
                
                cleaned_article = {
                    'article_id': article['id'],
                    'title_cleaned': article.get('title', ''),
                    'lead_paragraph': lead_paragraph,
                    'preprocessing_metadata': {
                        'duplicate_removal': {
                            'processed_at': datetime.now().isoformat(),
                            'title_duplicates_removed': 0,  # ê°œë³„ ê¸°ì‚¬ì—ì„œëŠ” 0
                            'content_duplicates_removed': 0,
                            'similarity_threshold': self.similarity_calculator.content_threshold,
                            'method': 'hybrid'
                        },
                        'basic_filter': {
                            'processed_at': datetime.now().isoformat(),
                            'no_content_removed': False,
                            'news_agency_removed': False,
                            'short_article_removed': False
                        },
                        'lead_extraction': {
                            'processed_at': datetime.now().isoformat(),
                            'max_sentences': 3,
                            'lead_length': len(lead_paragraph),
                            'original_length': len(article.get('content', ''))
                        }
                    },
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }
                cleaned_articles.append(cleaned_article)
            
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ ë³€ê²½ (ë°°ì¹˜ ì €ì¥ ì˜¤ë¥˜ ë°©ì§€)
            success_count = 0
            for article in cleaned_articles:
                try:
                    result = self.supabase_manager.client.table('articles_cleaned').insert([article]).execute()
                    if result.data:
                        success_count += 1
                except Exception as e:
                    print(f"âš ï¸ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨ (ID: {article.get('article_id', 'unknown')}): {e}")
                    continue
            
            print(f"âœ… {success_count}/{len(cleaned_articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ articles_cleaned ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def process_integrated_preprocessing(self, limit: Optional[int] = None) -> PreprocessingResult:
        """
        í†µí•© ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§)
        
        Args:
            limit: ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ ì œí•œ
            
        Returns:
            í†µí•© ì „ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = datetime.now()
        
        try:
            print("ğŸš€ í†µí•© ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
            
            # 1. Supabaseì—ì„œ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ
            print("ğŸ“¡ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ ì¤‘...")
            articles = self.fetch_articles_from_supabase(limit)
            print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
            
            if not articles:
                return PreprocessingResult(
                    total_articles=0,
                    title_duplicates_removed=0,
                    content_duplicates_removed=0,
                    no_content_removed=0,
                    news_agency_removed=0,
                    short_articles_removed=0,
                    final_articles=0,
                    processing_time=0,
                    success=True
                )
            
            # 2. ì œëª© ì¤‘ë³µ ì œê±°
            print("ğŸ” ì œëª© ì¤‘ë³µ ì œê±° ì¤‘...")
            articles_after_title, title_duplicates = self.remove_duplicate_titles(articles)
            print(f"âœ… ì œëª© ì¤‘ë³µ {title_duplicates}ê°œ ì œê±° ì™„ë£Œ")
            
            # 3. í•˜ì´ë¸Œë¦¬ë“œ ë³¸ë¬¸ ì¤‘ë³µ ì œê±°
            print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ë³¸ë¬¸ ì¤‘ë³µ ì œê±° ì¤‘...")
            articles_after_content, content_duplicates = self.remove_hybrid_duplicate_contents(articles_after_title)
            print(f"âœ… ë³¸ë¬¸ ì¤‘ë³µ {content_duplicates}ê°œ ì œê±° ì™„ë£Œ")
            
            # 4. ê¸°ë³¸ í•„í„°ë§ (ë³¸ë¬¸ ì—†ëŠ” ê¸°ì‚¬, ë‰´ìŠ¤í†µì‹ ì‚¬ì—…ì, ì§§ì€ ê¸°ì‚¬ ì œê±°)
            print("ğŸ” ê¸°ë³¸ í•„í„°ë§ ì‹œì‘...")
            # ê°„ë‹¨í•œ ê¸°ë³¸ í•„í„°ë§ (ë³´ì¡´ ëª¨ë“œ)
            basic_filter_result = {
                'total_articles': len(articles_after_content),
                'no_content_removed': 0,
                'news_agency_removed': 0,
                'short_articles_removed': 0,
                'final_articles': len(articles_after_content),
                'processing_time': 0,
                'success': True
            }
            
            if not basic_filter_result['success']:
                raise Exception(f"ê¸°ë³¸ í•„í„°ë§ ì‹¤íŒ¨: {basic_filter_result.get('error_message', 'Unknown error')}")
            
            print(f"âœ… ê¸°ë³¸ í•„í„°ë§ ì™„ë£Œ: {basic_filter_result['final_articles']}ê°œ ê¸°ì‚¬ ë‚¨ìŒ")
            
            # 5. ìµœì¢… ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ í•„í„°ë§ í›„ ë‚¨ì€ ê¸°ì‚¬ë“¤)
            # basic_filter.process_basic_filteringì€ ê²°ê³¼ë§Œ ë°˜í™˜í•˜ë¯€ë¡œ, ì‹¤ì œ í•„í„°ë§ëœ ê¸°ì‚¬ë¥¼ ë‹¤ì‹œ ê°€ì ¸ì™€ì•¼ í•¨
            final_articles = self._apply_basic_filtering(articles_after_content)
            
            # 6. articles_cleanedì— ì €ì¥
            print("ğŸ’¾ articles_cleanedì— ì €ì¥ ì¤‘...")
            save_success = self.save_to_articles_cleaned(final_articles)
            
            if not save_success:
                raise Exception("articles_cleaned ì €ì¥ ì‹¤íŒ¨")
            
            print(f"âœ… {len(final_articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
            
            # 7. ê²°ê³¼ ë°˜í™˜
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return PreprocessingResult(
                total_articles=len(articles),
                title_duplicates_removed=title_duplicates,
                content_duplicates_removed=content_duplicates,
                no_content_removed=basic_filter_result['no_content_removed'],
                news_agency_removed=basic_filter_result['news_agency_removed'],
                short_articles_removed=basic_filter_result['short_articles_removed'],
                final_articles=len(final_articles),
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            error_msg = f"í†µí•© ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            
            return PreprocessingResult(
                total_articles=len(articles) if 'articles' in locals() else 0,
                title_duplicates_removed=0,
                content_duplicates_removed=0,
                no_content_removed=0,
                news_agency_removed=0,
                short_articles_removed=0,
                final_articles=0,
                processing_time=processing_time,
                success=False,
                error_message=error_msg
            )
    
    def _apply_basic_filtering(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ê¸°ë³¸ í•„í„°ë§ì„ ì ìš©í•˜ì—¬ ì‹¤ì œ í•„í„°ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í•„í„°ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        # ê°„ë‹¨í•œ ê¸°ë³¸ í•„í„°ë§ (ë³´ì¡´ ëª¨ë“œ - ëª¨ë“  ê¸°ì‚¬ ìœ ì§€)
        articles_filtered = articles
        print(f"âœ… ê¸°ë³¸ í•„í„°ë§ ì™„ë£Œ: {len(articles_filtered)}ê°œ ê¸°ì‚¬ ìœ ì§€ (ë³´ì¡´ ëª¨ë“œ)")
        
        return articles_filtered

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í†µí•© ì „ì²˜ë¦¬ê¸° ìƒì„±
    preprocessor = IntegratedPreprocessor()
    
    # í†µí•© ì „ì²˜ë¦¬ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 100ê°œë§Œ)
    result = preprocessor.process_integrated_preprocessing()  # ëª¨ë“  ê¸°ì‚¬ ì²˜ë¦¬
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š í†µí•© ì „ì²˜ë¦¬ ê²°ê³¼:")
    print(f"  ì´ ê¸°ì‚¬ ìˆ˜: {result.total_articles}")
    print(f"  ì œëª© ì¤‘ë³µ ì œê±°: {result.title_duplicates_removed}ê°œ")
    print(f"  ë³¸ë¬¸ ì¤‘ë³µ ì œê±°: {result.content_duplicates_removed}ê°œ")
    print(f"  ë³¸ë¬¸ ì—†ëŠ” ê¸°ì‚¬ ì œê±°: {result.no_content_removed}ê°œ")
    print(f"  ë‰´ìŠ¤í†µì‹ ì‚¬ì—…ì ê¸°ì‚¬ ì œê±°: {result.news_agency_removed}ê°œ")
    print(f"  ì§§ì€ ê¸°ì‚¬ ì œê±°: {result.short_articles_removed}ê°œ")
    print(f"  ìµœì¢… ê¸°ì‚¬ ìˆ˜: {result.final_articles}")
    print(f"  ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
    print(f"  ì„±ê³µ ì—¬ë¶€: {'âœ… ì„±ê³µ' if result.success else 'âŒ ì‹¤íŒ¨'}")
    
    if result.error_message:
        print(f"  ì˜¤ë¥˜ ë©”ì‹œì§€: {result.error_message}")

# ì´ì „ ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
DuplicateRemover = IntegratedPreprocessor
DuplicateRemovalResult = PreprocessingResult
