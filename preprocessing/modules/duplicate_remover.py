#!/usr/bin/env python3
"""
ì¤‘ë³µ ì œê±° ë©”ì¸ ëª¨ë“ˆ
- Supabaseì—ì„œ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ
- ì œëª©ê³¼ ë³¸ë¬¸ ì¤‘ë³µ ì œê±°
- ìµœì‹  ê¸°ì‚¬ë§Œ ìœ ì§€í•˜ì—¬ articles_cleanedì— ì €ì¥
"""

import sys
import os
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils.supabase_manager import SupabaseManager
from preprocessing.utils.similarity_calculator import SimilarityCalculator, SimilarityResult
from preprocessing.modules.basic_filter import BasicFilter

@dataclass
class PreprocessingResult:
    """ì „ì²˜ë¦¬ ê²°ê³¼ (ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§)"""
    total_articles: int
    title_duplicates_removed: int
    content_duplicates_removed: int
    no_content_removed: int
    news_agency_removed: int
    short_articles_removed: int
    final_articles: int
    processing_time: float
    success: bool
    error_message: Optional[str] = None

class IntegratedPreprocessor:
    """í†µí•© ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§)"""
    
    def __init__(self, title_threshold: float = 1.0, content_threshold: float = 0.95, 
                 min_sentences: int = 3, min_content_length: int = 100):
        """
        ì´ˆê¸°í™”
        
        Args:
            title_threshold: ì œëª© ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 1.0 = ì •í™•í•œ ë§¤ì¹­)
            content_threshold: ë³¸ë¬¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.95)
            min_sentences: ìµœì†Œ ë¬¸ì¥ ìˆ˜ (ê¸°ë³¸ê°’: 3)
            min_content_length: ìµœì†Œ ë³¸ë¬¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 100ì)
        """
        self.supabase_manager = SupabaseManager()
        self.similarity_calculator = SimilarityCalculator(title_threshold, content_threshold)
        self.basic_filter = BasicFilter(min_sentences, min_content_length)
        
    def fetch_articles_from_supabase(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Supabaseì—ì„œ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›)
        
        Args:
            limit: ì¡°íšŒí•  ê¸°ì‚¬ ìˆ˜ ì œí•œ (Noneì´ë©´ ëª¨ë“  ê¸°ì‚¬)
            
        Returns:
            ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        if not self.supabase_manager.client:
            raise Exception("Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            all_articles = []
            page_size = 1000  # Supabase í•œ ë²ˆì— ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê°œìˆ˜
            offset = 0
            
            while True:
                # í˜ì´ì§€ë³„ ì¡°íšŒ
                query = self.supabase_manager.client.table('articles').select('*').range(offset, offset + page_size - 1).order('created_at', desc=True)
                
                result = query.execute()
                
                if not result.data:
                    break  # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                
                all_articles.extend(result.data)
                print(f"ğŸ“„ {len(result.data)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ (ì´ {len(all_articles)}ê°œ)")
                
                # limitì´ ì„¤ì •ë˜ì–´ ìˆê³  ë„ë‹¬í–ˆìœ¼ë©´ ì¤‘ë‹¨
                if limit and len(all_articles) >= limit:
                    all_articles = all_articles[:limit]
                    break
                
                # ë§ˆì§€ë§‰ í˜ì´ì§€ì¸ ê²½ìš° (ì¡°íšŒëœ ë°ì´í„°ê°€ page_sizeë³´ë‹¤ ì ìœ¼ë©´)
                if len(result.data) < page_size:
                    break
                
                offset += page_size
            
            print(f"âœ… ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
            return all_articles
            
        except Exception as e:
            raise Exception(f"ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
    
    def remove_duplicate_titles(self, articles: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
        """
        ì œëª© ì¤‘ë³µ ì œê±° (ê°€ì¥ ìµœì‹  ê²ƒë§Œ ìœ ì§€)
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ì¤‘ë³µ ìˆ˜)
        """
        if not articles:
            return articles, 0
        
        # published_at ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_articles = sorted(
            articles, 
            key=lambda x: x.get('published_at', ''), 
            reverse=True
        )
        
        # ì œëª©ë³„ë¡œ ê·¸ë£¹í™”
        title_groups = {}
        for article in sorted_articles:
            title = article.get('title', '').strip()
            if not title:
                continue
                
            if title not in title_groups:
                title_groups[title] = []
            title_groups[title].append(article)
        
        # ê° ê·¸ë£¹ì—ì„œ ì²« ë²ˆì§¸(ìµœì‹ ) ê¸°ì‚¬ë§Œ ìœ ì§€
        unique_articles = []
        duplicates_removed = 0
        
        for title, group in title_groups.items():
            if len(group) > 1:
                duplicates_removed += len(group) - 1
            unique_articles.append(group[0])  # ìµœì‹  ê¸°ì‚¬ë§Œ ìœ ì§€
        
        return unique_articles, duplicates_removed
    
    def remove_duplicate_contents(self, articles: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
        """
        ë³¸ë¬¸ ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ 0.95 ê¸°ì¤€, ê°€ì¥ ìµœì‹  ê²ƒë§Œ ìœ ì§€)
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ì¤‘ë³µ ìˆ˜)
        """
        if not articles:
            return articles, 0
        
        # published_at ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_articles = sorted(
            articles, 
            key=lambda x: x.get('published_at', ''), 
            reverse=True
        )
        
        # ì¤‘ë³µ ë³¸ë¬¸ ì°¾ê¸°
        content_duplicates = self.similarity_calculator.find_duplicate_contents(sorted_articles)
        
        # ì œê±°í•  ê¸°ì‚¬ ì¸ë±ìŠ¤ ì§‘í•©
        indices_to_remove = set()
        
        for i, j, similarity_result in content_duplicates:
            # ë” ìµœì‹  ê¸°ì‚¬(ì¸ë±ìŠ¤ê°€ ì‘ì€ ê²ƒ)ë¥¼ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì œê±°
            indices_to_remove.add(j)  # jê°€ ë” ì˜¤ë˜ëœ ê¸°ì‚¬
        
        # ì œê±°í•  ì¸ë±ìŠ¤ê°€ ì•„ë‹Œ ê¸°ì‚¬ë§Œ ìœ ì§€
        unique_articles = [
            article for idx, article in enumerate(sorted_articles)
            if idx not in indices_to_remove
        ]
        
        return unique_articles, len(indices_to_remove)
    
    def remove_hybrid_duplicate_contents(self, articles: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ë³¸ë¬¸ ì¤‘ë³µ ì œê±° (O(n) ì‹œê°„ ë³µì¡ë„)
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ì¤‘ë³µ ìˆ˜)
        """
        if not articles:
            return articles, 0
        
        # published_at ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_articles = sorted(
            articles, 
            key=lambda x: x.get('published_at', ''), 
            reverse=True
        )
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì¤‘ë³µ ì°¾ê¸° (O(n) ë³µì¡ë„)
        content_duplicates = self.similarity_calculator.find_hybrid_duplicates(sorted_articles)
        
        # ì œê±°í•  ê¸°ì‚¬ ì¸ë±ìŠ¤ ì§‘í•©
        indices_to_remove = set()
        
        for i, j, similarity_result in content_duplicates:
            # ë” ìµœì‹  ê¸°ì‚¬(ì¸ë±ìŠ¤ê°€ ì‘ì€ ê²ƒ)ë¥¼ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì œê±°
            indices_to_remove.add(j)  # jê°€ ë” ì˜¤ë˜ëœ ê¸°ì‚¬
        
        # ì¤‘ë³µì´ ì•„ë‹Œ ê¸°ì‚¬ë“¤ë§Œ ìœ ì§€
        unique_articles = [
            article for idx, article in enumerate(sorted_articles) 
            if idx not in indices_to_remove
        ]
        
        duplicates_removed = len(indices_to_remove)
        return unique_articles, duplicates_removed
    
    def save_to_articles_cleaned(self, articles: List[Dict[str, Any]]) -> bool:
        """
        ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ë¥¼ articles_cleaned í…Œì´ë¸”ì— ì €ì¥
        
        Args:
            articles: ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        if not self.supabase_manager.client:
            return False
        
        try:
            # articles_cleaned í…Œì´ë¸”ì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
            cleaned_articles = []
            
            for article in articles:
                cleaned_article = {
                    'original_article_id': article['id'],
                    'title_cleaned': article.get('title', ''),
                    'content_cleaned': article.get('content', ''),
                    'preprocessing_metadata': {
                        'duplicate_removal': {
                            'processed_at': datetime.now().isoformat(),
                            'title_duplicates_removed': 0,  # ê°œë³„ ê¸°ì‚¬ì—ì„œëŠ” 0
                            'content_duplicates_removed': 0,
                            'similarity_threshold': self.similarity_calculator.content_threshold,
                            'method': 'hybrid'
                        },
                        'basic_filter': {
                            'processed_at': datetime.now().isoformat(),
                            'no_content_removed': False,
                            'news_agency_removed': False,
                            'short_article_removed': False
                        }
                    },
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }
                cleaned_articles.append(cleaned_article)
            
            # ë°°ì¹˜ë¡œ ì €ì¥
            result = self.supabase_manager.client.table('articles_cleaned').insert(cleaned_articles).execute()
            
            return bool(result.data)
            
        except Exception as e:
            print(f"âŒ articles_cleaned ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def process_integrated_preprocessing(self, limit: Optional[int] = None) -> PreprocessingResult:
        """
        í†µí•© ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§)
        
        Args:
            limit: ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ ì œí•œ
            
        Returns:
            í†µí•© ì „ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = datetime.now()
        
        try:
            print("ğŸš€ í†µí•© ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
            
            # 1. Supabaseì—ì„œ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ
            print("ğŸ“¡ ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ ì¤‘...")
            articles = self.fetch_articles_from_supabase(limit)
            print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
            
            if not articles:
                return PreprocessingResult(
                    total_articles=0,
                    title_duplicates_removed=0,
                    content_duplicates_removed=0,
                    no_content_removed=0,
                    news_agency_removed=0,
                    short_articles_removed=0,
                    final_articles=0,
                    processing_time=0,
                    success=True
                )
            
            # 2. ì œëª© ì¤‘ë³µ ì œê±°
            print("ğŸ” ì œëª© ì¤‘ë³µ ì œê±° ì¤‘...")
            articles_after_title, title_duplicates = self.remove_duplicate_titles(articles)
            print(f"âœ… ì œëª© ì¤‘ë³µ {title_duplicates}ê°œ ì œê±° ì™„ë£Œ")
            
            # 3. í•˜ì´ë¸Œë¦¬ë“œ ë³¸ë¬¸ ì¤‘ë³µ ì œê±°
            print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ë³¸ë¬¸ ì¤‘ë³µ ì œê±° ì¤‘...")
            articles_after_content, content_duplicates = self.remove_hybrid_duplicate_contents(articles_after_title)
            print(f"âœ… ë³¸ë¬¸ ì¤‘ë³µ {content_duplicates}ê°œ ì œê±° ì™„ë£Œ")
            
            # 4. ê¸°ë³¸ í•„í„°ë§ (ë³¸ë¬¸ ì—†ëŠ” ê¸°ì‚¬, ë‰´ìŠ¤í†µì‹ ì‚¬ì—…ì, ì§§ì€ ê¸°ì‚¬ ì œê±°)
            print("ğŸ” ê¸°ë³¸ í•„í„°ë§ ì‹œì‘...")
            basic_filter_result = self.basic_filter.process_basic_filtering(articles_after_content)
            
            if not basic_filter_result.success:
                raise Exception(f"ê¸°ë³¸ í•„í„°ë§ ì‹¤íŒ¨: {basic_filter_result.error_message}")
            
            print(f"âœ… ê¸°ë³¸ í•„í„°ë§ ì™„ë£Œ: {basic_filter_result.final_articles}ê°œ ê¸°ì‚¬ ë‚¨ìŒ")
            
            # 5. ìµœì¢… ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ í•„í„°ë§ í›„ ë‚¨ì€ ê¸°ì‚¬ë“¤)
            # basic_filter.process_basic_filteringì€ ê²°ê³¼ë§Œ ë°˜í™˜í•˜ë¯€ë¡œ, ì‹¤ì œ í•„í„°ë§ëœ ê¸°ì‚¬ë¥¼ ë‹¤ì‹œ ê°€ì ¸ì™€ì•¼ í•¨
            final_articles = self._apply_basic_filtering(articles_after_content)
            
            # 6. articles_cleanedì— ì €ì¥
            print("ğŸ’¾ articles_cleanedì— ì €ì¥ ì¤‘...")
            save_success = self.save_to_articles_cleaned(final_articles)
            
            if not save_success:
                raise Exception("articles_cleaned ì €ì¥ ì‹¤íŒ¨")
            
            print(f"âœ… {len(final_articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
            
            # 7. ê²°ê³¼ ë°˜í™˜
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return PreprocessingResult(
                total_articles=len(articles),
                title_duplicates_removed=title_duplicates,
                content_duplicates_removed=content_duplicates,
                no_content_removed=basic_filter_result.no_content_removed,
                news_agency_removed=basic_filter_result.news_agency_removed,
                short_articles_removed=basic_filter_result.short_articles_removed,
                final_articles=len(final_articles),
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            error_msg = f"í†µí•© ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            
            return PreprocessingResult(
                total_articles=len(articles) if 'articles' in locals() else 0,
                title_duplicates_removed=0,
                content_duplicates_removed=0,
                no_content_removed=0,
                news_agency_removed=0,
                short_articles_removed=0,
                final_articles=0,
                processing_time=processing_time,
                success=False,
                error_message=error_msg
            )
    
    def _apply_basic_filtering(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ê¸°ë³¸ í•„í„°ë§ì„ ì ìš©í•˜ì—¬ ì‹¤ì œ í•„í„°ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í•„í„°ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        # 1. ë³¸ë¬¸ ì—†ëŠ” ê¸°ì‚¬ ì œê±°
        articles_filtered, _ = self.basic_filter.filter_no_content_articles(articles)
        
        # 2. ë‰´ìŠ¤í†µì‹ ì‚¬ì—…ì ê¸°ì‚¬ ì œê±°
        articles_filtered, _ = self.basic_filter.filter_news_agency_articles(articles_filtered)
        
        # 3. ì§§ì€ ê¸°ì‚¬ ì œê±°
        articles_filtered, _ = self.basic_filter.filter_short_articles(articles_filtered)
        
        return articles_filtered

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í†µí•© ì „ì²˜ë¦¬ê¸° ìƒì„±
    preprocessor = IntegratedPreprocessor()
    
    # í†µí•© ì „ì²˜ë¦¬ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 100ê°œë§Œ)
    result = preprocessor.process_integrated_preprocessing(limit=100)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š í†µí•© ì „ì²˜ë¦¬ ê²°ê³¼:")
    print(f"  ì´ ê¸°ì‚¬ ìˆ˜: {result.total_articles}")
    print(f"  ì œëª© ì¤‘ë³µ ì œê±°: {result.title_duplicates_removed}ê°œ")
    print(f"  ë³¸ë¬¸ ì¤‘ë³µ ì œê±°: {result.content_duplicates_removed}ê°œ")
    print(f"  ë³¸ë¬¸ ì—†ëŠ” ê¸°ì‚¬ ì œê±°: {result.no_content_removed}ê°œ")
    print(f"  ë‰´ìŠ¤í†µì‹ ì‚¬ì—…ì ê¸°ì‚¬ ì œê±°: {result.news_agency_removed}ê°œ")
    print(f"  ì§§ì€ ê¸°ì‚¬ ì œê±°: {result.short_articles_removed}ê°œ")
    print(f"  ìµœì¢… ê¸°ì‚¬ ìˆ˜: {result.final_articles}")
    print(f"  ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
    print(f"  ì„±ê³µ ì—¬ë¶€: {'âœ… ì„±ê³µ' if result.success else 'âŒ ì‹¤íŒ¨'}")
    
    if result.error_message:
        print(f"  ì˜¤ë¥˜ ë©”ì‹œì§€: {result.error_message}")

# ì´ì „ ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
DuplicateRemover = IntegratedPreprocessor
DuplicateRemovalResult = PreprocessingResult
