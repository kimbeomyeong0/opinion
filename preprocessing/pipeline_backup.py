#!/usr/bin/env python3
"""
ì¢…í•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- ëª¨ë“  ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
- ë‹¨ê³„ë³„ ì‹¤í–‰ ë° ì „ì²´ ì‹¤í–‰ ì§€ì›
- ì§„í–‰ ìƒí™© ì¶”ì  ë° ì˜¤ë¥˜ ì²˜ë¦¬
"""

import sys
import os
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from utils.supabase_manager import SupabaseManager
from preprocessing.modules.duplicate_remover import IntegratedPreprocessor
from preprocessing.modules.text_cleaner import TextCleaner
from preprocessing.modules.text_normalizer import TextNormalizer
from preprocessing.modules.content_merger import ContentMerger

# ë¡œê¹… ì„¤ì •
def setup_logging(verbose: bool = False):
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •"""
    log_level = logging.DEBUG if verbose else logging.INFO
    
    # ë¡œê·¸ í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì„ íƒì )
    try:
        file_handler = logging.FileHandler('preprocessing.log', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
    except Exception as e:
        print(f"âš ï¸  ë¡œê·¸ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")

# ê¸°ë³¸ ë¡œê¹… ì„¤ì •
setup_logging()
logger = logging.getLogger(__name__)

@dataclass
class PipelineResult:
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼"""
    stage: str
    success: bool
    total_articles: int
    processed_articles: int
    processing_time: float
    message: str
    error_message: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    memory_usage: Optional[float] = None  # MB
    throughput: Optional[float] = None  # articles/second

@dataclass
class FullPipelineResult:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼"""
    total_execution_time: float
    stages_completed: List[str]
    stages_failed: List[str]
    final_article_count: int
    stage_results: Dict[str, PipelineResult]
    overall_success: bool

class PreprocessingPipeline:
    """ì¢…í•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    # ìƒìˆ˜ ì •ì˜
    DEFAULT_PAGE_SIZE = 1000
    PROGRESS_LOG_INTERVAL = 100
    BATCH_SIZE = 50
    
    def __init__(self, verbose: bool = False):
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        # ë¡œê¹… ì„¤ì •
        setup_logging(verbose)
        self.logger = logging.getLogger(__name__)
        
        self.supabase_manager = SupabaseManager()
        
        # ê° ëª¨ë“ˆ ì´ˆê¸°í™”
        self.duplicate_processor = IntegratedPreprocessor()
        self.text_cleaner = TextCleaner()
        self.text_normalizer = TextNormalizer()
        self.content_merger = ContentMerger()
        
        # ë¦¬ë“œë¬¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” (ì„±ëŠ¥ ìµœì í™”)
        from preprocessing.modules.lead_extractor import LeadExtractor
        self.lead_extractor = LeadExtractor()
        
        # ë‹¨ê³„ë³„ ì •ì˜
        self.stages = {
            'duplicate_removal': '1ë‹¨ê³„: ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§',
            'text_cleaning': '2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ',
            'text_normalization': '3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ê·œí™”',
            'content_merging': '4ë‹¨ê³„: ì œëª©+ë³¸ë¬¸ í†µí•©'
        }
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_metrics = {}
    
    def _get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (MB)"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            # psutilì´ ì—†ëŠ” ê²½ìš° ëŒ€ëµì  ì¶”ì •
            import sys
            return sys.getsizeof(self) / 1024 / 1024
        except Exception:
            return 0.0
    
    def _calculate_throughput(self, processed_articles: int, processing_time: float) -> float:
        """ì²˜ë¦¬ëŸ‰ ê³„ì‚° (articles/second)"""
        if processing_time > 0:
            return processed_articles / processing_time
        return 0.0
    
    def _log_performance_metrics(self, stage: str, result: PipelineResult):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        self.performance_metrics[stage] = {
            'processing_time': result.processing_time,
            'processed_articles': result.processed_articles,
            'throughput': result.throughput,
            'memory_usage': result.memory_usage,
            'success': result.success
        }
        
        self.logger.info(f"ğŸ“Š {stage} ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        self.logger.info(f"  â±ï¸  ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
        self.logger.info(f"  ğŸ“ˆ ì²˜ë¦¬ëŸ‰: {result.throughput:.2f} articles/sec")
        self.logger.info(f"  ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {result.memory_usage:.2f}MB")
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
        try:
            if not self.supabase_manager.client:
                return {'pipeline_ready': False, 'error': 'Supabase client not initialized'}
            
            # ê°„ë‹¨í•œ ì¹´ìš´íŠ¸ ì¡°íšŒ
            articles_total = self.supabase_manager.client.table('articles').select('id', count='exact').execute()
            articles_preprocessed = self.supabase_manager.client.table('articles').select('id', count='exact').eq('is_preprocessed', True).execute()
            cleaned_result = self.supabase_manager.client.table('articles_cleaned').select('id', count='exact').execute()
            merged_count = self.supabase_manager.client.table('articles_cleaned').select('id', count='exact').not_.is_('merged_content', 'null').execute()
            
            return {
                'articles_total': articles_total.count if articles_total else 0,
                'articles_preprocessed': articles_preprocessed.count if articles_preprocessed else 0,
                'cleaned_articles': cleaned_result.count if cleaned_result else 0,
                'merged_articles': merged_count.count if merged_count else 0,
                'pipeline_ready': True
            }
            
        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'pipeline_ready': False, 'error': str(e)}
    
    def run_stage_1_duplicate_removal(self) -> PipelineResult:
        """1ë‹¨ê³„: ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§"""
        start_time = time.time()
        stage = 'duplicate_removal'
        
        try:
            self.logger.info("ğŸ”„ 1ë‹¨ê³„: ì¤‘ë³µ ì œê±° + ê¸°ë³¸ í•„í„°ë§ ì‹œì‘")
            
            # í†µí•© ì „ì²˜ë¦¬ ì‹¤í–‰
            result = self.duplicate_processor.process_integrated_preprocessing()
            
            processing_time = time.time() - start_time
            
            if result.success:
                message = f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {result.final_articles}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ë¨"
                self.logger.info(message)
                
                return PipelineResult(
                    stage=stage,
                    success=True,
                    total_articles=result.total_articles,
                    processed_articles=result.final_articles,
                    processing_time=processing_time,
                    message=message,
                    metadata={
                        'title_duplicates_removed': result.title_duplicates_removed,
                        'content_duplicates_removed': result.content_duplicates_removed,
                        'no_content_removed': result.no_content_removed,
                        'news_agency_removed': result.news_agency_removed,
                        'short_articles_removed': result.short_articles_removed
                    }
                )
            else:
                error_msg = f"âŒ 1ë‹¨ê³„ ì‹¤íŒ¨: {result.error_message}"
                self.logger.error(error_msg)
                
                return PipelineResult(
                    stage=stage,
                    success=False,
                    total_articles=0,
                    processed_articles=0,
                    processing_time=processing_time,
                    message=error_msg,
                    error_message=result.error_message
                )
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"âŒ 1ë‹¨ê³„ ì˜ˆì™¸ ë°œìƒ: {e}"
            self.logger.error(error_msg)
            
            return PipelineResult(
                stage=stage,
                success=False,
                total_articles=0,
                processed_articles=0,
                processing_time=processing_time,
                message=error_msg,
                error_message=str(e)
            )
    
    def run_stage_2_text_cleaning(self) -> PipelineResult:
        """2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ (ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬)"""
        start_time = time.time()
        stage = 'text_cleaning'
        
        try:
            self.logger.info("ğŸ”„ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ ì‹œì‘")
            
            # 1. ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì¡°íšŒí•˜ì—¬ ìºì‹œ
            media_cache = self._build_media_cache()
            
            # 2. ì •ì œë˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤ì„ ë°°ì¹˜ë¡œ ì¡°íšŒ
            articles = self._fetch_articles_for_cleaning()
            
            if not articles:
                message = "âœ… 2ë‹¨ê³„: ì •ì œí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤"
                self.logger.info(message)
                
                return PipelineResult(
                    stage=stage,
                    success=True,
                    total_articles=0,
                    processed_articles=0,
                    processing_time=time.time() - start_time,
                    message=message
                )
            
            # 3. ë°°ì¹˜ ì²˜ë¦¬ë¡œ í…ìŠ¤íŠ¸ ì •ì œ ì‹¤í–‰
            processed_count, failed_count = self._process_articles_batch(
                articles, media_cache, self._clean_single_article
            )
            
            processing_time = time.time() - start_time
            message = f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {processed_count}ê°œ ì •ì œ, {failed_count}ê°œ ì‹¤íŒ¨"
            self.logger.info(message)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            memory_usage = self._get_memory_usage()
            throughput = self._calculate_throughput(processed_count, processing_time)
            
            result = PipelineResult(
                stage=stage,
                success=True,
                total_articles=len(articles),
                processed_articles=processed_count,
                processing_time=processing_time,
                message=message,
                metadata={'failed_count': failed_count},
                memory_usage=memory_usage,
                throughput=throughput
            )
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…
            self._log_performance_metrics(stage, result)
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"âŒ 2ë‹¨ê³„ ì˜ˆì™¸ ë°œìƒ: {e}"
            self.logger.error(error_msg)
            
            return PipelineResult(
                stage=stage,
                success=False,
                total_articles=0,
                processed_articles=0,
                processing_time=processing_time,
                message=error_msg,
                error_message=str(e)
            )
    
    def run_stage_3_text_normalization(self) -> PipelineResult:
        """3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ê·œí™” (ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬)"""
        start_time = time.time()
        stage = 'text_normalization'
        
        try:
            self.logger.info("ğŸ”„ 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹œì‘")
            
            # ì •ê·œí™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤ì„ ë°°ì¹˜ë¡œ ì¡°íšŒ
            articles = self._fetch_articles_for_normalization()
            
            if not articles:
                message = "âœ… 3ë‹¨ê³„: ì •ê·œí™”í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤"
                self.logger.info(message)
                
                return PipelineResult(
                    stage=stage,
                    success=True,
                    total_articles=0,
                    processed_articles=0,
                    processing_time=time.time() - start_time,
                    message=message
                )
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹¤í–‰
            processed_count, failed_count = self._process_articles_batch(
                articles, {}, self._normalize_single_article
            )
            
            processing_time = time.time() - start_time
            message = f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: {processed_count}ê°œ ì •ê·œí™”, {failed_count}ê°œ ì‹¤íŒ¨"
            self.logger.info(message)
            
            return PipelineResult(
                stage=stage,
                success=True,
                total_articles=len(articles),
                processed_articles=processed_count,
                processing_time=processing_time,
                message=message,
                metadata={'failed_count': failed_count}
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"âŒ 3ë‹¨ê³„ ì˜ˆì™¸ ë°œìƒ: {e}"
            self.logger.error(error_msg)
            
            return PipelineResult(
                stage=stage,
                success=False,
                total_articles=0,
                processed_articles=0,
                processing_time=processing_time,
                message=error_msg,
                error_message=str(e)
            )
    
    def run_stage_4_content_merging(self) -> PipelineResult:
        """4ë‹¨ê³„: ì œëª©+ë³¸ë¬¸ í†µí•©"""
        start_time = time.time()
        stage = 'content_merging'
        
        try:
            self.logger.info("ğŸ”„ 4ë‹¨ê³„: ì œëª©+ë³¸ë¬¸ í†µí•© ì‹œì‘")
            
            # í†µí•© ì‹¤í–‰
            result = self.content_merger.process_content_merge()
            
            processing_time = time.time() - start_time
            
            if result['successful_saves'] > 0:
                message = f"âœ… 4ë‹¨ê³„ ì™„ë£Œ: {result['successful_saves']}ê°œ ê¸°ì‚¬ í†µí•©ë¨"
                self.logger.info(message)
                
                return PipelineResult(
                    stage=stage,
                    success=True,
                    total_articles=result['total_articles'],
                    processed_articles=result['successful_saves'],
                    processing_time=processing_time,
                    message=message,
                    metadata={
                        'successful_merges': result['successful_merges'],
                        'failed_merges': result['failed_merges'],
                        'merge_strategies': result['merge_strategies']
                    }
                )
            else:
                error_msg = f"âŒ 4ë‹¨ê³„ ì‹¤íŒ¨: í†µí•©ëœ ê¸°ì‚¬ê°€ ì—†ìŒ"
                self.logger.error(error_msg)
                
                return PipelineResult(
                    stage=stage,
                    success=False,
                    total_articles=result['total_articles'],
                    processed_articles=0,
                    processing_time=processing_time,
                    message=error_msg,
                    error_message="No articles were merged"
                )
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"âŒ 4ë‹¨ê³„ ì˜ˆì™¸ ë°œìƒ: {e}"
            self.logger.error(error_msg)
            
            return PipelineResult(
                stage=stage,
                success=False,
                total_articles=0,
                processed_articles=0,
                processing_time=processing_time,
                message=error_msg,
                error_message=str(e)
            )
    
    def run_single_stage(self, stage_name: str) -> PipelineResult:
        """ë‹¨ì¼ ë‹¨ê³„ ì‹¤í–‰"""
        if stage_name not in self.stages:
            return PipelineResult(
                stage=stage_name,
                success=False,
                total_articles=0,
                processed_articles=0,
                processing_time=0,
                message=f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ê³„: {stage_name}",
                error_message=f"Available stages: {list(self.stages.keys())}"
            )
        
        self.logger.info(f"ğŸš€ {self.stages[stage_name]} ì‹¤í–‰ ì‹œì‘")
        
        if stage_name == 'duplicate_removal':
            return self.run_stage_1_duplicate_removal()
        elif stage_name == 'text_cleaning':
            return self.run_stage_2_text_cleaning()
        elif stage_name == 'text_normalization':
            return self.run_stage_3_text_normalization()
        elif stage_name == 'content_merging':
            return self.run_stage_4_content_merging()
    
    def _map_media_outlet_name(self, outlet_name: str) -> str:
        """í•œê¸€ ì–¸ë¡ ì‚¬ ì´ë¦„ì„ ì˜ë¬¸ í‚¤ë¡œ ë³€í™˜"""
        mapping = {
            'ì—°í•©ë‰´ìŠ¤': 'yonhap',
            'ê²½í–¥ì‹ ë¬¸': 'khan',
            'ì¤‘ì•™ì¼ë³´': 'joongang',
            'ì¡°ì„ ì¼ë³´': 'chosun',
            'ë™ì•„ì¼ë³´': 'donga',
            'ì˜¤ë§ˆì´ë‰´ìŠ¤': 'ohmynews',
            'ë‰´ì‹œìŠ¤': 'newsis',
            'í•œê²¨ë ˆ': 'hani',
            'ë‰´ìŠ¤1': 'news1',
            'ë‰´ìŠ¤ì›': 'newsone'
        }
        return mapping.get(outlet_name, 'unknown')
    
    def _build_media_cache(self) -> Dict[str, str]:
        """ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì¡°íšŒí•˜ì—¬ ìºì‹œ êµ¬ì¶•"""
        try:
            # articlesì™€ media_outletsë¥¼ ì¡°ì¸í•˜ì—¬ í•œ ë²ˆì— ì¡°íšŒ
            result = self.supabase_manager.client.table('articles').select(
                'id, media_id, media_outlets(name)'
            ).execute()
            
            cache = {}
            for article in result.data:
                if article.get('media_outlets') and article['media_outlets'].get('name'):
                    outlet_name = article['media_outlets']['name']
                    cache[article['id']] = self._map_media_outlet_name(outlet_name)
                else:
                    cache[article['id']] = 'unknown'
            
            self.logger.info(f"ğŸ“Š ì–¸ë¡ ì‚¬ ìºì‹œ êµ¬ì¶• ì™„ë£Œ: {len(cache)}ê°œ ê¸°ì‚¬")
            return cache
            
        except Exception as e:
            self.logger.warning(f"ì–¸ë¡ ì‚¬ ìºì‹œ êµ¬ì¶• ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            return {}
    
    def _fetch_articles_for_cleaning(self) -> List[Dict[str, Any]]:
        """ì •ì œë˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°íšŒ"""
        articles = []
        page_size = self.DEFAULT_PAGE_SIZE
        offset = 0
        
        while True:
            try:
                articles_result = self.supabase_manager.client.table('articles_cleaned').select(
                    'id, article_id, title_cleaned, lead_paragraph'
                ).or_(
                    'preprocessing_metadata->>text_cleaned.is.null,preprocessing_metadata->>text_cleaned.eq.false'
                ).order('created_at').range(offset, offset + page_size - 1).execute()
                
                page_data = articles_result.data if articles_result else []
                if not page_data:
                    break
                    
                articles.extend(page_data)
                
                if len(page_data) < page_size:
                    break
                    
                offset += page_size
                
            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (offset {offset}): {e}")
                break
        
        return articles
    
    def _process_articles_batch(self, articles: List[Dict[str, Any]], 
                               media_cache: Dict[str, str], 
                               process_func) -> Tuple[int, int]:
        """ê¸°ì‚¬ë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
        processed_count = 0
        failed_count = 0
        batch_size = 50  # ë°°ì¹˜ í¬ê¸°
        update_batch = []
        
        for i, article in enumerate(articles):
            try:
                # ê°œë³„ ê¸°ì‚¬ ì²˜ë¦¬
                result = process_func(article, media_cache)
                
                if result:
                    update_batch.append(result)
                    processed_count += 1
                else:
                    failed_count += 1
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì¼ê´„ ì—…ë°ì´íŠ¸
                if len(update_batch) >= batch_size or i == len(articles) - 1:
                    if update_batch:
                        self._batch_update_articles(update_batch)
                        update_batch = []
                
                # ì§„í–‰ ìƒí™© ë¡œê¹…
                if (i + 1) % self.PROGRESS_LOG_INTERVAL == 0:
                    self.logger.info(f"ì§„í–‰ ìƒí™©: {i + 1}/{len(articles)} ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
                    
            except Exception as e:
                failed_count += 1
                self.logger.error(f"ê¸°ì‚¬ {article.get('id', 'unknown')} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        return processed_count, failed_count
    
    def _clean_single_article(self, article: Dict[str, Any], 
                            media_cache: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ê¸°ì‚¬ ì •ì œ ì²˜ë¦¬ (ë¦¬ë“œë¬¸ ê¸°ë°˜)"""
        try:
            media_outlet = media_cache.get(article['article_id'], 'unknown')
            
            # ì œëª©ê³¼ ë¦¬ë“œë¬¸ ì •ì œ
            cleaned_title, title_patterns = self._clean_article_title(article, media_outlet)
            cleaned_lead, lead_patterns, lead_paragraph = self._clean_article_lead(article, media_outlet)
            
            return self._build_cleaned_article_result(
                article, cleaned_title, title_patterns, 
                cleaned_lead, lead_patterns, lead_paragraph, media_outlet
            )
            
        except Exception as e:
            self.logger.error(f"ê¸°ì‚¬ {article.get('id', 'unknown')} ì •ì œ ì‹¤íŒ¨: {e}")
            return None
    
    def _clean_article_title(self, article: Dict[str, Any], media_outlet: str) -> Tuple[str, List[str]]:
        """ê¸°ì‚¬ ì œëª© ì •ì œ"""
        return self.text_cleaner.clean_title(article['title_cleaned'] or '', media_outlet)
    
    def _clean_article_lead(self, article: Dict[str, Any], media_outlet: str) -> Tuple[str, List[str], str]:
        """ê¸°ì‚¬ ë¦¬ë“œë¬¸ ì •ì œ"""
        lead_paragraph = article.get('lead_paragraph', '')
        if not lead_paragraph:
            lead_paragraph = article.get('lead_paragraph', '')
        
        cleaned_lead, lead_patterns = self.text_cleaner.clean_content(lead_paragraph, media_outlet)
        return cleaned_lead, lead_patterns, lead_paragraph
    
    def _build_cleaned_article_result(self, article: Dict[str, Any], cleaned_title: str, 
                                    title_patterns: List[str], cleaned_lead: str, 
                                    lead_patterns: List[str], lead_paragraph: str, 
                                    media_outlet: str) -> Dict[str, Any]:
        """ì •ì œëœ ê¸°ì‚¬ ê²°ê³¼ êµ¬ì„±"""
        return {
            'id': article['id'],
            'title_cleaned': cleaned_title,
            'lead_paragraph': cleaned_lead,  # ì •ì œëœ ë¦¬ë“œë¬¸
            'preprocessing_metadata': {
                'text_cleaned': True,
                'text_cleaned_at': datetime.now().isoformat(),
                'title_patterns_removed': title_patterns,
                'lead_patterns_removed': lead_patterns,
                'media_outlet': media_outlet,
                'lead_extraction': {
                    'original_lead_length': len(lead_paragraph),
                    'cleaned_lead_length': len(cleaned_lead)
                }
            },
            'updated_at': datetime.now().isoformat()
        }
    
    def _batch_update_articles(self, update_batch: List[Dict[str, Any]]) -> None:
        """ê¸°ì‚¬ë“¤ì„ ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸"""
        try:
            for update_data in update_batch:
                self.supabase_manager.client.table('articles_cleaned').update({
                    'title_cleaned': update_data['title_cleaned'],
                    'lead_paragraph': update_data['lead_paragraph'],
                    'preprocessing_metadata': update_data['preprocessing_metadata'],
                    'updated_at': update_data['updated_at']
                }).eq('id', update_data['id']).execute()
                
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # ê°œë³„ ì—…ë°ì´íŠ¸ë¡œ í´ë°±
            for update_data in update_batch:
                try:
                    self.supabase_manager.client.table('articles_cleaned').update({
                        'title_cleaned': update_data['title_cleaned'],
                        'lead_paragraph': update_data['lead_paragraph'],
                        'preprocessing_metadata': update_data['preprocessing_metadata'],
                        'updated_at': update_data['updated_at']
                    }).eq('id', update_data['id']).execute()
                except Exception as individual_error:
                    self.logger.error(f"ê°œë³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ID: {update_data['id']}): {individual_error}")
    
    def _fetch_articles_for_normalization(self) -> List[Dict[str, Any]]:
        """ì •ê·œí™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°íšŒ"""
        articles = []
        page_size = self.DEFAULT_PAGE_SIZE
        offset = 0
        
        while True:
            try:
                articles_result = self.supabase_manager.client.table('articles_cleaned').select(
                    'id, title_cleaned, lead_paragraph'
                ).or_(
                    'preprocessing_metadata->>text_normalized.is.null,preprocessing_metadata->>text_normalized.eq.false'
                ).order('created_at').range(offset, offset + page_size - 1).execute()
                
                page_data = articles_result.data if articles_result else []
                if not page_data:
                    break
                    
                articles.extend(page_data)
                
                if len(page_data) < page_size:
                    break
                    
                offset += page_size
                
            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (offset {offset}): {e}")
                break
        
        return articles
    
    def _normalize_single_article(self, article: Dict[str, Any], 
                                media_cache: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ê¸°ì‚¬ ì •ê·œí™” ì²˜ë¦¬"""
        try:
            # í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹¤í–‰
            title_result = self.text_normalizer.normalize_text(article['title_cleaned'] or '')
            content_result = self.text_normalizer.normalize_text(article['lead_paragraph'] or '')
            
            return {
                'id': article['id'],
                'title_cleaned': title_result.normalized_text,
                'lead_paragraph': content_result.normalized_text,
                'preprocessing_metadata': {
                    'text_normalized': True,
                    'text_normalized_at': datetime.now().isoformat(),
                    'title_changes': title_result.changes_made,
                    'content_changes': content_result.changes_made,
                    'normalization_stats': {
                        'title_changes_count': len(title_result.changes_made),
                        'content_changes_count': len(content_result.changes_made)
                    }
                },
                'updated_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ê¸°ì‚¬ {article.get('id', 'unknown')} ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return None
    
    def run_full_pipeline(self, skip_stages: List[str] = None) -> FullPipelineResult:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        skip_stages = skip_stages or []
        
        self.logger.info("ğŸš€ ì¢…í•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        self.logger.info("=" * 60)
        
        # ì´ˆê¸° ìƒíƒœ í™•ì¸
        initial_status = self.get_pipeline_status()
        self.logger.info(f"ğŸ“Š ì´ˆê¸° ìƒíƒœ: ì „ì²´ ê¸°ì‚¬ {initial_status.get('articles_total', 0)}ê°œ")
        
        stage_results = {}
        stages_completed = []
        stages_failed = []
        
        # ë‹¨ê³„ë³„ ì‹¤í–‰
        stage_order = ['duplicate_removal', 'text_cleaning', 'text_normalization', 'content_merging']
        
        for stage_name in stage_order:
            if stage_name in skip_stages:
                self.logger.info(f"â­ï¸  {self.stages[stage_name]} ê±´ë„ˆëœ€")
                continue
                
            result = self.run_single_stage(stage_name)
            stage_results[stage_name] = result
            
            if result.success:
                stages_completed.append(stage_name)
                self.logger.info(f"âœ… {self.stages[stage_name]} ì™„ë£Œ")
            else:
                stages_failed.append(stage_name)
                self.logger.error(f"âŒ {self.stages[stage_name]} ì‹¤íŒ¨")
                
                # ì¤‘ìš”í•œ ë‹¨ê³„ ì‹¤íŒ¨ ì‹œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨
                if stage_name in ['duplicate_removal']:
                    self.logger.error("ğŸ›‘ ì¤‘ìš”í•œ ë‹¨ê³„ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
                    break
            
            self.logger.info("-" * 40)
        
        # ìµœì¢… ìƒíƒœ í™•ì¸
        final_status = self.get_pipeline_status()
        final_article_count = final_status.get('cleaned_articles', 0)
        
        total_execution_time = time.time() - start_time
        overall_success = len(stages_failed) == 0
        
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        self.logger.info(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_execution_time:.2f}ì´ˆ")
        self.logger.info(f"âœ… ì™„ë£Œëœ ë‹¨ê³„: {len(stages_completed)}ê°œ")
        self.logger.info(f"âŒ ì‹¤íŒ¨í•œ ë‹¨ê³„: {len(stages_failed)}ê°œ")
        self.logger.info(f"ğŸ“Š ìµœì¢… ì²˜ë¦¬ëœ ê¸°ì‚¬: {final_article_count}ê°œ")
        
        return FullPipelineResult(
            total_execution_time=total_execution_time,
            stages_completed=stages_completed,
            stages_failed=stages_failed,
            final_article_count=final_article_count,
            stage_results=stage_results,
            overall_success=overall_success
        )

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = PreprocessingPipeline()
    
    # ìƒíƒœ í™•ì¸
    print("ğŸ“‹ íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸...")
    status = pipeline.get_pipeline_status()
    print(f"ì „ì²´ ê¸°ì‚¬: {status.get('articles_total', 0)}ê°œ")
    print(f"ì „ì²˜ë¦¬ëœ ê¸°ì‚¬: {status.get('articles_preprocessed', 0)}ê°œ")
    print(f"ì •ì œëœ ê¸°ì‚¬: {status.get('cleaned_articles', 0)}ê°œ")
    print(f"í†µí•©ëœ ê¸°ì‚¬: {status.get('merged_articles', 0)}ê°œ")
    print()
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = pipeline.run_full_pipeline()
    
    print("\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    print(f"ì „ì²´ ì„±ê³µ: {'âœ…' if result.overall_success else 'âŒ'}")
    print(f"ì‹¤í–‰ ì‹œê°„: {result.total_execution_time:.2f}ì´ˆ")
    print(f"ìµœì¢… ê¸°ì‚¬ ìˆ˜: {result.final_article_count}ê°œ")
