#!/usr/bin/env python3
"""
LLM ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
merged_contentë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ì´ìŠˆë¥¼ êµ°ì§‘í™”í•˜ëŠ” ì‹¤í—˜
"""

import sys
import os
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from utils.supabase_manager import SupabaseManager

# OpenAI ì„¤ì¹˜ í™•ì¸ ë° import
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    print("âŒ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("pip install openai ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    OPENAI_AVAILABLE = False

@dataclass
class Article:
    """ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    title: str
    merged_content: str
    media_id: str
    published_at: str

@dataclass
class Cluster:
    """í´ëŸ¬ìŠ¤í„° ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    name: str
    description: str
    articles: List[Article]
    keywords: List[str]

class LLMClusteringExperiment:
    """LLM ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í—˜ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise Exception("Supabase ì—°ê²° ì‹¤íŒ¨")
        
        if OPENAI_AVAILABLE:
            self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        else:
            self.openai_client = None
    
    def fetch_all_articles(self) -> List[Article]:
        """ëª¨ë“  ê¸°ì‚¬ ì¡°íšŒ"""
        try:
            print("ğŸ“¡ ëª¨ë“  ê¸°ì‚¬ ì¡°íšŒ ì¤‘...")
            
            result = self.supabase_manager.client.table('articles_cleaned').select(
                'article_id, merged_content, media_id, published_at'
            ).order('published_at', desc=True).execute()
            
            if not result.data:
                print("âŒ ê¸°ì‚¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            articles = []
            for item in result.data:
                article = Article(
                    id=item['article_id'],
                    title="",  # ì œëª© ë¶ˆí•„ìš”
                    merged_content=item['merged_content'],
                    media_id=item['media_id'],
                    published_at=item['published_at']
                )
                articles.append(article)
            
            print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
            return articles
            
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def split_articles_into_batches(self, articles: List[Article], batch_size: int = 200) -> List[List[Article]]:
        """ê¸°ì‚¬ë“¤ì„ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        for i in range(0, len(articles), batch_size):
            batch = articles[i:i + batch_size]
            batches.append(batch)
        return batches
    
    def create_clustering_prompt(self, articles: List[Article]) -> str:
        """í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        articles_text = ""
        for i, article in enumerate(articles, 1):
            articles_text += f"{i}. {article.merged_content}\n\n"
        
        prompt = f"""
ë‹¤ìŒ ì •ì¹˜ ê¸°ì‚¬ ë‚´ìš©ë“¤ì„ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì´ìŠˆ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ê¸°ì‚¬ ë‚´ìš©:
{articles_text}

ìš”êµ¬ì‚¬í•­:
1. ê°™ì€ ì •ì¹˜ ì´ìŠˆë¥¼ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ë“¤ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ì£¼ì„¸ìš”
2. ê° ê·¸ë£¹ì— ì ì ˆí•œ ì´ë¦„ê³¼ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”
3. ê° ê·¸ë£¹ì˜ í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”
4. ê·¸ë£¹ì´ ë„ˆë¬´ ë§ê±°ë‚˜ ì ì§€ ì•Šë„ë¡ ì ì ˆí•œ ìˆ˜ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "clusters": [
        {{
            "name": "ì´ìŠˆ ê·¸ë£¹ ì´ë¦„",
            "description": "ì´ìŠˆ ê·¸ë£¹ ì„¤ëª…",
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
            "article_indices": [1, 3, 5]
        }}
    ]
}}
"""
        return prompt
    
    def cluster_with_llm(self, articles: List[Article]) -> Optional[List[Cluster]]:
        """LLMì„ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§"""
        if not self.openai_client:
            print("âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            print("ğŸ¤– LLM í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.create_clustering_prompt(articles)
            
            # LLM í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì¹˜ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ë“¤ì„ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì´ìŠˆ ê·¸ë£¹ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=4000
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content
            print("ğŸ“ LLM ì‘ë‹µ ë°›ìŒ")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                json_text = response_text[json_start:json_end]
                
                result = json.loads(json_text)
                clusters_data = result.get('clusters', [])
                
                # Cluster ê°ì²´ë¡œ ë³€í™˜
                clusters = []
                for i, cluster_data in enumerate(clusters_data):
                    article_indices = cluster_data.get('article_indices', [])
                    cluster_articles = [articles[idx-1] for idx in article_indices if 1 <= idx <= len(articles)]
                    
                    cluster = Cluster(
                        id=f"cluster_{i+1}",
                        name=cluster_data.get('name', f'ê·¸ë£¹ {i+1}'),
                        description=cluster_data.get('description', ''),
                        articles=cluster_articles,
                        keywords=cluster_data.get('keywords', [])
                    )
                    clusters.append(cluster)
                
                print(f"âœ… {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ")
                return clusters
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                print(f"ì‘ë‹µ ë‚´ìš©: {response_text[:500]}...")
                return None
                
        except Exception as e:
            print(f"âŒ LLM í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def analyze_clusters(self, clusters: List[Cluster]) -> None:
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š LLM í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¶„ì„")
        print("="*60)
        
        for i, cluster in enumerate(clusters, 1):
            print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„° {i}: {cluster.name}")
            print(f"ğŸ“ ì„¤ëª…: {cluster.description}")
            print(f"ğŸ·ï¸  í‚¤ì›Œë“œ: {', '.join(cluster.keywords)}")
            print(f"ğŸ“° ê¸°ì‚¬ ìˆ˜: {len(cluster.articles)}ê°œ")
            
            print("ğŸ“‹ í¬í•¨ëœ ê¸°ì‚¬ë“¤:")
            for j, article in enumerate(cluster.articles, 1):
                print(f"  {j}. {article.merged_content[:50]}...")
            
            print("-" * 40)
    
    def generate_background(self, cluster: Cluster) -> str:
        """í´ëŸ¬ìŠ¤í„°ì˜ ë°°ê²½ ì •ë³´ ìƒì„±"""
        if not cluster.articles:
            return ""
        
        # ê¸°ì‚¬ ë‚´ìš©ì˜ ì•ë¶€ë¶„ì„ ë°°ê²½ ì •ë³´ë¡œ ì‚¬ìš©
        contents = [article.merged_content[:100] for article in cluster.articles[:3]]  # ìµœëŒ€ 3ê°œ, 100ìì”©
        background = f"ê´€ë ¨ ê¸°ì‚¬ ë‚´ìš©:\n"
        for i, content in enumerate(contents, 1):
            background += f"â€¢ {content}...\n"
        
        return background.strip()
    
    def save_to_issues_table(self, clusters: List[Cluster]) -> bool:
        """í´ëŸ¬ìŠ¤í„°ë¥¼ issues í…Œì´ë¸”ì— ì €ì¥"""
        try:
            print(f"ğŸ’¾ {len(clusters)}ê°œ ì´ìŠˆë¥¼ issues í…Œì´ë¸”ì— ì €ì¥ ì¤‘...")
            
            issues_data = []
            for cluster in clusters:
                if not cluster.articles:  # ê¸°ì‚¬ê°€ ì—†ëŠ” í´ëŸ¬ìŠ¤í„°ëŠ” ê±´ë„ˆë›°ê¸°
                    continue
                
                issue_data = {
                    "date": datetime.now().date().isoformat(),
                    "title": cluster.name,
                    "summary": f"{len(cluster.articles)}ê°œ ê¸°ì‚¬ë¡œ êµ¬ì„±ëœ ì´ìŠˆ",
                    "subtitle": cluster.description,
                    "background": self.generate_background(cluster),
                    "source": len(cluster.articles),
                    "left_source": 0,
                    "center_source": 0,
                    "right_source": 0,
                    "created_at": datetime.now().isoformat()
                }
                issues_data.append(issue_data)
            
            if not issues_data:
                print("âš ï¸ ì €ì¥í•  ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # issues í…Œì´ë¸”ì— ì €ì¥
            result = self.supabase_manager.client.table('issues').insert(issues_data).execute()
            
            if result.data:
                print(f"âœ… {len(result.data)}ê°œ ì´ìŠˆ ì €ì¥ ì™„ë£Œ")
                
                # issue_articles í…Œì´ë¸”ì— ì—°ê²° ì •ë³´ ì €ì¥
                self.save_issue_articles_connections(clusters, result.data)
                return True
            else:
                print("âŒ ì´ìŠˆ ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì´ìŠˆ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def save_issue_articles_connections(self, clusters: List[Cluster], saved_issues: List[Dict]) -> bool:
        """issue_articles í…Œì´ë¸”ì— ì—°ê²° ì •ë³´ ì €ì¥"""
        try:
            print("ğŸ”— ì´ìŠˆ-ê¸°ì‚¬ ì—°ê²° ì •ë³´ ì €ì¥ ì¤‘...")
            
            connections = []
            for i, cluster in enumerate(clusters):
                if i >= len(saved_issues) or not cluster.articles:
                    continue
                
                issue_id = saved_issues[i]['id']
                for article in cluster.articles:
                    # articles_cleaned í…Œì´ë¸”ì—ì„œ cleaned_article_id ì¡°íšŒ
                    cleaned_result = self.supabase_manager.client.table('articles_cleaned').select('id').eq('article_id', article.id).execute()
                    
                    if cleaned_result.data:
                        cleaned_article_id = cleaned_result.data[0]['id']
                    else:
                        print(f"âš ï¸ article_id {article.id}ì— í•´ë‹¹í•˜ëŠ” cleaned_article_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        cleaned_article_id = None
                    
                    connection = {
                        "issue_id": issue_id,
                        "article_id": article.id,
                        "cleaned_article_id": cleaned_article_id
                    }
                    connections.append(connection)
            
            if connections:
                result = self.supabase_manager.client.table('issue_articles').insert(connections).execute()
                if result.data:
                    print(f"âœ… {len(result.data)}ê°œ ì—°ê²° ì •ë³´ ì €ì¥ ì™„ë£Œ")
                    return True
            
            print("âš ï¸ ì—°ê²° ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ ì—°ê²° ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def save_results(self, clusters: List[Cluster], filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë°±ì—…ìš©)"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"llm_clustering_results_{timestamp}.json"
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "total_clusters": len(clusters),
            "clusters": []
        }
        
        for cluster in clusters:
            cluster_data = {
                "id": cluster.id,
                "name": cluster.name,
                "description": cluster.description,
                "keywords": cluster.keywords,
                "article_count": len(cluster.articles),
                "articles": [
                    {
                        "id": article.id,
                        "title": article.title,
                        "merged_content": article.merged_content,
                        "media_id": article.media_id,
                        "published_at": article.published_at
                    }
                    for article in cluster.articles
                ]
            }
            results["clusters"].append(cluster_data)
        
        filepath = os.path.join(project_root, "experiments", filename)
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ë°±ì—… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
    
    def create_merge_prompt(self, all_clusters: List[List[Cluster]]) -> str:
        """ë°°ì¹˜ ê²°ê³¼ í†µí•©ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        clusters_text = ""
        cluster_id = 1
        
        for batch_idx, batch_clusters in enumerate(all_clusters, 1):
            clusters_text += f"\n=== ë°°ì¹˜ {batch_idx} ê²°ê³¼ ===\n"
            for cluster in batch_clusters:
                clusters_text += f"{cluster_id}. {cluster.name}\n"
                clusters_text += f"   ì„¤ëª…: {cluster.description}\n"
                clusters_text += f"   í‚¤ì›Œë“œ: {', '.join(cluster.keywords)}\n"
                clusters_text += f"   ê¸°ì‚¬ ìˆ˜: {len(cluster.articles)}ê°œ\n"
                # ê¸°ì‚¬ ìƒ˜í”Œ ì¶”ê°€ (ë§¥ë½ ì´í•´ë¥¼ ìœ„í•´)
                sample_articles = cluster.articles[:3]  # ìµœëŒ€ 3ê°œ ê¸°ì‚¬ ìƒ˜í”Œ
                clusters_text += f"   ê¸°ì‚¬ ìƒ˜í”Œ:\n"
                for i, article in enumerate(sample_articles, 1):
                    clusters_text += f"     {i}. {article.merged_content[:100]}...\n"
                clusters_text += "\n"
                cluster_id += 1
        
        prompt = f"""
ë‹¤ìŒì€ ì—¬ëŸ¬ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ í´ëŸ¬ìŠ¤í„°ë§í•œ ì •ì¹˜ ë‰´ìŠ¤ ê²°ê³¼ì…ë‹ˆë‹¤. ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì´ìŠˆë“¤ì„ ì •í™•í•˜ê²Œ í•©ì³ì„œ ìµœì¢… ì´ìŠˆ ëª©ë¡ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:
{clusters_text}

ë¶„ì„ ê°€ì´ë“œë¼ì¸:
1. **ì •ì¹˜ì  ë§¥ë½ ê³ ë ¤**: ê°™ì€ ì •ì¹˜ ì´ìŠˆì˜ ë‹¤ë¥¸ í‘œí˜„ë“¤ì„ ì¸ì‹í•˜ì„¸ìš”
   - ì˜ˆ: "ì‚¬ë²•ê°œí˜"ê³¼ "ì‚¬ë²•ë¶€ ë…ë¦½"ì€ ê°™ì€ ì´ìŠˆ
   - ì˜ˆ: "ì •ì¹˜ìê¸ˆ"ê³¼ "ë¶€ì •ìˆ˜ê¸‰"ì€ ê°™ì€ ì´ìŠˆ
   - ì˜ˆ: "í•œë¯¸ê´€ê³„"ì™€ "ë¯¸êµ­ ë¹„ìë¬¸ì œ"ëŠ” ê°™ì€ ì´ìŠˆ

2. **ì‹œê°„ì  ì—°ì†ì„± ê³ ë ¤**: ê°™ì€ ì´ìŠˆì˜ ì§€ì†ì  ë³´ë„ë“¤ì„ ì¸ì‹í•˜ì„¸ìš”
   - ì˜ˆ: "íŠ¹ê²€ë²• ê°œì •" ê´€ë ¨ ê¸°ì‚¬ë“¤ì´ ì—¬ëŸ¬ ë°°ì¹˜ì— ê±¸ì³ ìˆì„ ìˆ˜ ìˆìŒ

3. **ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒë‹¨**: ë‹¨ìˆœ í‚¤ì›Œë“œê°€ ì•„ë‹Œ ì „ì²´ì  ë§¥ë½ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”
   - ê¸°ì‚¬ ë‚´ìš©ì˜ í•µì‹¬ ì£¼ì œê°€ ê°™ì€ì§€ í™•ì¸
   - ì •ì¹˜ì  ë§¥ë½ê³¼ ë°°ê²½ì´ ìœ ì‚¬í•œì§€ í™•ì¸

4. **ë…¸ì´ì¦ˆ ì‹ë³„**: ì–´ë–¤ ì´ìŠˆì—ë„ ì†í•˜ì§€ ì•ŠëŠ” ë…ë¦½ì ì¸ ê¸°ì‚¬ë“¤ë„ ê³ ë ¤í•˜ì„¸ìš”
   - ë‹¨ë°œì„± ì‚¬ê±´ì´ë‚˜ íŠ¹ìˆ˜í•œ ìƒí™©
   - ë‹¤ë¥¸ ì´ìŠˆë“¤ê³¼ ëª…í™•íˆ êµ¬ë¶„ë˜ëŠ” ë‚´ìš©

ìš”êµ¬ì‚¬í•­:
1. ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì´ìŠˆë“¤ì„ í•˜ë‚˜ë¡œ í•©ì³ì£¼ì„¸ìš”
2. ê° ìµœì¢… ì´ìŠˆì— ì ì ˆí•œ ì´ë¦„ê³¼ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”
3. ê° ì´ìŠˆì˜ í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”
4. í•©ì³ì§„ ì´ìŠˆì— í¬í•¨ëœ ì›ë³¸ í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë“¤ì„ ê¸°ë¡í•´ì£¼ì„¸ìš”
5. ë…ë¦½ì ì¸ ì´ìŠˆëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "final_clusters": [
        {{
            "name": "ìµœì¢… ì´ìŠˆ ì´ë¦„",
            "description": "ìµœì¢… ì´ìŠˆ ì„¤ëª…",
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
            "merged_from": [1, 5, 8],
            "confidence": "high|medium|low"
        }}
    ]
}}
"""
        return prompt
    
    def merge_batch_clusters(self, all_clusters: List[List[Cluster]]) -> Optional[List[Cluster]]:
        """ë°°ì¹˜ë³„ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë“¤ì„ í†µí•© (ê°œì„ ëœ ë²„ì „)"""
        if not self.openai_client:
            print("âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            print("ğŸ”„ ë°°ì¹˜ ê²°ê³¼ í†µí•© ì¤‘...")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.create_merge_prompt(all_clusters)
            
            # LLM í˜¸ì¶œ (ë” ì •êµí•œ ì„¤ì •)
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì¹˜ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë°°ì¹˜ì˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì´ìŠˆë“¤ì„ ì •í™•í•˜ê²Œ í•©ì³ì£¼ì„¸ìš”. ì •ì¹˜ì  ë§¥ë½ê³¼ ì‹œê°„ì  ì—°ì†ì„±ì„ ê³ ë ¤í•˜ì—¬ íŒë‹¨í•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì¶¤
                max_tokens=6000   # ë” ë§ì€ í† í° í• ë‹¹
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content
            print("ğŸ“ í†µí•© ê²°ê³¼ ë°›ìŒ")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                json_text = response_text[json_start:json_end]
                
                result = json.loads(json_text)
                final_clusters_data = result.get('final_clusters', [])
                
                # ëª¨ë“  ì›ë³¸ í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°
                all_original_clusters = []
                for batch_clusters in all_clusters:
                    all_original_clusters.extend(batch_clusters)
                
                # ìµœì¢… í´ëŸ¬ìŠ¤í„° ìƒì„±
                final_clusters = []
                for i, cluster_data in enumerate(final_clusters_data):
                    merged_from_indices = cluster_data.get('merged_from', [])
                    merged_articles = []
                    confidence = cluster_data.get('confidence', 'medium')
                    
                    # í†µí•©ëœ í´ëŸ¬ìŠ¤í„°ë“¤ì˜ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘
                    for idx in merged_from_indices:
                        if 1 <= idx <= len(all_original_clusters):
                            original_cluster = all_original_clusters[idx - 1]
                            merged_articles.extend(original_cluster.articles)
                    
                    # ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ê°™ì€ ê¸°ì‚¬ê°€ ì—¬ëŸ¬ ë°°ì¹˜ì— ìˆì„ ìˆ˜ ìˆìŒ)
                    unique_articles = []
                    seen_ids = set()
                    for article in merged_articles:
                        if article.id not in seen_ids:
                            unique_articles.append(article)
                            seen_ids.add(article.id)
                    
                    final_cluster = Cluster(
                        id=f"final_cluster_{i+1}",
                        name=cluster_data.get('name', f'í†µí•© ì´ìŠˆ {i+1}'),
                        description=cluster_data.get('description', ''),
                        articles=unique_articles,
                        keywords=cluster_data.get('keywords', [])
                    )
                    final_clusters.append(final_cluster)
                    
                    print(f"  ğŸ“Š í†µí•© ì´ìŠˆ {i+1}: {final_cluster.name} ({len(unique_articles)}ê°œ ê¸°ì‚¬, ì‹ ë¢°ë„: {confidence})")
                
                print(f"âœ… {len(final_clusters)}ê°œ ìµœì¢… í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ")
                return final_clusters
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                print(f"ì‘ë‹µ ë‚´ìš©: {response_text[:500]}...")
                return None
                
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ í†µí•© ì‹¤íŒ¨: {str(e)}")
            return None

    def run_batch_experiment(self, batch_size: int = 200) -> bool:
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í—˜ ì‹¤í–‰"""
        try:
            print("ğŸš€ LLM ê¸°ë°˜ ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í—˜ ì‹œì‘")
            print("="*60)
            
            # 1. ëª¨ë“  ê¸°ì‚¬ ì¡°íšŒ
            articles = self.fetch_all_articles()
            if not articles:
                return False
            
            # 2. ë°°ì¹˜ë¡œ ë¶„í• 
            batches = self.split_articles_into_batches(articles, batch_size)
            print(f"ğŸ“¦ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")
            
            # 3. ê° ë°°ì¹˜ë³„ í´ëŸ¬ìŠ¤í„°ë§
            all_clusters = []
            for i, batch in enumerate(batches, 1):
                print(f"\nğŸ”„ ë°°ì¹˜ {i}/{len(batches)} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ ê¸°ì‚¬)")
                batch_clusters = self.cluster_with_llm(batch)
                if batch_clusters:
                    all_clusters.append(batch_clusters)
                    print(f"âœ… ë°°ì¹˜ {i} ì™„ë£Œ: {len(batch_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
                else:
                    print(f"âŒ ë°°ì¹˜ {i} ì‹¤íŒ¨")
            
            if not all_clusters:
                print("âŒ ëª¨ë“  ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
                return False
            
            # 4. ë°°ì¹˜ ê²°ê³¼ í†µí•©
            print(f"\nğŸ”„ {len(all_clusters)}ê°œ ë°°ì¹˜ ê²°ê³¼ í†µí•© ì¤‘...")
            final_clusters = self.merge_batch_clusters(all_clusters)
            if not final_clusters:
                print("âŒ ë°°ì¹˜ í†µí•© ì‹¤íŒ¨")
                return False
            
            # 5. ê²°ê³¼ ë¶„ì„
            self.analyze_clusters(final_clusters)
            
            # 6. issues í…Œì´ë¸”ì— ì €ì¥
            save_success = self.save_to_issues_table(final_clusters)
            if not save_success:
                print("âŒ issues í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨")
                return False
            
            # 7. ë°±ì—… íŒŒì¼ ì €ì¥
            self.save_results(final_clusters)
            
            print(f"\nâœ… ë°°ì¹˜ ì‹¤í—˜ ì™„ë£Œ! {len(articles)}ê°œ ê¸°ì‚¬ â†’ {len(final_clusters)}ê°œ ìµœì¢… í´ëŸ¬ìŠ¤í„°")
            print("ğŸ“Š ê²°ê³¼ê°€ issues í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
            return False

    def run_experiment(self, article_limit: int = 30) -> bool:
        """ê¸°ì¡´ ì‹¤í—˜ ì‹¤í–‰ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        try:
            print("ğŸš€ LLM ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í—˜ ì‹œì‘")
            print("="*60)
            
            # 1. ìƒ˜í”Œ ê¸°ì‚¬ ì¡°íšŒ
            articles = self.fetch_sample_articles(article_limit)
            if not articles:
                return False
            
            # 2. LLM í´ëŸ¬ìŠ¤í„°ë§
            clusters = self.cluster_with_llm(articles)
            if not clusters:
                return False
            
            # 3. ê²°ê³¼ ë¶„ì„
            self.analyze_clusters(clusters)
            
            # 4. issues í…Œì´ë¸”ì— ì €ì¥
            save_success = self.save_to_issues_table(clusters)
            if not save_success:
                print("âŒ issues í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨")
                return False
            
            # 5. ë°±ì—… íŒŒì¼ ì €ì¥
            self.save_results(clusters)
            
            print(f"\nâœ… ì‹¤í—˜ ì™„ë£Œ! {len(articles)}ê°œ ê¸°ì‚¬ â†’ {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
            print("ğŸ“Š ê²°ê³¼ê°€ issues í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ§ª LLM ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í—˜")
    print("="*60)
    
    try:
        # ì‹¤í—˜ ëª¨ë“œ ì„ íƒ
        print("\nì‹¤í—˜ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì†Œê·œëª¨ ì‹¤í—˜ (30-50ê°œ ê¸°ì‚¬)")
        print("2. ì „ì²´ ê¸°ì‚¬ ë°°ì¹˜ ì²˜ë¦¬ (522ê°œ ê¸°ì‚¬)")
        
        while True:
            mode_input = input("\nëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()
            if mode_input == "1":
                # ì†Œê·œëª¨ ì‹¤í—˜
                while True:
                    try:
                        limit_input = input("ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 30): ").strip()
                        if not limit_input:
                            article_limit = 30
                            break
                        article_limit = int(limit_input)
                        if article_limit <= 0:
                            print("âŒ 1 ì´ìƒì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                            continue
                        break
                    except ValueError:
                        print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        continue
                
                experiment = LLMClusteringExperiment()
                success = experiment.run_experiment(article_limit)
                break
                
            elif mode_input == "2":
                # ì „ì²´ ê¸°ì‚¬ ë°°ì¹˜ ì²˜ë¦¬
                batch_size_input = input("ë°°ì¹˜ í¬ê¸°ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 200): ").strip()
                if not batch_size_input:
                    batch_size = 200
                else:
                    try:
                        batch_size = int(batch_size_input)
                        if batch_size <= 0:
                            print("âŒ 1 ì´ìƒì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                            continue
                    except ValueError:
                        print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        continue
                
                experiment = LLMClusteringExperiment()
                success = experiment.run_batch_experiment(batch_size)
                break
                
            else:
                print("âŒ 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
        
        if success:
            print("\nğŸ‰ ì‹¤í—˜ ì„±ê³µ!")
        else:
            print("\nâŒ ì‹¤í—˜ ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
