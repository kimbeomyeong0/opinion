#!/usr/bin/env python3
"""
ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í•  ê¸°ëŠ¥ì´ ì¶”ê°€ëœ í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤í¬ë¦½íŠ¸
- 100ê°œ ì´ìƒì˜ ëŒ€í˜• í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ìœ„ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„í• 
- í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œ ë° ê³„ì¸µì  ë¶„í•  ì§€ì›
"""

import sys
import os
import json
import numpy as np
from datetime import datetime, date
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from utils.supabase_manager import SupabaseManager

# HDBSCAN ì„¤ì¹˜ í™•ì¸ ë° import
try:
    import hdbscan
except ImportError:
    print("âŒ HDBSCANì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹: pip install hdbscan")
    sys.exit(1)

class ClusterSplitter:
    """í´ëŸ¬ìŠ¤í„° ë¶„í•  ê¸°ëŠ¥ì´ ì¶”ê°€ëœ í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ì„¤ì •
        self.MIN_CLUSTER_SIZE = 5
        self.MIN_SAMPLES = 3
        self.CLUSTER_SELECTION_EPSILON = 0.2
        self.METRIC = 'euclidean'
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ì œí•œ
        self.MAX_CLUSTER_SIZE = 50  # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°
        self.SPLIT_THRESHOLD = 100  # ë¶„í•  ì„ê³„ê°’
        
        # í’ˆì§ˆ ê²€ì¦ ì„ê³„ê°’
        self.MIN_SILHOUETTE_SCORE = 0.3
        self.MIN_CLUSTER_COHERENCE = 0.6
        
        # Supabase ì—°ê²°
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise Exception("Supabase ì—°ê²° ì‹¤íŒ¨")
    
    def fetch_embeddings_data(self) -> tuple:
        """ì„ë² ë”© ë°ì´í„° ì¡°íšŒ ë° í’ˆì§ˆ í•„í„°ë§"""
        try:
            print("ğŸ“¡ ì„ë² ë”© ë°ì´í„° ì¡°íšŒ ì¤‘...")
            
            result = self.supabase_manager.client.table('articles_embeddings').select(
                'id, cleaned_article_id, article_id, media_id, embedding_vector, model_name'
            ).execute()
            
            if not result.data:
                print("âŒ ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None
            
            print(f"âœ… {len(result.data)}ê°œ ì„ë² ë”© ë°ì´í„° ì¡°íšŒ ì™„ë£Œ")
            
            # ë²¡í„° íŒŒì‹± ë° í’ˆì§ˆ í•„í„°ë§
            embeddings = []
            article_ids = []
            article_metadata = []
            
            for item in result.data:
                try:
                    vector = json.loads(item['embedding_vector'])
                    
                    if len(vector) == 1536:
                        vector_norm = np.linalg.norm(vector)
                        if vector_norm > 0.1:
                            embeddings.append(vector)
                            article_ids.append(item['article_id'])
                            article_metadata.append({
                                'id': item['id'],
                                'cleaned_article_id': item['cleaned_article_id'],
                                'article_id': item['article_id'],
                                'media_id': item['media_id']
                            })
                        else:
                            print(f"âš ï¸ í’ˆì§ˆ ë‚®ì€ ë²¡í„° ì œê±° (ê¸°ì‚¬ ID: {item['article_id']})")
                    else:
                        print(f"âš ï¸ ì˜ëª»ëœ ë²¡í„° ì°¨ì›: {len(vector)}ì°¨ì› (ê¸°ì‚¬ ID: {item['article_id']})")
                        
                except Exception as e:
                    print(f"âš ï¸ ë²¡í„° íŒŒì‹± ì‹¤íŒ¨ (ê¸°ì‚¬ ID: {item['article_id']}): {str(e)}")
                    continue
            
            print(f"âœ… {len(embeddings)}ê°œ ê³ í’ˆì§ˆ ë²¡í„° íŒŒì‹± ì™„ë£Œ")
            
            # ë²¡í„° ì •ê·œí™”
            embeddings_array = np.array(embeddings)
            norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)
            normalized_embeddings = embeddings_array / norms
            
            return normalized_embeddings, article_ids, article_metadata
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None, None, None
    
    def perform_hdbscan_clustering(self, embeddings: np.ndarray) -> tuple:
        """HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        try:
            print("ğŸ”„ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
            
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=self.MIN_CLUSTER_SIZE,
                min_samples=self.MIN_SAMPLES,
                cluster_selection_epsilon=self.CLUSTER_SELECTION_EPSILON,
                metric=self.METRIC
            )
            
            cluster_labels = clusterer.fit_predict(embeddings)
            
            # ê²°ê³¼ ë¶„ì„
            unique_labels = np.unique(cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            
            print(f"âœ… HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ:")
            print(f"  - í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}ê°œ")
            print(f"  - ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {n_noise}ê°œ")
            print(f"  - í´ëŸ¬ìŠ¤í„°ë§ ë¹„ìœ¨: {((len(cluster_labels) - n_noise) / len(cluster_labels) * 100):.1f}%")
            
            return cluster_labels, clusterer
            
        except Exception as e:
            print(f"âŒ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return None, None
    
    def split_large_cluster(self, embeddings: np.ndarray, cluster_labels: np.ndarray, 
                           article_ids: List[str], article_metadata: List[Dict], 
                           cluster_id: int) -> tuple:
        """
        ëŒ€í˜• í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ìœ„ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„í• 
        
        Args:
            embeddings: ì „ì²´ ì„ë² ë”© ë²¡í„°
            cluster_labels: í´ëŸ¬ìŠ¤í„° ë¼ë²¨
            article_ids: ê¸°ì‚¬ ID ë¦¬ìŠ¤íŠ¸
            article_metadata: ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°
            cluster_id: ë¶„í• í•  í´ëŸ¬ìŠ¤í„° ID
            
        Returns:
            tuple: (ìƒˆë¡œìš´ ë¼ë²¨, ìƒˆë¡œìš´ ê¸°ì‚¬ ID, ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„°)
        """
        try:
            print(f"ğŸ”€ í´ëŸ¬ìŠ¤í„° {cluster_id} ë¶„í•  ì¤‘... (í¬ê¸°: {list(cluster_labels).count(cluster_id)}ê°œ)")
            
            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ë°ì´í„° ì¶”ì¶œ
            cluster_mask = cluster_labels == cluster_id
            cluster_embeddings = embeddings[cluster_mask]
            cluster_article_ids = [article_ids[i] for i in range(len(article_ids)) if cluster_mask[i]]
            cluster_metadata = [article_metadata[i] for i in range(len(article_metadata)) if cluster_mask[i]]
            
            # ìµœì ì˜ í•˜ìœ„ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
            n_samples = len(cluster_embeddings)
            optimal_clusters = min(max(2, n_samples // self.MAX_CLUSTER_SIZE), 10)  # 2~10ê°œ ì‚¬ì´
            
            print(f"  - {n_samples}ê°œ ìƒ˜í”Œì„ {optimal_clusters}ê°œ í•˜ìœ„ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„í• ")
            
            # K-meansë¡œ í•˜ìœ„ í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)
            sub_labels = kmeans.fit_predict(cluster_embeddings)
            
            # ìƒˆë¡œìš´ ë¼ë²¨ ìƒì„± (ê¸°ì¡´ í´ëŸ¬ìŠ¤í„° IDë¥¼ í•˜ìœ„ í´ëŸ¬ìŠ¤í„° IDë¡œ ë³€ê²½)
            new_labels = cluster_labels.copy()
            new_article_ids = article_ids.copy()
            new_metadata = article_metadata.copy()
            
            # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ID ì°¾ê¸°
            max_existing_label = np.max(cluster_labels[cluster_labels != -1])
            
            # í•˜ìœ„ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ í• ë‹¹
            for i, (original_idx, sub_label) in enumerate(zip(np.where(cluster_mask)[0], sub_labels)):
                new_cluster_id = max_existing_label + 1 + sub_label
                new_labels[original_idx] = new_cluster_id
            
            print(f"  - í´ëŸ¬ìŠ¤í„° {cluster_id} â†’ {optimal_clusters}ê°œ í•˜ìœ„ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„í•  ì™„ë£Œ")
            
            return new_labels, new_article_ids, new_metadata
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° ë¶„í•  ì‹¤íŒ¨: {str(e)}")
            return cluster_labels, article_ids, article_metadata
    
    def split_all_large_clusters(self, embeddings: np.ndarray, cluster_labels: np.ndarray,
                                article_ids: List[str], article_metadata: List[Dict]) -> tuple:
        """
        ëª¨ë“  ëŒ€í˜• í´ëŸ¬ìŠ¤í„°ë¥¼ ë¶„í• 
        """
        try:
            print("ğŸ” ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ê²€ì‚¬ ë° ë¶„í•  ì¤‘...")
            
            current_labels = cluster_labels.copy()
            current_article_ids = article_ids.copy()
            current_metadata = article_metadata.copy()
            
            split_count = 0
            
            while True:
                # ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                large_clusters = []
                for label in np.unique(current_labels):
                    if label == -1:
                        continue
                    
                    cluster_size = list(current_labels).count(label)
                    if cluster_size >= self.SPLIT_THRESHOLD:
                        large_clusters.append((label, cluster_size))
                
                if not large_clusters:
                    break
                
                # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°ë¶€í„° ë¶„í• 
                large_clusters.sort(key=lambda x: x[1], reverse=True)
                largest_cluster_id, largest_size = large_clusters[0]
                
                print(f"  - ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë°œê²¬: ID {largest_cluster_id} (í¬ê¸°: {largest_size}ê°œ)")
                
                # ë¶„í•  ìˆ˜í–‰
                current_labels, current_article_ids, current_metadata = self.split_large_cluster(
                    embeddings, current_labels, current_article_ids, current_metadata, largest_cluster_id
                )
                
                split_count += 1
                
                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if split_count > 10:
                    print("âš ï¸ ë¶„í•  íšŸìˆ˜ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                    break
            
            print(f"âœ… ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í•  ì™„ë£Œ: {split_count}ê°œ í´ëŸ¬ìŠ¤í„° ë¶„í• ë¨")
            
            return current_labels, current_article_ids, current_metadata
            
        except Exception as e:
            print(f"âŒ ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í•  ì‹¤íŒ¨: {str(e)}")
            return cluster_labels, article_ids, article_metadata
    
    def evaluate_cluster_quality(self, embeddings: np.ndarray, cluster_labels: np.ndarray) -> Dict[str, float]:
        """í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€"""
        try:
            # ë…¸ì´ì¦ˆ ì œê±°
            valid_mask = cluster_labels != -1
            if np.sum(valid_mask) < 2:
                return {'silhouette_score': 0.0, 'avg_coherence': 0.0}
            
            valid_embeddings = embeddings[valid_mask]
            valid_labels = cluster_labels[valid_mask]
            
            # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
            if len(np.unique(valid_labels)) > 1:
                silhouette = silhouette_score(valid_embeddings, valid_labels)
            else:
                silhouette = 0.0
            
            # í´ëŸ¬ìŠ¤í„° ë‚´ ì¼ê´€ì„± ê³„ì‚°
            coherence_scores = []
            for label in np.unique(valid_labels):
                if label == -1:
                    continue
                
                cluster_mask = valid_labels == label
                cluster_embeddings = valid_embeddings[cluster_mask]
                
                if len(cluster_embeddings) > 1:
                    centroid = np.mean(cluster_embeddings, axis=0)
                    distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)
                    avg_distance = np.mean(distances)
                    coherence = max(0, 1 - avg_distance)
                    coherence_scores.append(coherence)
            
            avg_coherence = np.mean(coherence_scores) if coherence_scores else 0.0
            
            return {
                'silhouette_score': silhouette,
                'avg_coherence': avg_coherence
            }
            
        except Exception as e:
            print(f"âš ï¸ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            return {'silhouette_score': 0.0, 'avg_coherence': 0.0}
    
    def filter_low_quality_clusters(self, embeddings: np.ndarray, cluster_labels: np.ndarray, 
                                  article_ids: List[str], article_metadata: List[Dict]) -> tuple:
        """ì €í’ˆì§ˆ í´ëŸ¬ìŠ¤í„° í•„í„°ë§"""
        try:
            print("ğŸ” ì €í’ˆì§ˆ í´ëŸ¬ìŠ¤í„° í•„í„°ë§ ì¤‘...")
            
            filtered_labels = cluster_labels.copy()
            
            # ê° í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
            for label in np.unique(cluster_labels):
                if label == -1:  # ë…¸ì´ì¦ˆëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                    continue
                
                cluster_mask = cluster_labels == label
                cluster_embeddings = embeddings[cluster_mask]
                
                if len(cluster_embeddings) < 3:  # ë„ˆë¬´ ì‘ì€ í´ëŸ¬ìŠ¤í„°ëŠ” ë…¸ì´ì¦ˆë¡œ ë³€ê²½
                    filtered_labels[cluster_mask] = -1
                    continue
                
                # í´ëŸ¬ìŠ¤í„° ë‚´ ì¼ê´€ì„± ê²€ì‚¬
                centroid = np.mean(cluster_embeddings, axis=0)
                distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)
                max_distance = np.max(distances)
                
                if max_distance > 0.8:  # ë„ˆë¬´ ë¶„ì‚°ëœ í´ëŸ¬ìŠ¤í„°ëŠ” ë…¸ì´ì¦ˆë¡œ ë³€ê²½
                    filtered_labels[cluster_mask] = -1
                    print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {label} ì œê±° (ì¼ê´€ì„± ë¶€ì¡±)")
            
            # í•„í„°ë§ ê²°ê³¼ ë¶„ì„
            original_clusters = len(np.unique(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            filtered_clusters = len(np.unique(filtered_labels)) - (1 if -1 in filtered_labels else 0)
            original_noise = list(cluster_labels).count(-1)
            filtered_noise = list(filtered_labels).count(-1)
            
            print(f"âœ… í•„í„°ë§ ì™„ë£Œ:")
            print(f"  - ì›ë³¸ í´ëŸ¬ìŠ¤í„°: {original_clusters}ê°œ â†’ í•„í„°ë§ í›„: {filtered_clusters}ê°œ")
            print(f"  - ì›ë³¸ ë…¸ì´ì¦ˆ: {original_noise}ê°œ â†’ í•„í„°ë§ í›„: {filtered_noise}ê°œ")
            
            return filtered_labels, article_ids, article_metadata
            
        except Exception as e:
            print(f"âŒ í•„í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return cluster_labels, article_ids, article_metadata
    
    def clear_existing_data(self) -> bool:
        """ê¸°ì¡´ ì´ìŠˆ ë°ì´í„° ì´ˆê¸°í™”"""
        try:
            print("ğŸ—‘ï¸ ê¸°ì¡´ ì´ìŠˆ ë°ì´í„° ì´ˆê¸°í™” ì¤‘...")
            
            self.supabase_manager.client.table('issue_articles').delete().neq('issue_id', '00000000-0000-0000-0000-000000000000').execute()
            self.supabase_manager.client.table('issues').delete().neq('id', '00000000-0000-0000-0000-000000000000').execute()
            
            print("âœ… ê¸°ì¡´ ì´ìŠˆ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def analyze_media_bias(self, article_metadata: List[Dict]) -> Dict[str, str]:
        """ì„±í–¥ë³„ ì–¸ë¡ ì‚¬ ì •ë³´ ë¶„ì„"""
        try:
            media_bias_map = {
                '755f5a31-507f-42d7-aa8c-587a9459896c': 'right',  # ì¡°ì„ ì¼ë³´
                '629d6050-76e2-466a-ae66-f2532d1f359c': 'right',  # ì¤‘ì•™ì¼ë³´
                'afcd9fc8-e4fd-44c7-8d9d-59722bb21b26': 'right',  # ë™ì•„ì¼ë³´
                'ea42a075-88e0-4c6e-a21b-8854ec10dec9': 'left',   # í•œê²¨ë ˆ
                '81324c3e-5f68-4356-bc91-bd5c7719f5c9': 'left',   # ì˜¤ë§ˆì´ë‰´ìŠ¤
                '3847c39d-bc90-44e5-8650-331e67cbe140': 'left',   # ê²½í–¥ì‹ ë¬¸
                '33e32516-abb5-46ca-b87d-306304a61c34': 'center', # ì—°í•©ë‰´ìŠ¤
                'a29afc6a-1764-43fe-8b0e-3a5962d90402': 'center', # ë‰´ì‹œìŠ¤
                'a8fecf98-41ac-4018-85e8-19bfeb702fe5': 'center'  # ë‰´ìŠ¤ì›
            }
            
            media_counts = {}
            for article in article_metadata:
                media_id = article['media_id']
                if media_id in media_bias_map:
                    bias = media_bias_map[media_id]
                    if bias not in media_counts:
                        media_counts[bias] = 0
                    media_counts[bias] += 1
            
            left_count = media_counts.get('left', 0)
            center_count = media_counts.get('center', 0)
            right_count = media_counts.get('right', 0)
            total_count = left_count + center_count + right_count
            
            return {
                'total_source': str(total_count),
                'left_source': str(left_count),
                'center_source': str(center_count),
                'right_source': str(right_count)
            }
            
        except Exception as e:
            print(f"âš ï¸ ì„±í–¥ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                'total_source': '0',
                'left_source': '0',
                'center_source': '0',
                'right_source': '0'
            }
    
    def create_issue(self, cluster_id: int, article_ids: List[str], article_metadata: List[Dict]) -> Optional[str]:
        """ì´ìŠˆ ìƒì„± ë° ì €ì¥"""
        try:
            source_info = self.analyze_media_bias(article_metadata)
            
            issue_data = {
                'date': date.today().isoformat(),
                'title': f'ì´ìŠˆ {cluster_id + 1}',
                'summary': f'{len(article_ids)}ê°œ ê¸°ì‚¬ë¡œ êµ¬ì„±ëœ ì´ìŠˆ',
                'subtitle': f'í´ëŸ¬ìŠ¤í„° {cluster_id + 1}',
                'source': source_info['total_source'],
                'left_source': source_info['left_source'],
                'center_source': source_info['center_source'],
                'right_source': source_info['right_source'],
                'created_at': datetime.now().isoformat()
            }
            
            result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
            
            if result.data:
                issue_id = result.data[0]['id']
                print(f"âœ… ì´ìŠˆ {cluster_id + 1} ìƒì„± ì™„ë£Œ (ID: {issue_id}) - {len(article_ids)}ê°œ ê¸°ì‚¬")
                return issue_id
            else:
                print(f"âŒ ì´ìŠˆ {cluster_id + 1} ìƒì„± ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            print(f"âŒ ì´ìŠˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None
    
    def create_issue_articles(self, issue_id: str, article_ids: List[str], article_metadata: List[Dict]) -> bool:
        """ì´ìŠˆ-ê¸°ì‚¬ ì—°ê²° ìƒì„±"""
        try:
            connections = []
            for i, article_id in enumerate(article_ids):
                cleaned_article_id = None
                for metadata in article_metadata:
                    if metadata['article_id'] == article_id:
                        cleaned_article_id = metadata['cleaned_article_id']
                        break
                
                connections.append({
                    'issue_id': issue_id,
                    'article_id': article_id,
                    'cleaned_article_id': cleaned_article_id,
                    'stance': 'center'
                })
            
            result = self.supabase_manager.client.table('issue_articles').insert(connections).execute()
            
            if result.data:
                print(f"âœ… {len(connections)}ê°œ ì—°ê²° ìƒì„± ì™„ë£Œ")
                return True
            else:
                print(f"âŒ ì—°ê²° ìƒì„± ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì—°ê²° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def run_clustering_with_split(self) -> bool:
        """ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í•  ê¸°ëŠ¥ì´ í¬í•¨ëœ í´ëŸ¬ìŠ¤í„°ë§"""
        try:
            print("ğŸš€ ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í•  ê¸°ëŠ¥ì´ í¬í•¨ëœ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
            
            # 1. ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™”
            if not self.clear_existing_data():
                return False
            
            # 2. ì„ë² ë”© ë°ì´í„° ì¡°íšŒ
            embeddings, article_ids, article_metadata = self.fetch_embeddings_data()
            if embeddings is None:
                return False
            
            # 3. ì´ˆê¸° í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            cluster_labels, clusterer = self.perform_hdbscan_clustering(embeddings)
            if cluster_labels is None:
                return False
            
            # 4. ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í• 
            cluster_labels, article_ids, article_metadata = self.split_all_large_clusters(
                embeddings, cluster_labels, article_ids, article_metadata
            )
            
            # 5. í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
            quality_metrics = self.evaluate_cluster_quality(embeddings, cluster_labels)
            print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ:")
            print(f"  - ì‹¤ë£¨ì—£ ì ìˆ˜: {quality_metrics['silhouette_score']:.3f}")
            print(f"  - í‰ê·  ì¼ê´€ì„±: {quality_metrics['avg_coherence']:.3f}")
            
            # 6. ì €í’ˆì§ˆ í´ëŸ¬ìŠ¤í„° í•„í„°ë§
            cluster_labels, article_ids, article_metadata = self.filter_low_quality_clusters(
                embeddings, cluster_labels, article_ids, article_metadata
            )
            
            # 7. í´ëŸ¬ìŠ¤í„°ë³„ ì´ìŠˆ ìƒì„±
            unique_labels = np.unique(cluster_labels)
            created_issues = 0
            failed_issues = 0
            
            # í´ëŸ¬ìŠ¤í„° í¬ê¸° í†µê³„
            cluster_sizes = []
            
            for label in unique_labels:
                if label == -1:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ê±´ë„ˆë›°ê¸°
                    continue
                
                cluster_mask = cluster_labels == label
                cluster_article_ids = [article_ids[i] for i in range(len(article_ids)) if cluster_mask[i]]
                cluster_metadata = [article_metadata[i] for i in range(len(article_metadata)) if cluster_mask[i]]
                
                cluster_sizes.append(len(cluster_article_ids))
                
                issue_id = self.create_issue(label, cluster_article_ids, cluster_metadata)
                
                if issue_id:
                    if self.create_issue_articles(issue_id, cluster_article_ids, cluster_metadata):
                        created_issues += 1
                    else:
                        failed_issues += 1
                else:
                    failed_issues += 1
            
            # ìµœì¢… í†µê³„
            print(f"\nğŸ“Š ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
            print(f"  - ìƒì„±ëœ ì´ìŠˆ: {created_issues}ê°œ")
            print(f"  - ì‹¤íŒ¨í•œ ì´ìŠˆ: {failed_issues}ê°œ")
            print(f"  - ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {list(cluster_labels).count(-1)}ê°œ")
            print(f"  - í´ëŸ¬ìŠ¤í„°ë§ ë¹„ìœ¨: {((len(cluster_labels) - list(cluster_labels).count(-1)) / len(cluster_labels) * 100):.1f}%")
            
            if cluster_sizes:
                print(f"  - í´ëŸ¬ìŠ¤í„° í¬ê¸° í†µê³„:")
                print(f"    * í‰ê·  í¬ê¸°: {np.mean(cluster_sizes):.1f}ê°œ")
                print(f"    * ìµœëŒ€ í¬ê¸°: {np.max(cluster_sizes)}ê°œ")
                print(f"    * ìµœì†Œ í¬ê¸°: {np.min(cluster_sizes)}ê°œ")
                print(f"    * 50ê°œ ì´ìƒ í´ëŸ¬ìŠ¤í„°: {sum(1 for size in cluster_sizes if size >= 50)}ê°œ")
            
            return created_issues > 0
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {str(e)}")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ”® ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í•  ê¸°ëŠ¥ì´ í¬í•¨ëœ í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    
    try:
        # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        clusterer = ClusterSplitter()
        success = clusterer.run_clustering_with_split()
        
        if success:
            print("\nâœ… ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í•  í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        else:
            print("\nâŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()

