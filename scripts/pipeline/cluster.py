#!/usr/bin/env python3
"""
ì „ì²´ ì •ì¹˜ ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤í¬ë¦½íŠ¸
- 8ê°œ ì •ì¹˜ ì¹´í…Œê³ ë¦¬ë³„ ë…ë¦½ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
- LLM ê¸°ë°˜ ë™ì  ì‚¬ê±´ íŒ¨í„´ ìƒì„±
- UMAP ì°¨ì›ì¶•ì†Œ + HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
- ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„°ë¥¼ issues í…Œì´ë¸”ì— ì €ì¥ (20ê°œ ê¸°ì‚¬ ì´ìƒë§Œ)
- í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬: ëŒ€ìš©ëŸ‰ ìˆœì°¨, ì†ŒëŸ‰ ë³‘ë ¬
"""

import sys
import os
import json
import numpy as np
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from utils.supabase_manager import SupabaseManager
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn

console = Console()

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    import umap
    import hdbscan
    from openai import OpenAI
except ImportError as e:
    console.print(f"âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    console.print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
    console.print("pip install umap-learn hdbscan scikit-learn openai")
    sys.exit(1)


class MultiCategoryClusterer:
    """ì „ì²´ ì •ì¹˜ ì¹´í…Œê³ ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise Exception("Supabase ì—°ê²° ì‹¤íŒ¨")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.openai_client = OpenAI()
            console.print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            console.print(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise Exception("OpenAI ì—°ê²° ì‹¤íŒ¨")
        
        # ì •ì¹˜ ì¹´í…Œê³ ë¦¬ ì •ì˜
        self.categories = {
            # ëŒ€ìš©ëŸ‰ ì¹´í…Œê³ ë¦¬ (ìˆœì°¨ ì²˜ë¦¬)
            "large": ["í–‰ì •ë¶€", "ì‚¬ë²•/ê²€ì°°", "ê¸°íƒ€", "êµ­íšŒ/ì •ë‹¹", "ì™¸êµ/ì•ˆë³´"],
            # ì†ŒëŸ‰ ì¹´í…Œê³ ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)  
            "small": ["ì •ì±…/ê²½ì œì‚¬íšŒ", "ì„ ê±°", "ì§€ì—­ì •ì¹˜"]
        }
        
        console.print("âœ… MultiCategoryClusterer ì´ˆê¸°í™” ì™„ë£Œ")
        console.print(f"ğŸ“Š ëŒ€ìš©ëŸ‰ ì¹´í…Œê³ ë¦¬: {len(self.categories['large'])}ê°œ")
        console.print(f"ğŸ“Š ì†ŒëŸ‰ ì¹´í…Œê³ ë¦¬: {len(self.categories['small'])}ê°œ")
    
    def fetch_articles_by_category(self, category: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ë“¤ê³¼ ì„ë² ë”© ì¡°íšŒ"""
        try:
            console.print(f"ğŸ” {category} ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ ì¡°íšŒ ì¤‘...")
            
            all_articles = []
            page_size = 1000
            offset = 0
            
            while True:
                result = self.supabase_manager.client.table('articles').select(
                    'id, title, media_id, political_category, embedding, published_at'
                ).eq('political_category', category).not_.is_('embedding', 'null').range(
                    offset, offset + page_size - 1
                ).execute()
                
                if not result.data:
                    break
                    
                all_articles.extend(result.data)
                
                if len(result.data) < page_size:
                    break
                    
                offset += page_size
                console.print(f"ğŸ“„ í˜ì´ì§€ ì¡°íšŒ ì¤‘... {len(all_articles)}ê°œ ìˆ˜ì§‘ë¨")
            
            console.print(f"âœ… {category} ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ {len(all_articles)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return all_articles
            
        except Exception as e:
            console.print(f"âŒ {category} ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def extract_embeddings(self, articles: List[Dict[str, Any]]) -> Tuple[np.ndarray, List[Dict[str, Any]]]:
        """ì„ë² ë”© ë²¡í„° ì¶”ì¶œ ë° ìœ íš¨í•œ ê¸°ì‚¬ í•„í„°ë§"""
        try:
            console.print("ğŸ”„ ì„ë² ë”© ë²¡í„° ì¶”ì¶œ ì¤‘...")
            
            valid_articles = []
            embeddings = []
            
            for article in articles:
                try:
                    embedding_json = article.get('embedding')
                    if embedding_json:
                        embedding_vector = json.loads(embedding_json)
                        if isinstance(embedding_vector, list) and len(embedding_vector) > 0:
                            embeddings.append(embedding_vector)
                            valid_articles.append(article)
                except Exception as e:
                    console.print(f"âš ï¸ ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨: {article.get('id', 'Unknown')} - {str(e)}")
                    continue
            
            embeddings_array = np.array(embeddings)
            console.print(f"âœ… ìœ íš¨í•œ ì„ë² ë”© {len(valid_articles)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
            console.print(f"ğŸ“Š ì„ë² ë”© ì°¨ì›: {embeddings_array.shape}")
            
            return embeddings_array, valid_articles
            
        except Exception as e:
            console.print(f"âŒ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return np.array([]), []
    
    def perform_umap_reduction(self, embeddings: np.ndarray) -> np.ndarray:
        """UMAP ì°¨ì›ì¶•ì†Œ ìˆ˜í–‰"""
        try:
            console.print("ğŸ”„ UMAP ì°¨ì›ì¶•ì†Œ ìˆ˜í–‰ ì¤‘...")
            console.print(f"âš™ï¸ ì„¤ì •: n_neighbors=30, n_components=15, min_dist=0.1, metric=cosine")
            
            # UMAP íŒŒë¼ë¯¸í„° ì„¤ì •
            umap_reducer = umap.UMAP(
                n_neighbors=30,
                n_components=15,
                min_dist=0.1,
                metric='cosine',
                random_state=42,
                n_jobs=1  # ì•ˆì •ì„±ì„ ìœ„í•´ ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‚¬ìš©
            )
            
            # ì°¨ì›ì¶•ì†Œ ìˆ˜í–‰
            reduced_embeddings = umap_reducer.fit_transform(embeddings)
            
            console.print(f"âœ… UMAP ì°¨ì›ì¶•ì†Œ ì™„ë£Œ: {embeddings.shape} â†’ {reduced_embeddings.shape}")
            return reduced_embeddings
            
        except Exception as e:
            console.print(f"âŒ UMAP ì°¨ì›ì¶•ì†Œ ì‹¤íŒ¨: {str(e)}")
            return np.array([])
    
    def perform_hdbscan_clustering(self, reduced_embeddings: np.ndarray) -> np.ndarray:
        """HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        try:
            console.print("ğŸ”„ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
            console.print(f"âš™ï¸ ì„¤ì •: min_cluster_size=15, min_samples=6, cluster_selection_method=eom")
            
            # HDBSCAN íŒŒë¼ë¯¸í„° ì„¤ì •
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=15,
                min_samples=6,
                cluster_selection_method='eom',
                metric='euclidean'
            )
            
            # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            cluster_labels = clusterer.fit_predict(reduced_embeddings)
            
            # í´ëŸ¬ìŠ¤í„° í†µê³„
            unique_labels = np.unique(cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # -1ì€ ë…¸ì´ì¦ˆ
            n_noise = np.sum(cluster_labels == -1)
            
            console.print(f"âœ… HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ")
            console.print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}ê°œ")
            console.print(f"ğŸ“Š ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {n_noise}ê°œ")
            
            # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° ì¶œë ¥
            for label in unique_labels:
                if label != -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                    cluster_size = np.sum(cluster_labels == label)
                    console.print(f"   í´ëŸ¬ìŠ¤í„° {label}: {cluster_size}ê°œ ê¸°ì‚¬")
            
            return cluster_labels
            
        except Exception as e:
            console.print(f"âŒ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return np.array([])
    
    def get_media_bias_mapping(self) -> Dict[str, str]:
        """ì–¸ë¡ ì‚¬ IDë³„ bias ë§¤í•‘ ì¡°íšŒ"""
        try:
            result = self.supabase_manager.client.table('media_outlets').select('id, bias').execute()
            
            bias_mapping = {}
            for outlet in result.data:
                bias_mapping[outlet['id']] = outlet['bias']
            
            console.print(f"âœ… ì–¸ë¡ ì‚¬ bias ë§¤í•‘ {len(bias_mapping)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return bias_mapping
            
        except Exception as e:
            console.print(f"âŒ ì–¸ë¡ ì‚¬ bias ë§¤í•‘ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def generate_dynamic_patterns_with_llm(self, cluster_articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """LLMì„ í™œìš©í•œ ë™ì  ì‚¬ê±´ íŒ¨í„´ ìƒì„±"""
        try:
            console.print(f"ğŸ¤– LLMìœ¼ë¡œ {len(cluster_articles)}ê°œ ê¸°ì‚¬ì˜ ì‚¬ê±´ íŒ¨í„´ ë¶„ì„ ì¤‘...")
            
            # ê¸°ì‚¬ ì œëª©ë“¤ ìˆ˜ì§‘ (ìµœëŒ€ 50ê°œë¡œ ì œí•œí•˜ì—¬ í† í° ì ˆì•½)
            titles = []
            for article in cluster_articles[:50]:
                title = article.get('title', '')
                if title:
                    titles.append(title)
            
            if not titles:
                return {}
            
            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            titles_text = '\n'.join([f"{i+1}. {title}" for i, title in enumerate(titles)])
            
            prompt = f"""ë‹¤ìŒ {len(titles)}ê°œì˜ í•œêµ­ ì •ì¹˜ ê¸°ì‚¬ ì œëª©ë“¤ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” ì •ì¹˜ ì‚¬ê±´ë“¤ì„ ì‹ë³„í•˜ê³  ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

{titles_text}

ìš”êµ¬ì‚¬í•­:
1. 3-6ê°œì˜ ì£¼ìš” ì •ì¹˜ ì‚¬ê±´ì„ ì‹ë³„í•˜ì„¸ìš”
2. ê° ì‚¬ê±´ë³„ë¡œ í•µì‹¬ í‚¤ì›Œë“œ 3-4ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”  
3. ê° ì‚¬ê±´ì˜ ì •í™•í•œ ëª…ì¹­ì„ 20ì ë‚´ì™¸ë¡œ ì‘ì„±í•˜ì„¸ìš”
4. ëª…ë°±íˆ ë‹¤ë¥¸ ì£¼ì œì˜ ê¸°ì‚¬ëŠ” 'ê¸°íƒ€'ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”

JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "events": [
    {{
      "event_id": "ì‚¬ê±´_ì‹ë³„ì",
      "title": "20ì ë‚´ì™¸ ì‚¬ê±´ëª…",
      "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
      "description": "ì‚¬ê±´ ê°„ë‹¨ ì„¤ëª…"
    }}
  ]
}}"""

            # OpenAI API í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ ì •ì¹˜ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ê°ê´€ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                max_tokens=1000
            )
            
            # JSON íŒŒì‹±
            import json
            try:
                llm_response = response.choices[0].message.content
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json íƒœê·¸ ì œê±°)
                if "```json" in llm_response:
                    json_start = llm_response.find("```json") + 7
                    json_end = llm_response.find("```", json_start)
                    llm_response = llm_response[json_start:json_end]
                elif "```" in llm_response:
                    json_start = llm_response.find("```") + 3
                    json_end = llm_response.rfind("```")
                    llm_response = llm_response[json_start:json_end]
                
                patterns = json.loads(llm_response.strip())
                console.print(f"âœ… LLM íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: {len(patterns.get('events', []))}ê°œ ì‚¬ê±´ ì‹ë³„")
                
                return patterns
                
            except json.JSONDecodeError as e:
                console.print(f"âŒ LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                console.print(f"ì›ë³¸ ì‘ë‹µ: {llm_response[:200]}...")
                return {}
                
        except Exception as e:
            console.print(f"âŒ LLM íŒ¨í„´ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {}

    def create_subgroups_within_cluster(self, cluster_articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """LLM ê¸°ë°˜ ë™ì  ì†Œê·¸ë£¹ ìƒì„± (í˜ì‹ ì  2.1ë‹¨ê³„)"""
        try:
            console.print(f"ğŸ“ {len(cluster_articles)}ê°œ ê¸°ì‚¬ì˜ LLM ê¸°ë°˜ ì†Œê·¸ë£¹ ìƒì„± ì¤‘...")
            
            # LLMìœ¼ë¡œ ë™ì  íŒ¨í„´ ìƒì„±
            llm_patterns = self.generate_dynamic_patterns_with_llm(cluster_articles)
            
            if not llm_patterns or 'events' not in llm_patterns:
                console.print("âš ï¸ LLM íŒ¨í„´ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ê·¸ë£¹í™”ë¡œ ëŒ€ì²´")
                return self._fallback_grouping(cluster_articles)
            
            # LLMì´ ìƒì„±í•œ íŒ¨í„´ìœ¼ë¡œ ë¶„ë¥˜
            event_patterns = {}
            for event in llm_patterns['events']:
                event_id = event.get('event_id', f"event_{len(event_patterns)}")
                event_patterns[event_id] = {
                    'keywords': event.get('keywords', []),
                    'title': event.get('title', 'ì •ì¹˜ ì´ìŠˆ')
                }
            
            console.print(f"ğŸ¯ LLM ìƒì„± íŒ¨í„´: {list(event_patterns.keys())}")
            
            # ì‚¬ê±´ë³„ ê·¸ë£¹ ë¶„ë¥˜
            event_groups = {event: [] for event in event_patterns.keys()}
            noise_articles = []
            
            for article in cluster_articles:
                title = article.get('title', '').lower()
                matched_events = []
                
                # ê° LLM ìƒì„± íŒ¨í„´ê³¼ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                for event_name, pattern_info in event_patterns.items():
                    score = 0
                    for keyword in pattern_info["keywords"]:
                        if keyword.lower() in title:
                            score += 1
                    
                    if score >= 2:  # ìµœì†Œ 2ê°œ í‚¤ì›Œë“œ ë§¤ì¹­
                        matched_events.append((event_name, score))
                
                # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì‚¬ê±´ì— ë°°ì •
                if matched_events:
                    best_event = max(matched_events, key=lambda x: x[1])[0]
                    event_groups[best_event].append(article)
                else:
                    noise_articles.append(article)
            
            # ì†Œê·¸ë£¹ ìƒì„±
            subgroups = []
            
            # LLM íŒ¨í„´ ê¸°ë°˜ ê·¸ë£¹ ì¶”ê°€ (ìµœì†Œ 3ê°œ ì´ìƒì¸ ê²ƒë§Œ)
            for event_name, articles in event_groups.items():
                if len(articles) >= 3:
                    subgroups.append({
                        'subgroup_id': event_name,
                        'articles': articles,
                        'article_count': len(articles),
                        'event_type': event_name,
                        'predefined_title': event_patterns[event_name]["title"]
                    })
                    console.print(f"   âœ… {event_name}: {len(articles)}ê°œ ê¸°ì‚¬ - '{event_patterns[event_name]['title']}'")
            
            # ë…¸ì´ì¦ˆ ê¸°ì‚¬ë“¤ì€ ê°ê° ê°œë³„ ì†Œê·¸ë£¹ìœ¼ë¡œ
            for i, article in enumerate(noise_articles):
                subgroups.append({
                    'subgroup_id': f"noise_{i}",
                    'articles': [article],
                    'article_count': 1,
                    'event_type': "noise",
                    'predefined_title': None
                })
            
            # ì‚¬ê±´ë³„ ê·¸ë£¹ì—ì„œ 3ê°œ ë¯¸ë§Œì¸ ê²ƒë“¤ë„ ê°œë³„ ì†Œê·¸ë£¹ìœ¼ë¡œ
            for event_name, articles in event_groups.items():
                if 0 < len(articles) < 3:
                    for i, article in enumerate(articles):
                        subgroups.append({
                            'subgroup_id': f"small_{event_name}_{i}",
                            'articles': [article],
                            'article_count': 1,
                            'event_type': "small_group",
                            'predefined_title': None
                        })
            
            # í†µê³„ ì¶œë ¥
            major_groups = [sg for sg in subgroups if sg['article_count'] >= 3]
            individual_groups = [sg for sg in subgroups if sg['article_count'] == 1]
            
            console.print(f"âœ… LLM ê¸°ë°˜ ì†Œê·¸ë£¹ ìƒì„± ì™„ë£Œ:")
            console.print(f"   - ì£¼ìš” ì‚¬ê±´ ê·¸ë£¹: {len(major_groups)}ê°œ")
            console.print(f"   - ê°œë³„ ê¸°ì‚¬ ê·¸ë£¹: {len(individual_groups)}ê°œ")
            
            return subgroups
            
        except Exception as e:
            console.print(f"âŒ LLM ì†Œê·¸ë£¹ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return self._fallback_grouping(cluster_articles)
    
    def _fallback_grouping(self, cluster_articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """LLM ì‹¤íŒ¨ ì‹œ ë°±ì—… ê·¸ë£¹í™”"""
        console.print("ğŸ”„ ë°±ì—… ê·¸ë£¹í™” ì‹œìŠ¤í…œ ì‚¬ìš©")
        subgroups = []
        for i, article in enumerate(cluster_articles):
            subgroups.append({
                'subgroup_id': f"fallback_{i}",
                'articles': [article],
                'article_count': 1,
                'event_type': "fallback",
                'predefined_title': None
            })
        return subgroups
    
    def generate_subgroup_headlines(self, subgroups: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê° ì†Œê·¸ë£¹ë³„ ì •í™•í•œ í—¤ë“œë¼ì¸ ìƒì„± (ê°œì„ ëœ 2.2ë‹¨ê³„)"""
        try:
            console.print(f"ğŸ“ {len(subgroups)}ê°œ ì†Œê·¸ë£¹ í—¤ë“œë¼ì¸ ìƒì„± ì¤‘...")
            
            for subgroup in subgroups:
                # ì‚¬ì „ ì •ì˜ëœ ì œëª©ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                if subgroup.get('predefined_title'):
                    subgroup['title'] = subgroup['predefined_title']
                else:
                    # ê°œë³„ ê¸°ì‚¬ë‚˜ ì†Œê·œëª¨ ê·¸ë£¹ì€ ì œëª© ê°„ì†Œí™”
                    articles = subgroup['articles']
                    if len(articles) == 1:
                        original_title = articles[0].get('title', '')
                        simplified_title = self._simplify_single_title(original_title)
                        subgroup['title'] = simplified_title
                    else:
                        # ì—¬ëŸ¬ ê¸°ì‚¬ì§€ë§Œ ì‚¬ì „ íŒ¨í„´ì— ì—†ëŠ” ê²½ìš°
                        titles = [article.get('title', '') for article in articles]
                        subgroup['title'] = self._create_custom_headline(titles)
                
            console.print("âœ… ì†Œê·¸ë£¹ í—¤ë“œë¼ì¸ ìƒì„± ì™„ë£Œ")
            return subgroups
            
        except Exception as e:
            console.print(f"âŒ ì†Œê·¸ë£¹ í—¤ë“œë¼ì¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return subgroups
    
    def _simplify_single_title(self, title: str) -> str:
        """ë‹¨ì¼ ê¸°ì‚¬ ì œëª© ê°„ì†Œí™”"""
        import re
        
        if not title:
            return "ì •ì¹˜ ì´ìŠˆ"
        
        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
        simplified = title
        
        # ë”°ì˜´í‘œ ë‚´ìš© ì œê±°
        simplified = re.sub(r'["""].*?["""]', '', simplified)
        
        # ê´„í˜¸ ë‚´ìš© ì œê±°  
        simplified = re.sub(r'\([^)]*\)', '', simplified)
        
        # ì—°ì† ê³µë°± ì •ë¦¬
        simplified = re.sub(r'\s+', ' ', simplified).strip()
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ê°„ì†Œí™”
        if 'ëŒ€í†µë ¹ì‹¤' in simplified and 'ë…¼ì˜' in simplified:
            return "ëŒ€í†µë ¹ì‹¤ ì…ì¥ ë°œí‘œ"
        elif 'ì¡°í¬ëŒ€' in simplified and 'ì‚¬í‡´' in simplified:
            return "ì¡°í¬ëŒ€ ì‚¬í‡´ ë…¼ë€"
        elif 'ì„¸ì¢…' in simplified and 'ì§‘ë¬´ì‹¤' in simplified:
            return "ì„¸ì¢… ì§‘ë¬´ì‹¤ ì´ì „"
        elif 'ê·œì œ' in simplified and ('ë°°ì„ì£„' in simplified or 'í•©ë¦¬í™”' in simplified):
            return "ê·œì œí•©ë¦¬í™” ì¶”ì§„"
        elif 'í•œë¯¸' in simplified and 'ê´€ì„¸' in simplified:
            return "í•œë¯¸ ê´€ì„¸í˜‘ìƒ"
        elif 'ë‚´ê°' in simplified and 'êµ¬ì„±' in simplified:
            return "ë‚´ê° êµ¬ì„±"
        
        # 20ì ë‚´ì™¸ë¡œ ì¡°ì •
        if len(simplified) > 20:
            simplified = simplified[:18] + ".."
        
        return simplified if simplified else "ì •ì¹˜ ì´ìŠˆ"
    
    def _create_custom_headline(self, titles: List[str]) -> str:
        """ì‚¬ì „ íŒ¨í„´ì— ì—†ëŠ” ê·¸ë£¹ì˜ í—¤ë“œë¼ì¸ ìƒì„±"""
        import re
        from collections import Counter
        
        all_text = ' '.join(titles)
        
        # ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£]{2,}', all_text)
        word_counts = Counter(words)
        
        # ìƒìœ„ 2ê°œ í‚¤ì›Œë“œë¡œ í—¤ë“œë¼ì¸ ìƒì„±
        top_words = [word for word, count in word_counts.most_common(2) if count >= 2]
        
        if len(top_words) >= 2:
            headline = f"{top_words[0]} {top_words[1]}"
        elif len(top_words) == 1:
            headline = f"{top_words[0]} ì´ìŠˆ"
        else:
            headline = "ì •ì¹˜ í˜„ì•ˆ"
        
        # 20ì ë‚´ì™¸ë¡œ ì¡°ì •
        if len(headline) > 20:
            headline = headline[:18] + ".."
        
        return headline
    
    def merge_similar_subgroups(self, all_subgroups: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì‚¬ê±´ ê¸°ë°˜ ì—„ê²©í•œ ì¬ë³‘í•© (ê°œì„ ëœ 2.3ë‹¨ê³„)"""
        try:
            console.print(f"ğŸ”„ {len(all_subgroups)}ê°œ ì†Œê·¸ë£¹ ì¬ë³‘í•© ì¤‘...")
            
            # ì£¼ìš” ì‚¬ê±´ ê·¸ë£¹ê³¼ ê°œë³„ ê·¸ë£¹ ë¶„ë¦¬
            major_groups = [sg for sg in all_subgroups if sg['article_count'] >= 3]
            individual_groups = [sg for sg in all_subgroups if sg['article_count'] < 3]
            
            console.print(f"   - ì£¼ìš” ê·¸ë£¹: {len(major_groups)}ê°œ")
            console.print(f"   - ê°œë³„ ê·¸ë£¹: {len(individual_groups)}ê°œ")
            
            # ì£¼ìš” ê·¸ë£¹ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì´ë¯¸ ëª…í™•í•œ ì‚¬ê±´ë³„ë¡œ ë¶„ë¥˜ë¨)
            final_groups = []
            
            # ì£¼ìš” ì‚¬ê±´ ê·¸ë£¹ë“¤ ì¶”ê°€
            for group in major_groups:
                final_groups.append({
                    'group_id': group['subgroup_id'],
                    'articles': group['articles'],
                    'article_count': group['article_count'],
                    'title': group.get('title', 'ì •ì¹˜ ì´ìŠˆ'),
                    'event_type': group.get('event_type', 'unknown')
                })
            
            # ê°œë³„ ê·¸ë£¹ë“¤ ì¤‘ì—ì„œ ìœ ì‚¬í•œ ê²ƒë“¤ë§Œ ì„ ë³„ì  ë³‘í•©
            merged_individuals = self._selective_merge_individuals(individual_groups)
            final_groups.extend(merged_individuals)
            
            console.print(f"âœ… {len(final_groups)}ê°œ ê·¸ë£¹ìœ¼ë¡œ ì¬ë³‘í•© ì™„ë£Œ")
            
            # ê·¸ë£¹ë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥
            for group in final_groups:
                if group['article_count'] >= 5:
                    console.print(f"   - {group['title']}: {group['article_count']}ê°œ ê¸°ì‚¬")
            
            return final_groups
            
        except Exception as e:
            console.print(f"âŒ ì¬ë³‘í•© ì‹¤íŒ¨: {str(e)}")
            return all_subgroups
    
    def _selective_merge_individuals(self, individual_groups: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê°œë³„ ê·¸ë£¹ë“¤ì˜ ì„ ë³„ì  ë³‘í•©"""
        import re
        
        # ë™ì¼ ì‚¬ê±´ íŒ¨í„´ ì •ì˜ (ë” ì—„ê²©)
        exact_patterns = {
            "ëŒ€í†µë ¹ì‹¤_ì…ì¥": ["ëŒ€í†µë ¹ì‹¤", "ì…ì¥", "ë°œí‘œ"],
            "ì¥ë™í˜_ë°œì–¸": ["ì¥ë™í˜", "ëŒ€í†µë ¹"],
            "ì†¡ì–¸ì„_ë°œì–¸": ["ì†¡ì–¸ì„", "ë¹„íŒ"],
            "êµ­í˜_ë°˜ë°œ": ["êµ­í˜", "ë°˜ë°œ", "ë¹„íŒ"]
        }
        
        pattern_groups = {pattern: [] for pattern in exact_patterns.keys()}
        remaining_individuals = []
        
        # íŒ¨í„´ë³„ ë¶„ë¥˜
        for group in individual_groups:
            title = group.get('title', '').lower()
            matched = False
            
            for pattern_name, keywords in exact_patterns.items():
                if sum(1 for keyword in keywords if keyword in title) >= 2:
                    pattern_groups[pattern_name].append(group)
                    matched = True
                    break
            
            if not matched:
                remaining_individuals.append(group)
        
        # ë³‘í•©ëœ ê·¸ë£¹ ìƒì„±
        merged_groups = []
        
        for pattern_name, groups in pattern_groups.items():
            if len(groups) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒë§Œ ë³‘í•©
                merged_articles = []
                for group in groups:
                    merged_articles.extend(group['articles'])
                
                merged_groups.append({
                    'group_id': f"merged_{pattern_name}",
                    'articles': merged_articles,
                    'article_count': len(merged_articles),
                    'title': self._get_pattern_title(pattern_name),
                    'event_type': 'merged'
                })
            else:
                # 3ê°œ ë¯¸ë§Œì€ ê°œë³„ ìœ ì§€
                remaining_individuals.extend(groups)
        
        # ê°œë³„ ê·¸ë£¹ë“¤ ì¶”ê°€ (5ê°œ ì´ìƒë§Œ - ë…¸ì´ì¦ˆ í•„í„°ë§)
        for group in remaining_individuals:
            if group['article_count'] >= 1:  # ì¼ë‹¨ ëª¨ë“  ê°œë³„ ê·¸ë£¹ í¬í•¨
                merged_groups.append({
                    'group_id': group['subgroup_id'],
                    'articles': group['articles'],
                    'article_count': group['article_count'],
                    'title': group.get('title', 'ì •ì¹˜ ì´ìŠˆ'),
                    'event_type': group.get('event_type', 'individual')
                })
        
        return merged_groups
    
    def _get_pattern_title(self, pattern_name: str) -> str:
        """íŒ¨í„´ë³„ ì œëª© ë°˜í™˜"""
        pattern_titles = {
            "ëŒ€í†µë ¹ì‹¤_ì…ì¥": "ëŒ€í†µë ¹ì‹¤ ê³µì‹ ì…ì¥",
            "ì¥ë™í˜_ë°œì–¸": "ì¥ë™í˜ ëŒ€í†µë ¹ ë¹„íŒ",
            "ì†¡ì–¸ì„_ë°œì–¸": "ì†¡ì–¸ì„ ì •ë¶€ ë¹„íŒ", 
            "êµ­í˜_ë°˜ë°œ": "êµ­ë¯¼ì˜í˜ ë°˜ë°œ"
        }
        return pattern_titles.get(pattern_name, "ì •ì¹˜ ì´ìŠˆ")

    def analyze_clusters(self, articles: List[Dict[str, Any]], cluster_labels: np.ndarray, 
                        bias_mapping: Dict[str, str]) -> List[Dict[str, Any]]:
        """ìƒˆë¡œìš´ 4ë‹¨ê³„ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ë¶„ì„"""
        try:
            console.print("ğŸ”„ 4ë‹¨ê³„ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘...")
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ê¸°ì‚¬ ê·¸ë£¹í™”
            cluster_groups = {}
            for idx, label in enumerate(cluster_labels):
                if label != -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                    if label not in cluster_groups:
                        cluster_groups[label] = []
                    cluster_groups[label].append(articles[idx])
            
            console.print(f"ğŸ“Š ì´ˆê¸° í´ëŸ¬ìŠ¤í„°: {len(cluster_groups)}ê°œ")
            
            # ëª¨ë“  ì†Œê·¸ë£¹ ìˆ˜ì§‘
            all_subgroups = []
            
            # 2.1ë‹¨ê³„: ê° í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ì†Œê·¸ë£¹ ìƒì„±
            console.print("\nğŸ”„ 2.1ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° ë‚´ ì†Œê·¸ë£¹ ìƒì„±")
            for label, cluster_articles in cluster_groups.items():
                console.print(f"ğŸ“ í´ëŸ¬ìŠ¤í„° {label} ({len(cluster_articles)}ê°œ ê¸°ì‚¬) ì²˜ë¦¬ ì¤‘...")
                subgroups = self.create_subgroups_within_cluster(cluster_articles)
                all_subgroups.extend(subgroups)
            
            console.print(f"âœ… 2.1ë‹¨ê³„ ì™„ë£Œ: ì´ {len(all_subgroups)}ê°œ ì†Œê·¸ë£¹ ìƒì„±")
            
            # 2.2ë‹¨ê³„: ê° ì†Œê·¸ë£¹ë³„ í—¤ë“œë¼ì¸ ìƒì„±
            console.print(f"\nğŸ”„ 2.2ë‹¨ê³„: ì†Œê·¸ë£¹ í—¤ë“œë¼ì¸ ìƒì„±")
            all_subgroups = self.generate_subgroup_headlines(all_subgroups)
            
            # 2.3ë‹¨ê³„: ì†Œê·¸ë£¹ë“¤ì„ ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ì¬ë³‘í•©
            console.print(f"\nğŸ”„ 2.3ë‹¨ê³„: ì†Œê·¸ë£¹ ì¬ë³‘í•©")
            final_groups = self.merge_similar_subgroups(all_subgroups)
            
            # ì–¸ë¡ ì‚¬ë³„ bias í†µê³„ ê³„ì‚°
            for group in final_groups:
                bias_counts = {'left': 0, 'center': 0, 'right': 0}
                for article in group['articles']:
                    media_id = article.get('media_id')
                    if media_id and media_id in bias_mapping:
                        bias = bias_mapping[media_id]
                        if bias in bias_counts:
                            bias_counts[bias] += 1
                
                group['left_source'] = bias_counts['left']
                group['center_source'] = bias_counts['center']
                group['right_source'] = bias_counts['right']
            
            # 2.4ë‹¨ê³„: í’ˆì§ˆ ê¸°ë°˜ ìƒìœ„ 3ê°œ ì„ ë³„ (ê°œì„ ë¨)
            console.print(f"\nğŸ”„ 2.4ë‹¨ê³„: í’ˆì§ˆ ê¸°ë°˜ ìƒìœ„ 3ê°œ ê·¸ë£¹ ì„ ë³„")
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê¸°ì‚¬ìˆ˜ + ì‚¬ê±´ ëª…í™•ë„)
            for group in final_groups:
                quality_score = group['article_count']
                
                # ì‚¬ê±´ ëª…í™•ë„ ë³´ë„ˆìŠ¤
                if group.get('event_type') in ['ì¡°í¬ëŒ€_ì‚¬í‡´', 'ì„¸ì¢…_ì§‘ë¬´ì‹¤', 'ê·œì œ_í•©ë¦¬í™”', 'í•œë¯¸_ê´€ì„¸']:
                    quality_score += 10  # ëª…í™•í•œ ì‚¬ê±´ì— ë³´ë„ˆìŠ¤
                elif group.get('event_type') == 'merged':
                    quality_score += 5   # ë³‘í•©ëœ ê·¸ë£¹ì— ì¤‘ê°„ ë³´ë„ˆìŠ¤
                
                # ìµœì†Œ ì„ê³„ê°’ ì ìš© (20ê°œë¡œ ìƒí–¥)
                if group['article_count'] < 20:
                    quality_score = 0  # 20ê°œ ë¯¸ë§Œ ê·¸ë£¹ì€ ì œì™¸
                
                group['quality_score'] = quality_score
            
            # í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            qualified_groups = [g for g in final_groups if g.get('quality_score', 0) > 0]
            qualified_groups = sorted(qualified_groups, key=lambda x: x['quality_score'], reverse=True)
            top_groups = qualified_groups[:3]
            
            console.print(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ!")
            console.print(f"ğŸ“Š ì „ì²´ ê·¸ë£¹: {len(final_groups)}ê°œ")
            console.print(f"ğŸ“Š ìê²© ìš”ê±´ ì¶©ì¡±: {len(qualified_groups)}ê°œ")
            console.print(f"ğŸ“Š ìµœì¢… ì„ ë³„: {len(top_groups)}ê°œ")
            
            for i, group in enumerate(top_groups, 1):
                event_type = group.get('event_type', 'unknown')
                console.print(f"   {i}ìœ„: {group['article_count']}ê°œ ê¸°ì‚¬ - '{group['title']}' ({event_type})")
            
            # ê¸°ì¡´ í˜•ì‹ì— ë§ì¶° ë³€í™˜
            result = []
            for i, group in enumerate(top_groups):
                result.append({
                    'cluster_id': group.get('group_id', f'final_{i}'),
                    'articles': group['articles'],
                    'total_articles': group['article_count'],
                    'title': group['title'],
                    'left_source': group['left_source'],
                    'center_source': group['center_source'],
                    'right_source': group['right_source']
                })
            
            return result
            
        except Exception as e:
            console.print(f"âŒ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def save_issues_to_db(self, top_clusters: List[Dict[str, Any]], category: str) -> List[str]:
        """ìƒìœ„ í´ëŸ¬ìŠ¤í„°ë¥¼ issues í…Œì´ë¸”ì— ì €ì¥ (ì¹´í…Œê³ ë¦¬ ì •ë³´ í¬í•¨)"""
        try:
            console.print(f"ğŸ’¾ {category} ì¹´í…Œê³ ë¦¬ issues í…Œì´ë¸”ì— ì €ì¥ ì¤‘...")
            
            saved_issue_ids = []
            
            for i, cluster in enumerate(top_clusters, 1):
                try:
                    issue_data = {
                        'title': cluster['title'],
                        'category': category,  # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                        'source': cluster['total_articles'],
                        'left_source': cluster['left_source'],
                        'center_source': cluster['center_source'],
                        'right_source': cluster['right_source'],
                        'created_at': datetime.now(timezone.utc).isoformat()
                    }
                    
                    result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
                    
                    if result.data:
                        issue_id = result.data[0]['id']
                        saved_issue_ids.append(issue_id)
                        console.print(f"âœ… {category} ì´ìŠˆ {i} ì €ì¥ ì™„ë£Œ: {cluster['title']} ({cluster['total_articles']}ê°œ ê¸°ì‚¬)")
                    else:
                        console.print(f"âŒ {category} ì´ìŠˆ {i} ì €ì¥ ì‹¤íŒ¨")
                        
                except Exception as e:
                    console.print(f"âŒ {category} ì´ìŠˆ {i} ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            
            console.print(f"âœ… {category} ì¹´í…Œê³ ë¦¬ {len(saved_issue_ids)}ê°œ ì´ìŠˆ ì €ì¥ ì™„ë£Œ")
            return saved_issue_ids
            
        except Exception as e:
            console.print(f"âŒ {category} issues í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def update_articles_with_issue_ids(self, top_clusters: List[Dict[str, Any]], 
                                     issue_ids: List[str]) -> int:
        """í´ëŸ¬ìŠ¤í„° ì†Œì† ê¸°ì‚¬ë“¤ì— issue_id ì—…ë°ì´íŠ¸"""
        try:
            console.print("ğŸ”„ ê¸°ì‚¬ë“¤ì— issue_id ì—…ë°ì´íŠ¸ ì¤‘...")
            
            total_updated = 0
            
            for cluster, issue_id in zip(top_clusters, issue_ids):
                cluster_articles = cluster['articles']
                updated_count = 0
                
                for article in cluster_articles:
                    try:
                        result = self.supabase_manager.client.table('articles').update({
                            'issue_id': issue_id
                        }).eq('id', article['id']).execute()
                        
                        if result.data:
                            updated_count += 1
                        
                    except Exception as e:
                        console.print(f"âŒ ê¸°ì‚¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {article.get('id', 'Unknown')} - {str(e)}")
                
                total_updated += updated_count
                console.print(f"âœ… í´ëŸ¬ìŠ¤í„° {cluster['cluster_id']}: {updated_count}ê°œ ê¸°ì‚¬ ì—…ë°ì´íŠ¸")
            
            console.print(f"âœ… ì´ {total_updated}ê°œ ê¸°ì‚¬ issue_id ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return total_updated
            
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ issue_id ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            return 0
    
    def process_single_category(self, category: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ ì²˜ë¦¬"""
        try:
            console.print(f"\n{'='*20} {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹œì‘ {'='*20}")
            
            # 1. ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ì¡°íšŒ
            articles = self.fetch_articles_by_category(category)
            if not articles:
                console.print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {'category': category, 'success': False, 'issues': 0, 'articles': 0}
            
            # 2. ì„ë² ë”© ì¶”ì¶œ
            embeddings, valid_articles = self.extract_embeddings(articles)
            if len(embeddings) == 0:
                console.print(f"âŒ {category} ìœ íš¨í•œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return {'category': category, 'success': False, 'issues': 0, 'articles': 0}
            
            if len(valid_articles) < 30:  # ìµœì†Œ ê¸°ì‚¬ ìˆ˜ í™•ì¸ (ì¹´í…Œê³ ë¦¬ë³„ ì¡°ì •)
                console.print(f"âš ï¸ {category} ê¸°ì‚¬ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(valid_articles)}ê°œ")
                console.print("í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 30ê°œ ì´ìƒì˜ ê¸°ì‚¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return {'category': category, 'success': False, 'issues': 0, 'articles': 0}
            
            # 3. UMAP ì°¨ì›ì¶•ì†Œ
            reduced_embeddings = self.perform_umap_reduction(embeddings)
            if len(reduced_embeddings) == 0:
                console.print(f"âŒ {category} UMAP ì°¨ì›ì¶•ì†Œ ì‹¤íŒ¨")
                return {'category': category, 'success': False, 'issues': 0, 'articles': 0}
            
            # 4. HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
            cluster_labels = self.perform_hdbscan_clustering(reduced_embeddings)
            if len(cluster_labels) == 0:
                console.print(f"âŒ {category} HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
                return {'category': category, 'success': False, 'issues': 0, 'articles': 0}
            
            # 5. ì–¸ë¡ ì‚¬ bias ë§¤í•‘ ì¡°íšŒ
            bias_mapping = self.get_media_bias_mapping()
            if not bias_mapping:
                console.print(f"âŒ {category} ì–¸ë¡ ì‚¬ bias ë§¤í•‘ ì¡°íšŒ ì‹¤íŒ¨")
                return {'category': category, 'success': False, 'issues': 0, 'articles': 0}
            
            # 6. í´ëŸ¬ìŠ¤í„° ë¶„ì„ ë° ìƒìœ„ 3ê°œ ì„ ë³„
            top_clusters = self.analyze_clusters(valid_articles, cluster_labels, bias_mapping)
            if not top_clusters:
                console.print(f"âŒ {category} ìœ íš¨í•œ í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {'category': category, 'success': False, 'issues': 0, 'articles': 0}
            
            # 7. issues í…Œì´ë¸”ì— ì €ì¥ (ì¹´í…Œê³ ë¦¬ ì •ë³´ í¬í•¨)
            issue_ids = self.save_issues_to_db(top_clusters, category)
            if not issue_ids:
                console.print(f"âŒ {category} issues í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨")
                return {'category': category, 'success': False, 'issues': 0, 'articles': 0}
            
            # 8. ê¸°ì‚¬ë“¤ì— issue_id ì—…ë°ì´íŠ¸
            updated_count = self.update_articles_with_issue_ids(top_clusters, issue_ids)
            
            console.print(f"âœ… {category} ì¹´í…Œê³ ë¦¬ ì™„ë£Œ: {len(issue_ids)}ê°œ ì´ìŠˆ, {updated_count}ê°œ ê¸°ì‚¬")
            return {
                'category': category, 
                'success': True, 
                'issues': len(issue_ids), 
                'articles': updated_count
            }
            
        except Exception as e:
            console.print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {'category': category, 'success': False, 'issues': 0, 'articles': 0}

    def run_clustering(self) -> bool:
        """í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì „ì²´ ì¹´í…Œê³ ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"""
        try:
            console.print("=" * 80)
            console.print("ğŸš€ ì „ì²´ ì •ì¹˜ ì¹´í…Œê³ ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)")
            console.print("=" * 80)
            
            total_results = []
            
            # 1ë‹¨ê³„: ëŒ€ìš©ëŸ‰ ì¹´í…Œê³ ë¦¬ ìˆœì°¨ ì²˜ë¦¬
            console.print("\nğŸ”„ 1ë‹¨ê³„: ëŒ€ìš©ëŸ‰ ì¹´í…Œê³ ë¦¬ ìˆœì°¨ ì²˜ë¦¬")
            for category in self.categories["large"]:
                result = self.process_single_category(category)
                total_results.append(result)
            
            # 2ë‹¨ê³„: ì†ŒëŸ‰ ì¹´í…Œê³ ë¦¬ ë³‘ë ¬ ì²˜ë¦¬ (ì¶”í›„ êµ¬í˜„)
            console.print("\nğŸ”„ 2ë‹¨ê³„: ì†ŒëŸ‰ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬")
            for category in self.categories["small"]:
                result = self.process_single_category(category)
                total_results.append(result)
            
            # ìµœì¢… ê²°ê³¼ ì§‘ê³„
            total_issues = sum(r['issues'] for r in total_results)
            total_articles = sum(r['articles'] for r in total_results)
            successful_categories = [r['category'] for r in total_results if r['success']]
            
            console.print("\n" + "=" * 80)
            console.print("ğŸ‰ ì „ì²´ ì¹´í…Œê³ ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
            console.print(f"âœ… ì²˜ë¦¬ëœ ì¹´í…Œê³ ë¦¬: {len(successful_categories)}ê°œ")
            console.print(f"âœ… ìƒì„±ëœ ì´ ì´ìŠˆ: {total_issues}ê°œ")
            console.print(f"âœ… ì—…ë°ì´íŠ¸ëœ ì´ ê¸°ì‚¬: {total_articles}ê°œ")
            console.print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼:")
            
            for result in total_results:
                status = "âœ…" if result['success'] else "âŒ"
                console.print(f"   {status} {result['category']}: {result['issues']}ê°œ ì´ìŠˆ, {result['articles']}ê°œ ê¸°ì‚¬")
            
            console.print("=" * 80)
            
            return len(successful_categories) > 0
            
        except Exception as e:
            console.print(f"âŒ ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        clusterer = MultiCategoryClusterer()
        success = clusterer.run_clustering()
        
        if success:
            console.print("\nâœ… ì „ì²´ ì¹´í…Œê³ ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ ì„±ê³µ!")
        else:
            console.print("\nâŒ ì „ì²´ ì¹´í…Œê³ ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        console.print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        console.print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    main()
