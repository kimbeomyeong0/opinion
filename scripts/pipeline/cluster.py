#!/usr/bin/env python3
"""
ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸
- HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
- í‚¤ì›Œë“œ ê¸°ë°˜ ì œëª© ìƒì„±
- ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µ í†µí•©
- ìµœì¢… ì´ìŠˆ ì €ì¥
"""

import time
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import

try:
    import umap
    import hdbscan
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:
    print("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("pip install umap-learn hdbscan scikit-learn")
    exit(1)

from utils.supabase_manager import SupabaseManager


class AdvancedClusteringPipeline:
    """ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, batch_size: int = 100):
        """ì´ˆê¸°í™”"""
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise Exception("Supabase ì—°ê²° ì‹¤íŒ¨")
        
        self.batch_size = batch_size
        
        # UMAP íŒŒë¼ë¯¸í„° (ë” í° í´ëŸ¬ìŠ¤í„° í—ˆìš©)
        self.umap_params = {
            'n_neighbors': 15,  # 5 â†’ 15ë¡œ ì¦ê°€ (ë” í° í´ëŸ¬ìŠ¤í„°)
            'n_components': 20,  # 10 â†’ 20ìœ¼ë¡œ ì¦ê°€
            'min_dist': 0.1,
            'metric': 'cosine',
            'random_state': 42
        }
        
        # HDBSCAN íŒŒë¼ë¯¸í„° (ë” í° í´ëŸ¬ìŠ¤í„° í—ˆìš©)
        self.hdbscan_params = {
            'min_cluster_size': 8,  # 3 â†’ 8ë¡œ ì¦ê°€ (ë” í° í´ëŸ¬ìŠ¤í„°)
            'min_samples': 5,  # 2 â†’ 5ë¡œ ì¦ê°€
            'metric': 'euclidean',
            'cluster_selection_epsilon': 0.2  # 0.1 â†’ 0.2ë¡œ ì¦ê°€
        }
        
        # í†µí•© ì„ê³„ê°’ (ë” ê°•í™”ëœ í†µí•©)
        self.merge_threshold = 0.5  # 0.6 â†’ 0.5ë¡œ ê°ì†Œ (ë” ë§ì€ í†µí•©)
        self.separate_threshold = 0.4  # 0.5 â†’ 0.4ë¡œ ê°ì†Œ
    
    def fetch_articles_by_category(self, category: str) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ì¡°íšŒ (ì„ë² ë”© í¬í•¨, ì„ë² ë”© ì—†ëŠ” ê¸°ì‚¬ë„ í¬í•¨)"""
        try:
            result = self.supabase_manager.client.table('articles').select(
                'id, title, lead_paragraph, political_category, embedding'
            ).eq('political_category', category).eq('is_preprocessed', True).execute()
            
            return result.data
        except Exception as e:
            print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    
    def reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:
        """UMAP ì°¨ì› ì¶•ì†Œ"""
        try:
            reducer = umap.UMAP(**self.umap_params)
            reduced_embeddings = reducer.fit_transform(embeddings)
            return reduced_embeddings
        except Exception as e:
            print(f"âŒ ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨: {str(e)}")
            return embeddings
    
    def perform_clustering(self, embeddings: np.ndarray) -> np.ndarray:
        """HDBSCAN êµ°ì§‘í™”"""
        try:
            clusterer = hdbscan.HDBSCAN(**self.hdbscan_params)
            cluster_labels = clusterer.fit_predict(embeddings)
            return cluster_labels
        except Exception as e:
            print(f"âŒ êµ°ì§‘í™” ì‹¤íŒ¨: {str(e)}")
            return np.array([-1] * len(embeddings))
    
    def extract_keywords_from_articles(self, articles: List[Dict[str, Any]]) -> List[str]:
        """ê¸°ì‚¬ë“¤ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not articles:
            return []
        
        # ëª¨ë“  ì œëª©ê³¼ ë¦¬ë“œë¬¸ë‹¨ ìˆ˜ì§‘
        all_texts = []
        for article in articles:
            all_texts.append(article['title'])
            if article.get('lead_paragraph'):
                all_texts.append(article['lead_paragraph'])
        
        # ë‹¨ì–´ ì¶”ì¶œ ë° ì •ì œ
        words = []
        for text in all_texts:
            text_words = text.replace('"', '').replace("'", '').split()
            words.extend(text_words)
        
        # ë¶ˆìš©ì–´ ì œê±° ë° í•„í„°ë§
        stop_words = {'ê´€ë ¨', 'ì´ìŠˆ', 'ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ë³´ë„', 'ë…¼ë€', 'ì‚¬íƒœ', 'ë¬¸ì œ', 'ì´ì•¼ê¸°', 'ì†Œì‹', 'ì „ë§', 'ë¶„ì„', 'í‰ê°€', 'ê²€í† ', 'ë…¼ì˜', 'í˜‘ì˜', 'ê²°ì •', 'ë°œí‘œ', 'ê³µê°œ', 'í™•ì¸', 'ì¡°ì‚¬', 'ìˆ˜ì‚¬', 'ì¬íŒ', 'íŒê²°', 'ê¸°ì†Œ', 'êµ¬ì†', 'ì²´í¬', 'ìˆ˜ì‚¬', 'ì¡°ì‚¬', 'í™•ì¸', 'ë°œí‘œ', 'ê³µê°œ', 'ê²°ì •', 'ë…¼ì˜', 'í˜‘ì˜', 'í‰ê°€', 'ê²€í† ', 'ë¶„ì„', 'ì „ë§', 'ì†Œì‹', 'ì´ì•¼ê¸°', 'ì´ìŠˆ', 'ë¬¸ì œ', 'ì‚¬íƒœ', 'ë…¼ë€', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ë³´ë„'}
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        word_freq = {}
        for word in words:
            word = word.strip('.,!?()[]{}"\'')
            if len(word) > 1 and word not in stop_words and not word.isdigit():
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜ (ë¹ˆë„ìˆœ, ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ì¶œ)
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:15]
        return [word for word, freq in top_words if freq > 1]
    
    def create_keyword_based_title(self, articles: List[Dict[str, Any]]) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì œëª© ìƒì„± (5-10ê°œ í‚¤ì›Œë“œ ì‚¬ìš©)"""
        if not articles:
            return "ë¯¸ë¶„ë¥˜ ì´ìŠˆ"
        
        keywords = self.extract_keywords_from_articles(articles)
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (5-10ê°œ)
        if len(keywords) >= 10:
            top_keywords = keywords[:10]  # ìµœëŒ€ 10ê°œ
        elif len(keywords) >= 5:
            top_keywords = keywords[:len(keywords)]  # 5ê°œ ì´ìƒì´ë©´ ëª¨ë‘ ì‚¬ìš©
        else:
            top_keywords = keywords  # 5ê°œ ë¯¸ë§Œì´ë©´ ìˆëŠ” ë§Œí¼
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì œëª© ìƒì„±
        if len(top_keywords) >= 5:
            # 5ê°œ ì´ìƒì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í•µì‹¬ í‚¤ì›Œë“œë“¤ì„ ì¡°í•©
            # ìƒìœ„ 3ê°œëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ, ë‚˜ë¨¸ì§€ëŠ” ë³´ì¡° í‚¤ì›Œë“œë¡œ
            main_keywords = top_keywords[:3]
            sub_keywords = top_keywords[3:8] if len(top_keywords) > 3 else []
            
            title_parts = []
            title_parts.extend(main_keywords)
            
            # ë³´ì¡° í‚¤ì›Œë“œ ì¤‘ ì¤‘ìš”í•œ ê²ƒë“¤ ì¶”ê°€ (ìµœëŒ€ 2ê°œ)
            if sub_keywords:
                title_parts.extend(sub_keywords[:2])
            
            return " ".join(title_parts) + " ê´€ë ¨ ì´ìŠˆ"
        elif len(top_keywords) >= 2:
            # 2-4ê°œ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ëª¨ë‘ ì‚¬ìš©
            return " ".join(top_keywords) + " ê´€ë ¨ ì´ìŠˆ"
        elif len(top_keywords) == 1:
            # í•˜ë‚˜ì˜ í‚¤ì›Œë“œë§Œ ìˆìœ¼ë©´ ë‹¨ë…
            return f"{top_keywords[0]} ê´€ë ¨ ì´ìŠˆ"
        else:
            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê¸°ì‚¬ ìˆ˜ë¡œ
            return f"{len(articles)}ê°œ ê¸°ì‚¬ í´ëŸ¬ìŠ¤í„°"
    
    def calculate_title_similarity(self, title1: str, title2: str) -> float:
        """ì œëª© ìœ ì‚¬ë„ ê³„ì‚° (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords1 = set(title1.replace('ê´€ë ¨ ì´ìŠˆ', '').replace('í´ëŸ¬ìŠ¤í„°', '').split())
        keywords2 = set(title2.replace('ê´€ë ¨ ì´ìŠˆ', '').replace('í´ëŸ¬ìŠ¤í„°', '').split())
        
        # ê³µí†µ í‚¤ì›Œë“œ ê³„ì‚°
        common_keywords = keywords1.intersection(keywords2)
        total_keywords = keywords1.union(keywords2)
        
        if len(total_keywords) == 0:
            return 0.0
        
        similarity = len(common_keywords) / len(total_keywords)
        return similarity
    
    def group_similar_titles(self, clusters: List[Dict[str, Any]]) -> List[List[int]]:
        """ë¹„ìŠ·í•œ ì œëª©ë¼ë¦¬ ê·¸ë£¹í•‘"""
        groups = []
        used_indices = set()
        
        for i, cluster in enumerate(clusters):
            if i in used_indices:
                continue
            
            # í˜„ì¬ í´ëŸ¬ìŠ¤í„°ì™€ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°ë“¤ ì°¾ê¸°
            similar_group = [i]
            used_indices.add(i)
            
            for j, other_cluster in enumerate(clusters):
                if j in used_indices:
                    continue
                
                similarity = self.calculate_title_similarity(
                    cluster['title'], other_cluster['title']
                )
                
                if similarity >= 0.15:  # 15% ì´ìƒ ìœ ì‚¬í•˜ë©´ ê·¸ë£¹ì— ì¶”ê°€ (ë”ìš± ê°•í™”ëœ í†µí•©)
                    similar_group.append(j)
                    used_indices.add(j)
            
            groups.append(similar_group)
        
        return groups
    
    def calculate_embedding_similarity(self, articles1: List[Dict[str, Any]], articles2: List[Dict[str, Any]]) -> float:
        """ë‘ í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ì‚¬ë“¤ ê°„ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° (ì €ì¥ëœ ì„ë² ë”© ì‚¬ìš©)"""
        try:
            import json
            
            # ì €ì¥ëœ ì„ë² ë”© ì¶”ì¶œ ë° íŒŒì‹±
            embeddings1 = []
            embeddings2 = []
            
            for article in articles1:
                if article.get('embedding'):
                    try:
                        # JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        embedding_data = json.loads(article['embedding'])
                        embeddings1.append(embedding_data)
                    except:
                        continue
            
            for article in articles2:
                if article.get('embedding'):
                    try:
                        # JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        embedding_data = json.loads(article['embedding'])
                        embeddings2.append(embedding_data)
                    except:
                        continue
            
            if len(embeddings1) == 0 or len(embeddings2) == 0:
                return 0.0
            
            embeddings1 = np.array(embeddings1)
            embeddings2 = np.array(embeddings2)
            
            # ê° í´ëŸ¬ìŠ¤í„°ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
            avg_embedding1 = np.mean(embeddings1, axis=0)
            avg_embedding2 = np.mean(embeddings2, axis=0)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity([avg_embedding1], [avg_embedding2])[0][0]
            
            return float(similarity)
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0
    
    def merge_similar_clusters(self, clusters: List[Dict[str, Any]], groups: List[List[int]]) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°ë“¤ í†µí•©"""
        merged_clusters = []
        
        for group in groups:
            if len(group) == 1:
                # ê·¸ë£¹ì— í´ëŸ¬ìŠ¤í„°ê°€ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                merged_clusters.append(clusters[group[0]])
            else:
                # ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ê°€ ìˆìœ¼ë©´ ì„ë² ë”© ìœ ì‚¬ë„ë¡œ í†µí•© ì—¬ë¶€ ê²°ì •
                print(f"  ğŸ” {len(group)}ê°œ í´ëŸ¬ìŠ¤í„° ê·¸ë£¹ ê²€í†  ì¤‘...")
                
                # ì²« ë²ˆì§¸ í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘
                merged_cluster = clusters[group[0]].copy()
                merged_articles = merged_cluster['articles'].copy()
                
                # ë‚˜ë¨¸ì§€ í´ëŸ¬ìŠ¤í„°ë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
                for i in range(1, len(group)):
                    current_cluster = clusters[group[i]]
                    
                    # ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = self.calculate_embedding_similarity(
                        merged_articles, current_cluster['articles']
                    )
                    
                    print(f"    ğŸ“Š ìœ ì‚¬ë„: {similarity:.3f}")
                    
                    if similarity >= self.merge_threshold:
                        # í†µí•©
                        merged_articles.extend(current_cluster['articles'])
                        print(f"    âœ… í†µí•©: {current_cluster['title']}")
                    else:
                        # ë¶„ë¦¬ (ë³„ë„ í´ëŸ¬ìŠ¤í„°ë¡œ ìœ ì§€)
                        merged_clusters.append(current_cluster)
                        print(f"    âŒ ë¶„ë¦¬: {current_cluster['title']}")
                
                # í†µí•©ëœ í´ëŸ¬ìŠ¤í„° ì—…ë°ì´íŠ¸
                merged_cluster['articles'] = merged_articles
                merged_cluster['title'] = self.create_keyword_based_title(merged_articles)
                merged_clusters.append(merged_cluster)
        
        return merged_clusters
    
    def categorize_articles_by_bias(self, articles: List[Dict[str, Any]]) -> Tuple[int, int, int]:
        """ê¸°ì‚¬ë¥¼ ì„±í–¥ë³„ë¡œ ë¶„ë¥˜í•˜ê³  ìˆ«ì ë°˜í™˜"""
        total_articles = len(articles)
        
        # ê· ë“± ë¶„ë¥˜ (ì‹¤ì œë¡œëŠ” ì„±í–¥ ë¶„ì„ ëª¨ë¸ ì‚¬ìš©)
        left_count = total_articles // 3
        center_count = total_articles // 3
        right_count = total_articles - left_count - center_count
        
        return left_count, center_count, right_count
    
    def save_cluster_to_issues(self, cluster_articles: List[Dict[str, Any]], cluster_id: int) -> Optional[str]:
        """í´ëŸ¬ìŠ¤í„°ë¥¼ issues í…Œì´ë¸”ì— ì €ì¥"""
        try:
            # ì´ìŠˆ ì œëª© ìƒì„±
            issue_title = self.create_keyword_based_title(cluster_articles)
            
            # ì„±í–¥ë³„ ë¶„ë¥˜ (ìˆ«ìë¡œ ë°˜í™˜)
            left_count, center_count, right_count = self.categorize_articles_by_bias(cluster_articles)
            total_count = len(cluster_articles)
            
            # issues í…Œì´ë¸”ì— ì €ì¥
            issue_data = {
                'title': issue_title,
                'left_source': str(left_count),
                'center_source': str(center_count),
                'right_source': str(right_count),
                'source': str(total_count),
                'created_at': datetime.now().isoformat()
            }
            
            result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
            issue_id = result.data[0]['id']
            
            # issue_articles í…Œì´ë¸”ì— ì €ì¥
            issue_articles_data = []
            for article in cluster_articles:
                issue_articles_data.append({
                    'issue_id': issue_id,
                    'article_id': article['id']
                })
            
            if issue_articles_data:
                self.supabase_manager.client.table('issue_articles').insert(issue_articles_data).execute()
            
            return issue_id
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def process_category(self, category: str) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì²˜ë¦¬"""
        print(f"ğŸ“Š {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ê¸°ì‚¬ ì¡°íšŒ
        articles = self.fetch_articles_by_category(category)
        if not articles:
            print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ì— ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'success': False, 'clusters': 0}
        
        print(f"  ğŸ“° ì¡°íšŒëœ ê¸°ì‚¬: {len(articles):,}ê°œ")
        
        # 2. ì„ë² ë”© ì²˜ë¦¬ (ì €ì¥ëœ ì„ë² ë”©ë§Œ ì‚¬ìš©)
        print(f"  ğŸ”„ ì„ë² ë”© ì²˜ë¦¬ ì¤‘...")
        
        # ì„ë² ë”©ì´ ìˆëŠ” ê¸°ì‚¬ë§Œ í•„í„°ë§
        articles_with_embedding = []
        for article in articles:
            if article.get('embedding'):
                articles_with_embedding.append(article)
        
        print(f"    ğŸ“Š ì„ë² ë”© ìˆëŠ” ê¸°ì‚¬: {len(articles_with_embedding)}ê°œ")
        
        if len(articles_with_embedding) == 0:
            print(f"âŒ {category} ì„ë² ë”©ì´ ìˆëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € generate_embeddings.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return {'success': False, 'clusters': 0}
        
        # ì„ë² ë”© ë°°ì—´ ìƒì„± (vector íƒ€ì… ì²˜ë¦¬)
        embeddings = []
        for article in articles_with_embedding:
            embedding_data = article['embedding']
            
            # vector íƒ€ì…ì€ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë¨
            if isinstance(embedding_data, list):
                embeddings.append(embedding_data)
            elif isinstance(embedding_data, str):
                # ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
                try:
                    # JSON í˜•íƒœì˜ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                    import json
                    embedding_list = json.loads(embedding_data)
                    embeddings.append(embedding_list)
                except:
                    try:
                        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                        embedding_list = eval(embedding_data)
                        embeddings.append(embedding_list)
                    except:
                        print(f"âŒ ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨: {article['id']}")
                        continue
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì„ë² ë”© íƒ€ì…: {type(embedding_data)}")
                continue
        
        if len(embeddings) == 0:
            print(f"âŒ {category} ìœ íš¨í•œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {'success': False, 'clusters': 0}
        
        embeddings = np.array(embeddings)
        print(f"    ğŸ“Š ì„ë² ë”© ë°°ì—´ í˜•íƒœ: {embeddings.shape}")
        print(f"    ğŸ“Š ì„ë² ë”© ì°¨ì›: {embeddings.shape[1] if len(embeddings.shape) > 1 else 'N/A'}")
        
        # ì„ë² ë”©ì´ ìˆëŠ” ê¸°ì‚¬ë“¤ë§Œ ì²˜ë¦¬
        articles = articles_with_embedding
        
        # 4. ì°¨ì› ì¶•ì†Œ
        print(f"  ğŸ“‰ ì°¨ì› ì¶•ì†Œ ì¤‘...")
        reduced_embeddings = self.reduce_dimensions(embeddings)
        
        # 5. êµ°ì§‘í™”
        print(f"  ğŸ¯ êµ°ì§‘í™” ì¤‘...")
        cluster_labels = self.perform_clustering(reduced_embeddings)
        
        # 6. í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ê¸°ì‚¬ë“¤ ê·¸ë£¹í•‘
        unique_clusters = np.unique(cluster_labels)
        clusters = []
        
        for cluster_id in unique_clusters:
            if cluster_id == -1:  # ë…¸ì´ì¦ˆ ìŠ¤í‚µ
                continue
            
            cluster_mask = cluster_labels == cluster_id
            cluster_articles = [articles[i] for i in range(len(articles)) if cluster_mask[i]]
            
            if len(cluster_articles) >= 5:  # ìµœì†Œ 5ê°œ ê¸°ì‚¬ ì´ìƒì¸ í´ëŸ¬ìŠ¤í„°ë§Œ
                cluster_title = self.create_keyword_based_title(cluster_articles)
                clusters.append({
                    'id': cluster_id,
                    'title': cluster_title,
                    'articles': cluster_articles,
                    'size': len(cluster_articles)
                })
        
        print(f"  ğŸ“Š ì´ˆê¸° í´ëŸ¬ìŠ¤í„°: {len(clusters)}ê°œ")
        
        # 7. ë¹„ìŠ·í•œ ì œëª©ë¼ë¦¬ ê·¸ë£¹í•‘
        print(f"  ğŸ” ë¹„ìŠ·í•œ ì œëª© ê·¸ë£¹í•‘ ì¤‘...")
        groups = self.group_similar_titles(clusters)
        print(f"  ğŸ“Š ì œëª© ê·¸ë£¹: {len(groups)}ê°œ")
        
        # 8. ì„ë² ë”© ê¸°ë°˜ í†µí•©
        print(f"  ğŸ”„ ì„ë² ë”© ê¸°ë°˜ í†µí•© ì¤‘...")
        merged_clusters = self.merge_similar_clusters(clusters, groups)
        print(f"  ğŸ“Š ìµœì¢… í´ëŸ¬ìŠ¤í„°: {len(merged_clusters)}ê°œ")
        
        # 9. í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì €ì¥
        saved_clusters = 0
        for cluster in merged_clusters:
            issue_id = self.save_cluster_to_issues(cluster['articles'], cluster['id'])
            if issue_id:
                saved_clusters += 1
        
        print(f"  âœ… {category} ì™„ë£Œ: {saved_clusters}ê°œ ì´ìŠˆ ìƒì„±")
        return {'success': True, 'clusters': saved_clusters}
    
    def run_full_pipeline(self, categories: Optional[List[str]] = None) -> bool:
        """ì „ì²´ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            print("=" * 60)
            print("ğŸ¯ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            print("=" * 60)
            
            # ì²˜ë¦¬í•  ì¹´í…Œê³ ë¦¬ ê²°ì •
            if categories is None:
                categories = ['êµ­íšŒ/ì •ë‹¹', 'í–‰ì •ë¶€', 'ì‚¬ë²•/ê²€ì°°', 'ì™¸êµ/ì•ˆë³´', 'ì •ì±…/ê²½ì œì‚¬íšŒ', 'ì„ ê±°', 'ì§€ì—­ì •ì¹˜']
            
            total_clusters = 0
            start_time = time.time()
            
            for category in categories:
                result = self.process_category(category)
                if result['success']:
                    total_clusters += result['clusters']
            
            # ìµœì¢… ê²°ê³¼
            total_time = time.time() - start_time
            print(f"\nğŸ‰ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
            print(f"âœ… ì´ ìƒì„±ëœ ì´ìŠˆ: {total_clusters}ê°œ")
            print(f"â±ï¸  ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„")
            
            return total_clusters > 0
            
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = AdvancedClusteringPipeline(batch_size=50)
        success = pipeline.run_full_pipeline()
        
        if success:
            print(f"\nâœ… ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        else:
            print(f"\nâŒ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    main()
