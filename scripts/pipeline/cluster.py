#!/usr/bin/env python3
"""
ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸
- HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
- í‚¤ì›Œë“œ ê¸°ë°˜ ì œëª© ìƒì„±
- ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µ í†µí•©
- ìµœì¢… ì´ìŠˆ ì €ì¥
"""

import time
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import warnings
import logging
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import

try:
    import umap
    import hdbscan
    from sklearn.metrics.pairwise import cosine_similarity
    import psutil  # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ìš©
except ImportError as e:
    print("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("pip install umap-learn hdbscan scikit-learn psutil")
    print(f"ì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {e}")
    exit(1)

from utils.supabase_manager import SupabaseManager


class ClusteringError(Exception):
    """í´ëŸ¬ìŠ¤í„°ë§ ê´€ë ¨ ì˜ˆì™¸"""
    pass


class ClusteringConfig:
    """í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì • ê´€ë¦¬"""
    
    # ë¶ˆìš©ì–´ ëª©ë¡
    STOP_WORDS = {
        'ê´€ë ¨', 'ì´ìŠˆ', 'ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ë³´ë„', 'ë…¼ë€', 'ì‚¬íƒœ', 'ë¬¸ì œ', 'ì´ì•¼ê¸°', 'ì†Œì‹', 'ì „ë§', 
        'ë¶„ì„', 'í‰ê°€', 'ê²€í† ', 'ë…¼ì˜', 'í˜‘ì˜', 'ê²°ì •', 'ë°œí‘œ', 'ê³µê°œ', 'í™•ì¸', 'ì¡°ì‚¬', 'ìˆ˜ì‚¬', 
        'ì¬íŒ', 'íŒê²°', 'ê¸°ì†Œ', 'êµ¬ì†', 'ì²´í¬', 'ì¡°ì‚¬', 'í™•ì¸', 'ë°œí‘œ', 'ê³µê°œ', 'ê²°ì •', 'ë…¼ì˜', 
        'í˜‘ì˜', 'í‰ê°€', 'ê²€í† ', 'ë¶„ì„', 'ì „ë§', 'ì†Œì‹', 'ì´ì•¼ê¸°', 'ì´ìŠˆ', 'ë¬¸ì œ', 'ì‚¬íƒœ', 'ë…¼ë€', 
        'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ë³´ë„'
    }
    
    # ì„±í–¥ë³„ ë¶„ë¥˜ ì„¤ì • (ì‚¬ìš© ì¤‘ë‹¨ - ë¶€ì •í™•í•œ ë°ì´í„° ìƒì„± ë°©ì§€)
    # BIAS_DISTRIBUTION = {
    #     'left_ratio': 1/3,
    #     'center_ratio': 1/3, 
    #     'right_ratio': 1/3
    # }
    # â†’ ì‹¤ì œ ì–¸ë¡ ì‚¬ ì„±í–¥ ë°ì´í„° ì—†ì´ ê°€ì§œ ë¶„ë¥˜ëŠ” ì˜ë¯¸ì—†ìŒ
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì„ê³„ê°’ (ê³ í’ˆì§ˆ ì´ìŠˆ ìƒì„± ìµœì í™”)
    THRESHOLDS = {
        'min_cluster_size': 3,         # HDBSCANê³¼ ì¼ì¹˜: ìµœì†Œ 3ê°œ ê¸°ì‚¬
        'merge_threshold': 0.9,        # ë§¤ìš° ì—„ê²©í•œ í†µí•© ê¸°ì¤€ (0.6â†’0.9)
        'separate_threshold': 0.8,     # ë†’ì€ ë¶„ë¦¬ ê¸°ì¤€
        'title_similarity_threshold': 0.2,  # ì œëª© ìœ ì‚¬ë„ ìƒí–¥
        'max_cluster_size': 50,        # ëŒ€í˜• í´ëŸ¬ìŠ¤í„° ë¶„í•  ê¸°ì¤€
        'noise_ratio_threshold': 0.7,  # ë…¸ì´ì¦ˆ ë¹„ìœ¨ ì„ê³„ê°’
        'quality_threshold': 0.3,      # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ìµœì†Œ ê¸°ì¤€
        'top_clusters_limit': 3        # ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„°ë§Œ ì €ì¥
    }
    
    # í‚¤ì›Œë“œ ì„¤ì •
    KEYWORD_SETTINGS = {
        'max_keywords': 15,
        'min_frequency': 2
    }


class AdvancedClusteringPipeline:
    """ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, batch_size: int = 100):
        """ì´ˆê¸°í™”"""
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise ClusteringError("Supabase ì—°ê²° ì‹¤íŒ¨")
        
        self.batch_size = batch_size
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            'total_articles_processed': 0,
            'successful_clusters': 0,
            'processing_time': 0,
            'memory_usage_peak': 0
        }
        
        # UMAP íŒŒë¼ë¯¸í„° (3072ì°¨ì› ì „ë©´ ì ìš© ìµœì í™”)
        self.umap_params = {
            'n_neighbors': 20,   # ë” ë§ì€ ì´ì›ƒìœ¼ë¡œ ì•ˆì •ì  ë§¤ë‹ˆí´ë“œ í•™ìŠµ
            'n_components': 512, # ì •ë³´ ì†ì‹¤ ìµœì†Œí™” (83% ì •ë³´ ë³´ì¡´)
            'min_dist': 0.0001, # ë§¤ìš° ì¡°ë°€í•œ í´ëŸ¬ìŠ¤í„° í˜•ì„±
            'metric': 'cosine',  # OpenAI ì„ë² ë”©ì— ìµœì í™”
            'random_state': 42,
            'n_jobs': -1,       # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
            'verbose': False,   # ë¡œê·¸ ì¶œë ¥ ì œì–´
            'low_memory': True  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
        }
        
        # HDBSCAN íŒŒë¼ë¯¸í„° (512ì°¨ì› ìµœì í™” + ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° 3)
        self.hdbscan_params = {
            'min_cluster_size': 3,     # ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°: 3ê°œ ê¸°ì‚¬ ì´ìƒ
            'min_samples': 2,          # ì½”ì–´ í¬ì¸íŠ¸ ê¸°ì¤€: ë” ë¯¼ê°í•œ íƒì§€
            'metric': 'euclidean',     # UMAP ì¶•ì†Œ í›„ ìœ í´ë¦¬ë“œ ê±°ë¦¬
            'cluster_selection_epsilon': 0.05,  # ë” ì„¸ë°€í•œ í´ëŸ¬ìŠ¤í„° ë¶„ë¦¬
            'cluster_selection_method': 'eom',   # Excess of Mass ë°©ë²•
            'core_dist_n_jobs': -1,    # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
            'algorithm': 'best'        # ìµœì  ì•Œê³ ë¦¬ì¦˜ ìë™ ì„ íƒ
        }
        
        # ì„¤ì • ê´€ë¦¬
        self.config = ClusteringConfig()
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        import os
        self.n_jobs = min(os.cpu_count() or 1, 8)  # CPU ì½”ì–´ ìˆ˜ ì œí•œ
        print(f"ğŸ’» ë³‘ë ¬ ì²˜ë¦¬ ì½”ì–´ ìˆ˜: {self.n_jobs}")
    
    def optimize_embedding_processing(self, articles: List[Dict[str, Any]]) -> Tuple[np.ndarray, List[Dict[str, Any]]]:
        """pgvector ë„¤ì´í‹°ë¸Œ ë²¡í„° ì²˜ë¦¬ ìµœì í™”"""
        embeddings = []
        valid_articles = []
        
        for article in articles:
            embedding_data = article.get('embedding')
            if not embedding_data:
                continue
                
            # pgvectorëŠ” ìë™ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ (ë„¤ì´í‹°ë¸Œ ì²˜ë¦¬)
            if isinstance(embedding_data, list):
                # 3072ì°¨ì› ë²¡í„° ê²€ì¦
                if len(embedding_data) == 3072:
                    embeddings.append(embedding_data)
                    valid_articles.append(article)
                else:
                    print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ë²¡í„° ì°¨ì›: {len(embedding_data)}ì°¨ì› - {article['id']}")
                    continue
            elif isinstance(embedding_data, str):
                # ë ˆê±°ì‹œ JSON ë¬¸ìì—´ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
                try:
                    import json
                    embedding_list = json.loads(embedding_data)
                    if isinstance(embedding_list, list) and len(embedding_list) == 3072:
                        embeddings.append(embedding_list)
                        valid_articles.append(article)
                    else:
                        print(f"âš ï¸ ì˜ëª»ëœ JSON ë²¡í„° í˜•ì‹: {article['id']}")
                        continue
                except (json.JSONDecodeError, ValueError) as e:
                    print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {article['id']} - {str(e)}")
                    continue
                except Exception as e:
                    print(f"âš ï¸ ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨: {article['id']} - {str(e)}")
                    continue
            else:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì„ë² ë”© íƒ€ì…: {type(embedding_data)} - {article['id']}")
                continue
        
        if len(embeddings) > 0:
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ numpy ë°°ì—´ ìƒì„± (float32 ì‚¬ìš©)
            return np.array(embeddings, dtype=np.float32), valid_articles
        else:
            return np.array([]), []

    def fetch_articles_by_category(self, category: str) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ì¡°íšŒ (ì„ë² ë”© + ì–¸ë¡ ì‚¬ ì„±í–¥ ì •ë³´ í¬í•¨)"""
        try:
            result = self.supabase_manager.client.table('articles').select(
                'id, title, lead_paragraph, political_category, embedding, media_id'
            ).eq('political_category', category).eq('is_preprocessed', True).execute()
            
            return result.data
        except Exception as e:
            print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    
    def reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:
        """UMAP ì°¨ì› ì¶•ì†Œ"""
        try:
            reducer = umap.UMAP(**self.umap_params)
            reduced_embeddings = reducer.fit_transform(embeddings)
            return reduced_embeddings
        except Exception as e:
            print(f"âŒ ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨: {str(e)}")
            return embeddings
    
    def perform_clustering(self, embeddings: np.ndarray) -> np.ndarray:
        """HDBSCAN êµ°ì§‘í™”"""
        try:
            clusterer = hdbscan.HDBSCAN(**self.hdbscan_params)
            cluster_labels = clusterer.fit_predict(embeddings)
            return cluster_labels
        except Exception as e:
            print(f"âŒ êµ°ì§‘í™” ì‹¤íŒ¨: {str(e)}")
            return np.array([-1] * len(embeddings))
    
    def perform_smart_clustering(self, embeddings: np.ndarray) -> np.ndarray:
        """ìŠ¤ë§ˆíŠ¸ í´ëŸ¬ìŠ¤í„°ë§: UMAP ìš°ì„  ì ìš© ì „ëµ"""
        try:
            n_samples, n_features = embeddings.shape
            print(f"    ğŸ“‹ ì…ë ¥ ë°ì´í„°: {n_samples:,}ê°œ ìƒ˜í”Œ, {n_features}ì°¨ì›")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ (ì •ë³´ ì œê³µìš©)
            memory_usage_gb = (n_samples * n_features * 4) / (1024**3)  # float32 ê¸°ì¤€
            print(f"    ğŸ’¾ ì›ë³¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage_gb:.2f}GB")
            
            # 3072ì°¨ì›: í•­ìƒ UMAP ì ìš© (ì¼ê´€ì„± ë° ì„±ëŠ¥ ìµœì í™”)
            if n_features == 3072:
                print(f"    ğŸ¯ 3072ì°¨ì› ê°ì§€ â†’ UMAPìœ¼ë¡œ 512ì°¨ì› ì¶•ì†Œ ì ìš©")
                print(f"    ğŸ“Š ì˜ˆìƒ íš¨ê³¼: ë©”ëª¨ë¦¬ 83% ì ˆì•½, ì²˜ë¦¬ì†ë„ 10-50ë°° í–¥ìƒ")
                
                # UMAP ì°¨ì› ì¶•ì†Œ ì ìš©
                reduced_embeddings = self.reduce_dimensions(embeddings)
                reduced_memory_gb = (n_samples * 512 * 4) / (1024**3)
                print(f"    ğŸ’¾ ì¶•ì†Œ í›„ ë©”ëª¨ë¦¬: {reduced_memory_gb:.2f}GB (ì ˆì•½: {memory_usage_gb - reduced_memory_gb:.2f}GB)")
                
                # ì¶•ì†Œëœ ì°¨ì›ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
                return self.perform_clustering(reduced_embeddings)
                
            # 512ì°¨ì› ì´í•˜: ì§ì ‘ í´ëŸ¬ìŠ¤í„°ë§ (ì´ë¯¸ ìµœì  ì°¨ì›)
            elif n_features <= 512:
                print(f"    âœ… ìµœì  ì°¨ì› ë²”ìœ„ ({n_features}ì°¨ì›), ì§ì ‘ HDBSCAN ì ìš©")
                clusterer = hdbscan.HDBSCAN(**self.hdbscan_params)
                return clusterer.fit_predict(embeddings)
                
            # 512~3072ì°¨ì›: UMAPìœ¼ë¡œ 512ì°¨ì› ì¶•ì†Œ
            else:
                print(f"    ğŸ“‰ ê³ ì°¨ì› ({n_features}ì°¨ì›) â†’ 512ì°¨ì› ì¶•ì†Œ í›„ í´ëŸ¬ìŠ¤í„°ë§")
                reduced_embeddings = self.reduce_dimensions(embeddings)
                return self.perform_clustering(reduced_embeddings)
                    
        except Exception as e:
            print(f"âŒ ìŠ¤ë§ˆíŠ¸ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return np.array([-1] * len(embeddings))
    
    def extract_keywords_from_articles(self, articles: List[Dict[str, Any]]) -> List[str]:
        """ê¸°ì‚¬ë“¤ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not articles:
            return []
        
        # ëª¨ë“  ì œëª©ê³¼ ë¦¬ë“œë¬¸ë‹¨ ìˆ˜ì§‘
        all_texts = []
        for article in articles:
            all_texts.append(article['title'])
            if article.get('lead_paragraph'):
                all_texts.append(article['lead_paragraph'])
        
        # ë‹¨ì–´ ì¶”ì¶œ ë° ì •ì œ
        words = []
        for text in all_texts:
            text_words = text.replace('"', '').replace("'", '').split()
            words.extend(text_words)
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        word_freq = {}
        for word in words:
            word = word.strip('.,!?()[]{}"\'')
            if len(word) > 1 and word not in self.config.STOP_WORDS and not word.isdigit():
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜ (ì„¤ì • ê¸°ë°˜)
        max_keywords = self.config.KEYWORD_SETTINGS['max_keywords']
        min_frequency = self.config.KEYWORD_SETTINGS['min_frequency']
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:max_keywords]
        return [word for word, freq in top_words if freq >= min_frequency]
    
    def create_keyword_based_title(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ì •ë³´ ìƒì„± (í‚¤ì›Œë“œ ë°°ì—´ + í‘œì‹œìš© ì œëª©)"""
        if not articles:
            return {
                'keywords': [],
                'display_title': "ë¯¸ë¶„ë¥˜ ì´ìŠˆ",
                'keyword_count': 0
            }
        
        keywords = self.extract_keywords_from_articles(articles)
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœëŒ€ 8ê°œ, ìµœì†Œ ë¹ˆë„ 2 ì´ìƒ)
        top_keywords = keywords[:8] if len(keywords) >= 8 else keywords
        
        # í‚¤ì›Œë“œ ìˆ˜ì— ë”°ë¥¸ ì²˜ë¦¬
        if len(top_keywords) >= 5:
            # 5ê°œ ì´ìƒ: ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ ì‚¬ìš©
            selected_keywords = top_keywords[:5]
            display_title = " ".join(selected_keywords)
        elif len(top_keywords) >= 2:
            # 2-4ê°œ: ëª¨ë“  í‚¤ì›Œë“œ ì‚¬ìš©
            selected_keywords = top_keywords
            display_title = " ".join(selected_keywords)
        elif len(top_keywords) == 1:
            # 1ê°œ: ë‹¨ì¼ í‚¤ì›Œë“œ
            selected_keywords = top_keywords
            display_title = top_keywords[0]
        else:
            # í‚¤ì›Œë“œ ì—†ìŒ: ê¸°ì‚¬ ìˆ˜ í‘œì‹œ
            selected_keywords = []
            display_title = f"{len(articles)}ê°œ_ê¸°ì‚¬_í´ëŸ¬ìŠ¤í„°"
        
        return {
            'keywords': selected_keywords,           # ìˆœìˆ˜ í‚¤ì›Œë“œ ë°°ì—´
            'display_title': display_title,         # í‘œì‹œìš© ì œëª© (ê°„ê²°)
            'keyword_count': len(selected_keywords), # í‚¤ì›Œë“œ ê°œìˆ˜
            'total_articles': len(articles)          # ê¸°ì‚¬ ìˆ˜
        }
    
    def calculate_keyword_similarity(self, keywords1: List[str], keywords2: List[str]) -> float:
        """í‚¤ì›Œë“œ ë°°ì—´ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)"""
        if not keywords1 or not keywords2:
            return 0.0
            
        set1, set2 = set(keywords1), set(keywords2)
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        if union == 0:
            return 0.0
            
        jaccard_similarity = intersection / union
        
        # ê°€ì¤‘ì¹˜ ì¶”ê°€: í‚¤ì›Œë“œ ìˆ˜ê°€ ë¹„ìŠ·í• ìˆ˜ë¡ ë” ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ íŒë‹¨
        size_similarity = 1 - abs(len(keywords1) - len(keywords2)) / max(len(keywords1), len(keywords2))
        
        # ìµœì¢… ìœ ì‚¬ë„: Jaccard(80%) + í¬ê¸° ìœ ì‚¬ë„(20%)
        final_similarity = jaccard_similarity * 0.8 + size_similarity * 0.2
        
        return final_similarity
    
    def calculate_title_similarity(self, title1: str, title2: str) -> float:
        """ë ˆê±°ì‹œ ì§€ì›: ê¸°ì¡´ ì œëª© ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords1 = title1.replace('ê´€ë ¨ ì´ìŠˆ', '').replace('í´ëŸ¬ìŠ¤í„°', '').replace('_', ' ').split()
        keywords2 = title2.replace('ê´€ë ¨ ì´ìŠˆ', '').replace('í´ëŸ¬ìŠ¤í„°', '').replace('_', ' ').split()
        
        return self.calculate_keyword_similarity(keywords1, keywords2)
    
    def group_similar_clusters(self, clusters: List[Dict[str, Any]]) -> List[List[int]]:
        """ë¹„ìŠ·í•œ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ ê·¸ë£¹í•‘ (ìµœì í™”ëœ ë²„ì „)"""
        groups = []
        used_indices = set()
        
        for i, cluster in enumerate(clusters):
            if i in used_indices:
                continue
            
            # í˜„ì¬ í´ëŸ¬ìŠ¤í„°ì™€ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°ë“¤ ì°¾ê¸°
            similar_group = [i]
            used_indices.add(i)
            
            current_keywords = cluster.get('keywords', [])
            
            for j, other_cluster in enumerate(clusters):
                if j in used_indices:
                    continue
                
                other_keywords = other_cluster.get('keywords', [])
                
                # í‚¤ì›Œë“œ ë°°ì—´ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
                if current_keywords and other_keywords:
                    similarity = self.calculate_keyword_similarity(current_keywords, other_keywords)
                else:
                    # í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš° ë ˆê±°ì‹œ ë°©ì‹ ì‚¬ìš©
                    similarity = self.calculate_title_similarity(
                        cluster['title'], other_cluster['title']
                    )
                
                if similarity >= self.config.THRESHOLDS['title_similarity_threshold']:
                    similar_group.append(j)
                    used_indices.add(j)
            
            groups.append(similar_group)
        
        return groups
    
    def group_similar_titles(self, clusters: List[Dict[str, Any]]) -> List[List[int]]:
        """ë ˆê±°ì‹œ ì§€ì›: ê¸°ì¡´ ì œëª© ê¸°ë°˜ ê·¸ë£¹í•‘"""
        return self.group_similar_clusters(clusters)
    
    def calculate_embedding_similarity(self, articles1: List[Dict[str, Any]], articles2: List[Dict[str, Any]]) -> float:
        """ë‘ í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ì‚¬ë“¤ ê°„ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° (ìµœì í™”ëœ ì„ë² ë”© ì²˜ë¦¬ ì‚¬ìš©)"""
        try:
            # ìµœì í™”ëœ ì„ë² ë”© ì²˜ë¦¬ ë©”ì„œë“œ ì‚¬ìš©
            embeddings1, _ = self.optimize_embedding_processing(articles1)
            embeddings2, _ = self.optimize_embedding_processing(articles2)
            
            if len(embeddings1) == 0 or len(embeddings2) == 0:
                return 0.0
            
            # ê° í´ëŸ¬ìŠ¤í„°ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
            avg_embedding1 = np.mean(embeddings1, axis=0)
            avg_embedding2 = np.mean(embeddings2, axis=0)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity([avg_embedding1], [avg_embedding2])[0][0]
            
            return float(similarity)
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0
    
    def merge_similar_clusters(self, clusters: List[Dict[str, Any]], groups: List[List[int]]) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°ë“¤ í†µí•©"""
        merged_clusters = []
        
        for group in groups:
            if len(group) == 1:
                # ê·¸ë£¹ì— í´ëŸ¬ìŠ¤í„°ê°€ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                merged_clusters.append(clusters[group[0]])
            else:
                # ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ê°€ ìˆìœ¼ë©´ ì„ë² ë”© ìœ ì‚¬ë„ë¡œ í†µí•© ì—¬ë¶€ ê²°ì •
                print(f"  ğŸ” {len(group)}ê°œ í´ëŸ¬ìŠ¤í„° ê·¸ë£¹ ê²€í†  ì¤‘...")
                
                # ì²« ë²ˆì§¸ í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘
                merged_cluster = clusters[group[0]].copy()
                merged_articles = merged_cluster['articles'].copy()
                
                # ë‚˜ë¨¸ì§€ í´ëŸ¬ìŠ¤í„°ë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
                for i in range(1, len(group)):
                    current_cluster = clusters[group[i]]
                    
                    # ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = self.calculate_embedding_similarity(
                        merged_articles, current_cluster['articles']
                    )
                    
                    print(f"    ğŸ“Š ìœ ì‚¬ë„: {similarity:.3f}")
                    
                    if similarity >= self.config.THRESHOLDS['merge_threshold']:
                        # í†µí•©
                        merged_articles.extend(current_cluster['articles'])
                        print(f"    âœ… í†µí•©: {current_cluster['title']}")
                    else:
                        # ë¶„ë¦¬ (ë³„ë„ í´ëŸ¬ìŠ¤í„°ë¡œ ìœ ì§€)
                        merged_clusters.append(current_cluster)
                        print(f"    âŒ ë¶„ë¦¬: {current_cluster['title']}")
                
                # í†µí•©ëœ í´ëŸ¬ìŠ¤í„° ì—…ë°ì´íŠ¸
                merged_cluster['articles'] = merged_articles
                merged_cluster['title'] = self.create_keyword_based_title(merged_articles)
                merged_clusters.append(merged_cluster)
        
        return merged_clusters
    
    def get_real_bias_distribution(self, articles: List[Dict[str, Any]]) -> Tuple[int, int, int]:
        """ì‹¤ì œ ì–¸ë¡ ì‚¬ ì„±í–¥ ê¸°ë°˜ ë¶„ë¥˜ (media_outlets í…Œì´ë¸” ì°¸ì¡°)"""
        left_count = center_count = right_count = 0
        
        try:
            for article in articles:
                media_id = article.get('media_id')
                if not media_id:
                    continue
                    
                # media_outlets í…Œì´ë¸”ì—ì„œ ì„±í–¥ ì •ë³´ ì¡°íšŒ
                media_info = self.supabase_manager.client.table('media_outlets').select(
                    'bias'
                ).eq('id', media_id).execute()
                
                if media_info.data:
                    bias = media_info.data[0].get('bias', 'center')
                    if bias == 'left':
                        left_count += 1
                    elif bias == 'right':
                        right_count += 1
                    else:  # center ë˜ëŠ” ê¸°íƒ€
                        center_count += 1
                else:
                    # ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìœ¼ë©´ ì¤‘ë„ë¡œ ì²˜ë¦¬
                    center_count += 1
                    
        except Exception as e:
            print(f"    âš ï¸ ì„±í–¥ ë¶„ë¥˜ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ì „ì²´ë¥¼ ì¤‘ë„ë¡œ ì²˜ë¦¬
            center_count = len(articles)
            left_count = right_count = 0
            
        return left_count, center_count, right_count
    
    def get_cluster_statistics(self, articles: List[Dict[str, Any]]) -> Dict[str, int]:
        """í´ëŸ¬ìŠ¤í„° í†µê³„ ì •ë³´ ê³„ì‚°"""
        total_articles = len(articles)
        
        return {
            'total_count': total_articles,
            'avg_per_day': total_articles // 7 if total_articles >= 7 else total_articles,
            'cluster_density': min(1.0, total_articles / 10.0)
        }
    
    def save_cluster_to_issues(self, cluster_articles: List[Dict[str, Any]], cluster_id: int) -> Optional[str]:
        """í´ëŸ¬ìŠ¤í„°ë¥¼ issues í…Œì´ë¸”ì— ì €ì¥ (ì‹¤ì œ ì–¸ë¡ ì‚¬ ì„±í–¥ ê¸°ë°˜)"""
        try:
            # ì´ìŠˆ ì •ë³´ ìƒì„±
            issue_info = self.create_keyword_based_title(cluster_articles)
            
            # ì‹¤ì œ ì–¸ë¡ ì‚¬ ì„±í–¥ ê¸°ë°˜ ë¶„ë¥˜
            left_count, center_count, right_count = self.get_real_bias_distribution(cluster_articles)
            total_count = len(cluster_articles)
            
            print(f"      ğŸ“ˆ ì„±í–¥ ë¶„í¬: ì¢Œ({left_count}) ì¤‘({center_count}) ìš°({right_count}) = ì´ {total_count}ê°œ")
            
            # issues í…Œì´ë¸”ì— ì €ì¥
            issue_data = {
                'title': issue_info['display_title'],  # í‚¤ì›Œë“œ ê¸°ë°˜ ì œëª©
                'source': str(total_count),            # ì „ì²´ ê¸°ì‚¬ ìˆ˜
                'left_source': str(left_count),        # ì‹¤ì œ ì¢Œí¸í–¥ ê¸°ì‚¬ ìˆ˜
                'center_source': str(center_count),    # ì‹¤ì œ ì¤‘ë„ ê¸°ì‚¬ ìˆ˜
                'right_source': str(right_count),      # ì‹¤ì œ ìš°í¸í–¥ ê¸°ì‚¬ ìˆ˜
                'created_at': datetime.now().isoformat()
            }
            
            result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
            issue_id = result.data[0]['id']
            
            # issue_articles í…Œì´ë¸”ì— ì €ì¥
            issue_articles_data = []
            for article in cluster_articles:
                issue_articles_data.append({
                    'issue_id': issue_id,
                    'article_id': article['id']
                })
            
            if issue_articles_data:
                self.supabase_manager.client.table('issue_articles').insert(issue_articles_data).execute()
            
            return issue_id
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _prepare_embeddings_for_category(self, category: str) -> Tuple[np.ndarray, List[Dict[str, Any]]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”© ë°ì´í„° ì¤€ë¹„"""
        print(f"  ğŸ”„ ì„ë² ë”© ì²˜ë¦¬ ì¤‘...")
        
        # ê¸°ì‚¬ ì¡°íšŒ
        articles = self.fetch_articles_by_category(category)
        if not articles:
            print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ì— ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return np.array([]), []
        
        print(f"    ğŸ“° ì¡°íšŒëœ ê¸°ì‚¬: {len(articles):,}ê°œ")
        
        # ìµœì í™”ëœ ì„ë² ë”© ì²˜ë¦¬
        embeddings, valid_articles = self.optimize_embedding_processing(articles)
        
        if len(embeddings) == 0:
            print(f"âŒ {category} ìœ íš¨í•œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € embeddings.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return np.array([]), []
        
        print(f"    ğŸ“Š ì„ë² ë”© ë°°ì—´ í˜•íƒœ: {embeddings.shape}")
        print(f"    ğŸ“Š ì„ë² ë”© ì°¨ì›: {embeddings.shape[1] if len(embeddings.shape) > 1 else 'N/A'}")
        print(f"    ğŸ“Š ì²˜ë¦¬ëœ ê¸°ì‚¬: {len(valid_articles)}ê°œ")
        
        return embeddings, valid_articles
    
    def _create_clusters_from_labels(self, cluster_labels: np.ndarray, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í´ëŸ¬ìŠ¤í„° ë¼ë²¨ë¡œë¶€í„° í´ëŸ¬ìŠ¤í„° ê°ì²´ ìƒì„± (ìµœì†Œ í¬ê¸° 3 ê²€ì¦)"""
        unique_clusters = np.unique(cluster_labels)
        clusters = []
        noise_count = np.sum(cluster_labels == -1)  # ë…¸ì´ì¦ˆ ê°œìˆ˜
        
        print(f"    ğŸ“ˆ HDBSCAN ê²°ê³¼: {len(unique_clusters)-1}ê°œ í´ëŸ¬ìŠ¤í„°, ë…¸ì´ì¦ˆ {noise_count}ê°œ")
        
        for cluster_id in unique_clusters:
            if cluster_id == -1:  # ë…¸ì´ì¦ˆ ìŠ¤í‚µ
                continue
            
            cluster_mask = cluster_labels == cluster_id
            cluster_articles = [articles[i] for i in range(len(articles)) if cluster_mask[i]]
            cluster_size = len(cluster_articles)
            
            # ìµœì†Œ í¬ê¸° 3 ê²€ì¦
            if cluster_size >= self.config.THRESHOLDS['min_cluster_size']:
                cluster_info = self.create_keyword_based_title(cluster_articles)
                
                # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
                quality_score = min(1.0, cluster_size / 10.0)  # 10ê°œ ì´ìƒì´ë©´ ìµœê³  í’ˆì§ˆ
                
                clusters.append({
                    'id': cluster_id,
                    'title': cluster_info['display_title'],      # í‘œì‹œìš© ì œëª©
                    'keywords': cluster_info['keywords'],        # ìˆœìˆ˜ í‚¤ì›Œë“œ ë°°ì—´
                    'keyword_count': cluster_info['keyword_count'],
                    'articles': cluster_articles,
                    'size': cluster_size,
                    'quality_score': quality_score,              # í’ˆì§ˆ ì ìˆ˜
                    'density': cluster_size / (noise_count + len(articles))  # ë°€ë„
                })
                print(f"      âœ… í´ëŸ¬ìŠ¤í„° {cluster_id}: {cluster_size}ê°œ ê¸°ì‚¬, í‚¤ì›Œë“œ {cluster_info['keyword_count']}ê°œ")
            else:
                print(f"      âŒ í´ëŸ¬ìŠ¤í„° {cluster_id}: {cluster_size}ê°œ ê¸°ì‚¬ (ìµœì†Œ 3ê°œ ë¯¸ë‹¬ë¡œ ì œì™¸)")
        
        print(f"    ğŸ† ìœ íš¨ í´ëŸ¬ìŠ¤í„°: {len(clusters)}ê°œ (ìµœì†Œ í¬ê¸° 3 ì´ìƒ)")
        return clusters
    
    def _group_and_merge_clusters(self, clusters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í´ëŸ¬ìŠ¤í„° ê·¸ë£¹í•‘ ë° í†µí•© (í‚¤ì›Œë“œ ê¸°ë°˜ ìµœì í™”)"""
        print(f"  ğŸ” ë¹„ìŠ·í•œ í‚¤ì›Œë“œ ê·¸ë£¹í•‘ ì¤‘...")
        groups = self.group_similar_clusters(clusters)
        print(f"    ğŸ“Š í‚¤ì›Œë“œ ê·¸ë£¹: {len(groups)}ê°œ")
        
        print(f"  ğŸ”„ ì„ë² ë”© ê¸°ë°˜ í†µí•© ì¤‘...")
        merged_clusters = self.merge_similar_clusters(clusters, groups)
        print(f"    ğŸ“Š ìµœì¢… í´ëŸ¬ìŠ¤í„°: {len(merged_clusters)}ê°œ")
        
        return merged_clusters
    
    def _save_clusters_to_database(self, clusters: List[Dict[str, Any]]) -> int:
        """í´ëŸ¬ìŠ¤í„°ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ìƒìœ„ 3ê°œë§Œ ì„ ë³„ ì €ì¥)"""
        if not clusters:
            return 0
            
        # ê¸°ì‚¬ ìˆ˜(source) ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        sorted_clusters = sorted(clusters, key=lambda x: len(x['articles']), reverse=True)
        
        print(f"  ğŸ“ˆ ì „ì²´ í´ëŸ¬ìŠ¤í„°: {len(sorted_clusters)}ê°œ")
        for i, cluster in enumerate(sorted_clusters[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
            print(f"    {i}ìœ„: {len(cluster['articles'])}ê°œ ê¸°ì‚¬ - '{cluster['title'][:40]}...'")
        
        # ìƒìœ„ 3ê°œë§Œ ì„ ë³„
        top_clusters = sorted_clusters[:self.config.THRESHOLDS['top_clusters_limit']]
        
        if len(sorted_clusters) > 3:
            excluded_count = len(sorted_clusters) - 3
            excluded_articles = sum(len(cluster['articles']) for cluster in sorted_clusters[3:])
            print(f"  âœ‚ï¸ í•˜ìœ„ {excluded_count}ê°œ í´ëŸ¬ìŠ¤í„° ì œì™¸ ({excluded_articles}ê°œ ê¸°ì‚¬)")
        
        print(f"  ğŸ† ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„°ë§Œ ì €ì¥:")
        
        # ìƒìœ„ 3ê°œë§Œ ì €ì¥
        saved_clusters = 0
        for i, cluster in enumerate(top_clusters, 1):
            print(f"    {i}ìœ„ ì €ì¥ ì¤‘: {len(cluster['articles'])}ê°œ ê¸°ì‚¬ - '{cluster['title']}'")
            issue_id = self.save_cluster_to_issues(cluster['articles'], cluster['id'])
            if issue_id:
                saved_clusters += 1
                print(f"      âœ… ì €ì¥ ì„±ê³µ: issue_id {issue_id}")
            else:
                print(f"      âŒ ì €ì¥ ì‹¤íŒ¨")
                
        return saved_clusters
    
    def process_category(self, category: str) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì²˜ë¦¬"""
        print(f"ğŸ“Š {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„
        embeddings, articles = self._prepare_embeddings_for_category(category)
        if len(embeddings) == 0:
            return {'success': False, 'clusters': 0}
        
        # 2. ìŠ¤ë§ˆíŠ¸ êµ°ì§‘í™”
        print(f"  ğŸ¯ ìŠ¤ë§ˆíŠ¸ êµ°ì§‘í™” ì¤‘...")
        cluster_labels = self.perform_smart_clustering(embeddings)
        
        # 3. í´ëŸ¬ìŠ¤í„° ìƒì„±
        clusters = self._create_clusters_from_labels(cluster_labels, articles)
        print(f"  ğŸ“Š ì´ˆê¸° í´ëŸ¬ìŠ¤í„°: {len(clusters)}ê°œ")
        
        # 4. í´ëŸ¬ìŠ¤í„° ê·¸ë£¹í•‘ ë° í†µí•©
        merged_clusters = self._group_and_merge_clusters(clusters)
        
        # 5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        saved_clusters = self._save_clusters_to_database(merged_clusters)
        
        print(f"  âœ… {category} ì™„ë£Œ: {saved_clusters}ê°œ ì´ìŠˆ ìƒì„±")
        return {'success': True, 'clusters': saved_clusters}
    
    def run_full_pipeline(self, categories: Optional[List[str]] = None) -> bool:
        """ì „ì²´ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
        try:
            print("=" * 60)
            print("ğŸ¯ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (3072ì°¨ì› ìµœì í™”)")
            print("=" * 60)
            
            # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
            memory_info = psutil.virtual_memory()
            print(f"ğŸ’» ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory_info.total / (1024**3):.1f}GB (ì‚¬ìš©ê°€ëŠ¥: {memory_info.available / (1024**3):.1f}GB)")
            print(f"ğŸ’» CPU ì½”ì–´: {self.n_jobs}ê°œ ë³‘ë ¬ ì²˜ë¦¬")
            
            # ì²˜ë¦¬í•  ì¹´í…Œê³ ë¦¬ ê²°ì •
            if categories is None:
                categories = ['êµ­íšŒ/ì •ë‹¹', 'í–‰ì •ë¶€', 'ì‚¬ë²•/ê²€ì°°', 'ì™¸êµ/ì•ˆë³´', 'ì •ì±…/ê²½ì œì‚¬íšŒ', 'ì„ ê±°', 'ì§€ì—­ì •ì¹˜']
            
            total_clusters = 0
            start_time = time.time()
            peak_memory = 0
            
            for i, category in enumerate(categories, 1):
                print(f"\nğŸ“‹ [{i}/{len(categories)}] {category} ì²˜ë¦¬ ì¤‘...")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                memory_before = psutil.virtual_memory().used / (1024**3)
                
                result = self.process_category(category)
                
                memory_after = psutil.virtual_memory().used / (1024**3)
                memory_used = memory_after - memory_before
                peak_memory = max(peak_memory, memory_after)
                
                if result['success']:
                    total_clusters += result['clusters']
                    print(f"  ğŸ“‹ {category}: {result['clusters']}ê°œ ì´ìŠˆ ìƒì„± (ë©”ëª¨ë¦¬: +{memory_used:.2f}GB)")
                else:
                    print(f"  âš ï¸ {category}: ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # ìµœì¢… ê²°ê³¼ ë° ì„±ëŠ¥ í†µê³„
            total_time = time.time() - start_time
            avg_time_per_category = total_time / len(categories)
            
            print(f"\n{'='*60}")
            print(f"ğŸ‰ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
            print(f"âœ… ì´ ìƒì„±ëœ ì´ìŠˆ: {total_clusters}ê°œ")
            print(f"â±ï¸  ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„ (ì¹´í…Œê³ ë¦¬ë‹¹ {avg_time_per_category/60:.1f}ë¶„)")
            print(f"ğŸ’¾ ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.2f}GB")
            print(f"ğŸ“ˆ í‰ê·  ì´ìŠˆ ìƒì„±ë¥ : {total_clusters/len(categories):.1f}ê°œ/ì¹´í…Œê³ ë¦¬")
            print(f"{'='*60}")
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_stats.update({
                'total_articles_processed': sum(result.get('articles_count', 0) for result in [self.process_category(cat) for cat in categories]),
                'successful_clusters': total_clusters,
                'processing_time': total_time,
                'memory_usage_peak': peak_memory
            })
            
            return total_clusters > 0
            
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜ - 3072ì°¨ì› pgvector ìµœì í™” ë²„ì „"""
    try:
        print("ğŸš€ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ v2.0")
        print("ğŸ”‹ pgvector ë„¤ì´í‹°ë¸Œ + 3072ì°¨ì› ìµœì í™”")
        print("ğŸ’¾ ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ + ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        
        # ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = AdvancedClusteringPipeline(batch_size=50)
        success = pipeline.run_full_pipeline()
        
        if success:
            print(f"\nâœ… ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
            
            # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
            stats = pipeline.performance_stats
            if stats['successful_clusters'] > 0:
                print(f"ğŸ“ˆ ì„±ëŠ¥ ìš”ì•½:")
                print(f"  - ì²˜ë¦¬ ì‹œê°„: {stats['processing_time']/60:.1f}ë¶„")
                print(f"  - ìƒì„± ì´ìŠˆ: {stats['successful_clusters']}ê°œ")
                print(f"  - ìµœëŒ€ ë©”ëª¨ë¦¬: {stats['memory_usage_peak']:.2f}GB")
        else:
            print(f"\nâŒ ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()