#!/usr/bin/env python3
"""
ê°œì„ ëœ ì´ìŠˆ í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤í¬ë¦½íŠ¸
- 'ë°' ë¬¸ì œ í•´ê²°: ë” ì„¸ë°€í•œ ì´ìŠˆ ë¶„ë¦¬
- ëª…í™•í•œ ë‹¨ì¼ ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§
"""

import sys
import os
import json
import pytz
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from utils.supabase_manager import SupabaseManager

# OpenAI ì„¤ì¹˜ í™•ì¸ ë° import
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    print("âŒ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("pip install openai ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    OPENAI_AVAILABLE = False

@dataclass
class Article:
    """ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    title: str
    lead_content: str
    media_id: str
    published_at: str

@dataclass
class Cluster:
    """í´ëŸ¬ìŠ¤í„° ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    name: str
    keywords: List[str]
    articles: List[Article]
    issue_number: Optional[int] = None

class ImprovedIssueClustering:
    """ê°œì„ ëœ ì´ìŠˆ í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise Exception("Supabase ì—°ê²° ì‹¤íŒ¨")
        
        if OPENAI_AVAILABLE:
            self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        else:
            self.openai_client = None
        
        # ì–¸ë¡ ì‚¬ ì„±í–¥ ë§¤í•‘ í…Œì´ë¸”
        self.media_bias_mapping = self.get_media_bias_mapping()
    
    def get_media_bias_mapping(self) -> Dict[str, str]:
        """ì–¸ë¡ ì‚¬ ID â†’ ì„±í–¥ ë§¤í•‘ í…Œì´ë¸” ìƒì„±"""
        try:
            result = self.supabase_manager.client.table('media_outlets').select('id, bias').execute()
            return {item['id']: item['bias'] for item in result.data}
        except Exception as e:
            print(f"âŒ ì–¸ë¡ ì‚¬ ì„±í–¥ ë§¤í•‘ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def parse_date_range(self, date_input: str) -> Tuple[str, str]:
        """ì‚¬ìš©ì ì…ë ¥ì„ KST ë‚ ì§œ ë²”ìœ„ë¡œ íŒŒì‹±"""
        try:
            if '~' in date_input:
                start_str, end_str = date_input.split('~')
            else:
                start_str = end_str = date_input.strip()
            
            current_year = datetime.now().year
            
            # í˜•ì‹ ë³€í™˜: 0910 -> 2024-09-10
            start_date = f"{current_year}-{start_str[:2]}-{start_str[2:]}"
            end_date = f"{current_year}-{end_str[:2]}-{end_str[2:]}"
            
            # KST â†’ UTC ë³€í™˜
            kst = pytz.timezone('Asia/Seoul')
            utc = pytz.UTC
            
            # ì‹œì‘: í•´ë‹¹ ë‚  00:00 KST
            start_kst = kst.localize(datetime.strptime(start_date, '%Y-%m-%d'))
            
            # ì¢…ë£Œ: ë‹¤ìŒë‚  00:00 KST (í•´ë‹¹ ë‚  23:59:59ê¹Œì§€ í¬í•¨)
            end_kst = kst.localize(datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1))
            
            start_utc = start_kst.astimezone(utc)
            end_utc = end_kst.astimezone(utc)
            
            return start_utc.isoformat(), end_utc.isoformat()
            
        except Exception as e:
            raise ValueError(f"ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {e}")
    
    def fetch_articles_by_date_range(self, start_date: str, end_date: str) -> List[Article]:
        """ë‚ ì§œ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë§Œ ì¡°íšŒ"""
        try:
            print(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date}")
            
            result = self.supabase_manager.client.table('articles').select(
                'id, title, content, media_id, published_at'
            ).eq('is_preprocessed', True).gte('published_at', start_date).lt('published_at', end_date).execute()
            
            if not result.data:
                print("âŒ í•´ë‹¹ ë‚ ì§œ ë²”ìœ„ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            articles = []
            for item in result.data:
                # ì²« ë‘ ë¬¸ì¥ ì¶”ì¶œ (ë§¥ë½ê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜•)
                first_two_sentences = ""
                if item['content']:
                    first_two_sentences = self.extract_first_two_sentences(item['content'])
                
                article = Article(
                    id=item['id'],
                    title=item['title'],
                    lead_content=first_two_sentences,
                    media_id=item['media_id'],
                    published_at=item['published_at']
                )
                articles.append(article)
            
            print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
            return articles
            
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def extract_first_two_sentences(self, content: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ì²« ë‘ ë¬¸ì¥ ì¶”ì¶œ (ë§¥ë½ê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜•)"""
        if not content:
            return ""
        
        # ë¬¸ì¥ë“¤ì„ ë§ˆì¹¨í‘œë¡œ ë¶„ë¦¬
        sentences = content.split('.')
        
        # ì²« ë‘ ë¬¸ì¥ ì¶”ì¶œ
        if len(sentences) >= 2:
            first_two = sentences[0].strip() + '. ' + sentences[1].strip() + '.'
        elif len(sentences) == 1:
            first_two = sentences[0].strip() + '.'
        else:
            first_two = content.split('\n')[0].strip()
        
        # ë„ˆë¬´ ê¸¸ë©´ 150ìë¡œ ì œí•œ
        if len(first_two) > 150:
            first_two = first_two[:150] + "..."
        
        return first_two
    
    def split_into_2_batches(self, articles: List[Article]) -> List[List[Article]]:
        """2ê°œ ë°°ì¹˜ë¡œ ê· ë“± ë¶„í•  (í† í° ì œí•œ í•´ê²°)"""
        batch_size = len(articles) // 2
        batches = []
        
        for i in range(2):
            start_idx = i * batch_size
            if i == 1:  # ë§ˆì§€ë§‰ ë°°ì¹˜
                end_idx = len(articles)
            else:
                end_idx = (i + 1) * batch_size
            
            batch = articles[start_idx:end_idx]
            batches.append(batch)
        
        return batches
    
    def create_precise_clustering_prompt(self, articles: List[Article]) -> str:
        """ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡¬í”„íŠ¸ - ë‹¨ì¼ ì£¼ì œ ê°•ì¡°"""
        articles_text = ""
        for i, article in enumerate(articles, 1):
            articles_text += f"{i}. ì œëª©: {article.title}\n"
            articles_text += f"   ë‚´ìš©: {article.lead_content}\n\n"
        
        prompt = f"""
ë‹¤ìŒì€ ì •ì¹˜ ë‰´ìŠ¤ì˜ ì œëª©ê³¼ ì²« ë‘ ë¬¸ì¥ì…ë‹ˆë‹¤. **í•˜ë‚˜ì˜ ëª…í™•í•œ ì •ì¹˜ ì´ìŠˆ**ë¥¼ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ë“¤ë§Œ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ê¸°ì‚¬ ëª©ë¡:
{articles_text}

ğŸš¨ **ì¤‘ìš”í•œ ë¶„ë¥˜ ì›ì¹™** ğŸš¨

1. **ë‹¨ì¼ ì£¼ì œ ì›ì¹™**: ê° ê·¸ë£¹ì€ ë°˜ë“œì‹œ **í•˜ë‚˜ì˜ êµ¬ì²´ì ì¸ ì •ì¹˜ ì´ìŠˆ**ë§Œ ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤
   - âŒ "ì‚¬ë²•ê°œí˜ ë° ê²€ì°° ìˆ˜ì‚¬" â†’ 2ê°œ ì´ìŠˆê°€ ì„ì„
   - âœ… "ì‚¬ë²•ê°œí˜" ë˜ëŠ” "ê²€ì°° ìˆ˜ì‚¬" â†’ ê°ê° ë³„ë„ ê·¸ë£¹

2. **êµ¬ì²´ì  ì‚¬ê±´/ì •ì±… ì¤‘ì‹¬**: ì¶”ìƒì ì´ì§€ ì•Šê³  êµ¬ì²´ì ì¸ ì‚¬ê±´ì´ë‚˜ ì •ì±…ìœ¼ë¡œ ë¶„ë¥˜
   - âœ… "ì´ì¬ëª… ëŒ€í‘œ ì‚¬ë²• ë¦¬ìŠ¤í¬"
   - âœ… "ìœ¤ì„ì—´ ì •ë¶€ ì˜ë£Œì§„ ì§‘ë‹¨í–‰ë™ ëŒ€ì‘"
   - âœ… "í•œë™í›ˆ ë‹¹ëŒ€í‘œ ì„ ì¶œ"
   - âŒ "ì •ì¹˜ ê°ˆë“±" (ë„ˆë¬´ í¬ê´„ì )

3. **ì¸ë¬¼ ì¤‘ì‹¬ ì´ìŠˆ**: íŠ¹ì • ì •ì¹˜ì¸ê³¼ ê´€ë ¨ëœ êµ¬ì²´ì  ì‚¬ê±´
   - âœ… "ì´ì¬ëª… í—¬ê¸° ì´ì†¡ ì˜í˜¹"
   - âœ… "í•œë™í›ˆ ì „ ì¥ê´€ ê²€ì°° ì¶œì„"
   - âŒ "ì•¼ë‹¹ ëŒ€í‘œë“¤ ë™í–¥" (ì—¬ëŸ¬ ì¸ë¬¼ ì„ì„)

4. **ì •ì±…ë³„ ë¶„ë¦¬**: ì„œë¡œ ë‹¤ë¥¸ ì •ì±… ì˜ì—­ì€ ë°˜ë“œì‹œ ë¶„ë¦¬
   - âœ… "ì˜ë£Œì§„ ì§‘ë‹¨í–‰ë™" (ì˜ë£Œì •ì±…)
   - âœ… "ë¶€ë™ì‚° ì •ì±…" (ë¶€ë™ì‚°ì •ì±…)
   - âŒ "ë¯¼ìƒì •ì±…" (ì—¬ëŸ¬ ì •ì±… ì„ì„)

5. **ì‹œê¸°ë³„ êµ¬ë¶„**: ê°™ì€ ì´ìŠˆë¼ë„ ì‹œê¸°ê°€ ë‹¤ë¥´ë©´ ë¶„ë¦¬ ê³ ë ¤
   - âœ… "9ì›” ì˜ëŒ€ ì¦ì› ë°œí‘œ"
   - âœ… "10ì›” ì˜ëŒ€ ì¦ì› í›„ì†ì¡°ì¹˜"

ğŸ¯ **ëª©í‘œ**: 'ë°', 'ê·¸ë¦¬ê³ ', 'ë“±' ì—†ì´ **ëª…í™•í•œ ë‹¨ì¼ ì£¼ì œëª…**

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "clusters": [
        {{
            "name": "êµ¬ì²´ì ì¸ ë‹¨ì¼ ì •ì¹˜ ì´ìŠˆëª…",
            "keywords": ["í•µì‹¬í‚¤ì›Œë“œ1", "í•µì‹¬í‚¤ì›Œë“œ2", "í•µì‹¬í‚¤ì›Œë“œ3"],
            "article_indices": [1, 3, 5],
            "confidence": "high|medium|low",
            "reasoning": "ì´ ê¸°ì‚¬ë“¤ì´ ê°™ì€ ê·¸ë£¹ì¸ ì´ìœ "
        }}
    ]
}}
"""
        return prompt
    
    def create_merge_prompt(self, all_clusters: List[List[Cluster]]) -> str:
        """ë°°ì¹˜ ê²°ê³¼ í†µí•©ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        clusters_text = ""
        cluster_id = 1
        
        for batch_idx, batch_clusters in enumerate(all_clusters, 1):
            clusters_text += f"\n=== ë°°ì¹˜ {batch_idx} ê²°ê³¼ ===\n"
            for cluster in batch_clusters:
                clusters_text += f"{cluster_id}. {cluster.name}\n"
                clusters_text += f"   í‚¤ì›Œë“œ: {', '.join(cluster.keywords)}\n"
                clusters_text += f"   ê¸°ì‚¬ ìˆ˜: {len(cluster.articles)}ê°œ\n"
                # ì²« ì„¸ ë¬¸ì¥ ìƒ˜í”Œ ì¶”ê°€
                sample_articles = cluster.articles[:2]
                clusters_text += f"   ë‚´ìš© ìƒ˜í”Œ:\n"
                for i, article in enumerate(sample_articles, 1):
                    clusters_text += f"     {i}. {article.lead_content[:100]}...\n"
                clusters_text += "\n"
                cluster_id += 1
        
        prompt = f"""
ë‹¤ìŒì€ 2ê°œ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ í´ëŸ¬ìŠ¤í„°ë§í•œ ì •ì¹˜ ë‰´ìŠ¤ ê²°ê³¼ì…ë‹ˆë‹¤. ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì •ì¹˜ ì´ìŠˆë“¤ì„ ì •í™•í•˜ê²Œ í•©ì³ì„œ ìµœì¢… ì´ìŠˆ ëª©ë¡ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:
{clusters_text}

ğŸš¨ **ì¤‘ìš”í•œ í†µí•© ì›ì¹™** ğŸš¨

1. **ë‹¨ì¼ ì£¼ì œ ìœ ì§€**: í†µí•© ì‹œì—ë„ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ êµ¬ì²´ì ì¸ ì •ì¹˜ ì´ìŠˆë§Œ ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤
   - âŒ "ì‚¬ë²•ê°œí˜ ë° ê²€ì°° ìˆ˜ì‚¬" â†’ 2ê°œ ì´ìŠˆê°€ ì„ì„
   - âœ… "ì‚¬ë²•ê°œí˜" ë˜ëŠ” "ê²€ì°° ìˆ˜ì‚¬" â†’ ê°ê° ë³„ë„ ì´ìŠˆ

2. **ì •ì¹˜ì  ë§¥ë½ ê³ ë ¤**: ê°™ì€ ì •ì¹˜ ì´ìŠˆì˜ ë‹¤ë¥¸ í‘œí˜„ë“¤ì„ ì¸ì‹í•˜ì„¸ìš”
   - ì˜ˆ: "ì¡°í¬ëŒ€ ëŒ€ë²•ì›ì¥ ì‚¬í‡´"ì™€ "ëŒ€ë²•ì›ì¥ ì‚¬í‡´ ìš”êµ¬"ëŠ” ê°™ì€ ì´ìŠˆ
   - ì˜ˆ: "í•œë¯¸ ê´€ì„¸ í˜‘ìƒ"ê³¼ "ë¯¸êµ­ ê´€ì„¸ í˜‘ìƒ"ì€ ê°™ì€ ì´ìŠˆ

3. **ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒë‹¨**: ë‹¨ìˆœ í‚¤ì›Œë“œê°€ ì•„ë‹Œ ì „ì²´ì  ë§¥ë½ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”
   - ì²« ì„¸ ë¬¸ì¥ì˜ í•µì‹¬ ì£¼ì œê°€ ê°™ì€ì§€ í™•ì¸
   - ì •ì¹˜ì  ë§¥ë½ê³¼ ë°°ê²½ì´ ìœ ì‚¬í•œì§€ í™•ì¸

4. **'ë°' ê¸ˆì§€**: ì ˆëŒ€ë¡œ ì—¬ëŸ¬ ì´ìŠˆë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ì§€ ë§ˆì„¸ìš”
   - ê° ì´ìŠˆëŠ” ëª…í™•í•œ ë‹¨ì¼ ì£¼ì œì—¬ì•¼ í•¨

ìš”êµ¬ì‚¬í•­:
1. ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì •ì¹˜ ì´ìŠˆë“¤ì„ í•˜ë‚˜ë¡œ í•©ì³ì£¼ì„¸ìš”
2. ê° ìµœì¢… ì´ìŠˆì— ì ì ˆí•œ ì´ë¦„ì„ ì œê³µí•´ì£¼ì„¸ìš” (ë‹¨ì¼ ì£¼ì œ)
3. ê° ì´ìŠˆì˜ í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”
4. í•©ì³ì§„ ì´ìŠˆì— í¬í•¨ëœ ì›ë³¸ í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë“¤ì„ ê¸°ë¡í•´ì£¼ì„¸ìš”

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "final_clusters": [
        {{
            "name": "ìµœì¢… ì •ì¹˜ ì´ìŠˆëª… (ë‹¨ì¼ ì£¼ì œ)",
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
            "merged_from": [1, 5, 8],
            "confidence": "high|medium|low"
        }}
    ]
}}
"""
        return prompt
    
    def merge_batch_clusters(self, all_clusters: List[List[Cluster]]) -> Optional[List[Cluster]]:
        """ë°°ì¹˜ë³„ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë“¤ì„ í†µí•©"""
        if not self.openai_client:
            print("âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            print("ğŸ”„ ë°°ì¹˜ ê²°ê³¼ í†µí•© ì¤‘...")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.create_merge_prompt(all_clusters)
            
            # LLM í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì¹˜ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë°°ì¹˜ì˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì •ì¹˜ ì´ìŠˆë“¤ì„ ì •í™•í•˜ê²Œ í•©ì³ì£¼ì„¸ìš”. ì ˆëŒ€ë¡œ ì—¬ëŸ¬ ì´ìŠˆë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ì§€ ë§ˆì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=6000
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content
            print("ğŸ“ í†µí•© ê²°ê³¼ ë°›ìŒ")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                json_text = response_text[json_start:json_end]
                
                result = json.loads(json_text)
                final_clusters_data = result.get('final_clusters', [])
                
                # ëª¨ë“  ì›ë³¸ í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°
                all_original_clusters = []
                for batch_clusters in all_clusters:
                    all_original_clusters.extend(batch_clusters)
                
                # ìµœì¢… í´ëŸ¬ìŠ¤í„° ìƒì„±
                final_clusters = []
                for i, cluster_data in enumerate(final_clusters_data):
                    merged_from_indices = cluster_data.get('merged_from', [])
                    merged_articles = []
                    confidence = cluster_data.get('confidence', 'medium')
                    
                    # í†µí•©ëœ í´ëŸ¬ìŠ¤í„°ë“¤ì˜ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘
                    for idx in merged_from_indices:
                        if 1 <= idx <= len(all_original_clusters):
                            original_cluster = all_original_clusters[idx - 1]
                            merged_articles.extend(original_cluster.articles)
                    
                    # ì¤‘ë³µ ê¸°ì‚¬ ì œê±°
                    unique_articles = []
                    seen_ids = set()
                    for article in merged_articles:
                        if article.id not in seen_ids:
                            unique_articles.append(article)
                            seen_ids.add(article.id)
                    
                    final_cluster = Cluster(
                        id=f"final_cluster_{i+1}",
                        name=cluster_data.get('name', f'í†µí•© ì´ìŠˆ {i+1}'),
                        keywords=cluster_data.get('keywords', []),
                        articles=unique_articles
                    )
                    final_clusters.append(final_cluster)
                    
                    print(f"  ğŸ“Š í†µí•© ì´ìŠˆ {i+1}: {final_cluster.name} ({len(unique_articles)}ê°œ ê¸°ì‚¬, ì‹ ë¢°ë„: {confidence})")
                
                print(f"âœ… {len(final_clusters)}ê°œ ìµœì¢… í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ")
                return final_clusters
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                print(f"ì‘ë‹µ ë‚´ìš©: {response_text[:500]}...")
                return None
                
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ í†µí•© ì‹¤íŒ¨: {str(e)}")
            return None
    
    def cluster_with_precise_llm(self, articles: List[Article]) -> Optional[List[Cluster]]:
        """ê°œì„ ëœ LLM í´ëŸ¬ìŠ¤í„°ë§"""
        if not self.openai_client:
            print("âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            print("ğŸ¯ ê°œì„ ëœ LLM í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.create_precise_clustering_prompt(articles)
            
            # LLM í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì¹˜ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ë“¤ì„ ëª…í™•í•œ ë‹¨ì¼ ì£¼ì œ ì´ìŠˆë¡œë§Œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”. ì ˆëŒ€ë¡œ ì—¬ëŸ¬ ì´ìŠˆë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ì§€ ë§ˆì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì¶¤
                max_tokens=6000
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content
            print("ğŸ“ LLM ì‘ë‹µ ë°›ìŒ")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                json_text = response_text[json_start:json_end]
                
                result = json.loads(json_text)
                clusters_data = result.get('clusters', [])
                
                # Cluster ê°ì²´ë¡œ ë³€í™˜
                clusters = []
                for i, cluster_data in enumerate(clusters_data):
                    article_indices = cluster_data.get('article_indices', [])
                    cluster_articles = [articles[idx-1] for idx in article_indices if 1 <= idx <= len(articles)]
                    
                    # 'ë°' ë¬¸ì œ ê²€ì¦
                    cluster_name = cluster_data.get('name', f'ê·¸ë£¹ {i+1}')
                    confidence = cluster_data.get('confidence', 'medium')
                    reasoning = cluster_data.get('reasoning', '')
                    
                    # ê²½ê³  ì²´í¬
                    warning_words = ['ë°', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ì•„ìš¸ëŸ¬', 'ë™ì‹œì—', 'í•¨ê»˜']
                    has_warning = any(word in cluster_name for word in warning_words)
                    
                    if has_warning:
                        print(f"âš ï¸ ì˜ì‹¬ í´ëŸ¬ìŠ¤í„°: '{cluster_name}' - ë³µìˆ˜ ì´ìŠˆ ê°€ëŠ¥ì„±")
                    
                    cluster = Cluster(
                        id=f"cluster_{i+1}",
                        name=cluster_name,
                        keywords=cluster_data.get('keywords', []),
                        articles=cluster_articles
                    )
                    clusters.append(cluster)
                
                print(f"âœ… {len(clusters)}ê°œ ì •ë°€ í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ")
                return clusters
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                print(f"ì‘ë‹µ ë‚´ìš©: {response_text[:500]}...")
                return None
                
        except Exception as e:
            print(f"âŒ LLM í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def validate_clusters(self, clusters: List[Cluster]) -> List[Cluster]:
        """í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê²€ì¦ ë° í•„í„°ë§"""
        validated_clusters = []
        
        print("ğŸ” í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        for cluster in clusters:
            issues = []
            
            # 1. 'ë°' ë“± ë³µìˆ˜ ì´ìŠˆ í‚¤ì›Œë“œ ì²´í¬
            warning_words = ['ë°', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ì•„ìš¸ëŸ¬', 'ë™ì‹œì—', 'í•¨ê»˜', 'ë“±']
            has_multiple_issues = any(word in cluster.name for word in warning_words)
            if has_multiple_issues:
                issues.append(f"ë³µìˆ˜ ì´ìŠˆ ì˜ì‹¬: '{cluster.name}'")
            
            # 2. ë„ˆë¬´ í¬ê´„ì ì¸ ì´ë¦„ ì²´í¬
            generic_words = ['ì •ì¹˜', 'ì •ë¶€', 'ì—¬ë‹¹', 'ì•¼ë‹¹', 'êµ­ì •', 'ì •ì±…', 'ë™í–¥', 'ìƒí™©']
            is_too_generic = any(word == cluster.name or cluster.name.startswith(word) for word in generic_words)
            if is_too_generic:
                issues.append(f"ë„ˆë¬´ í¬ê´„ì : '{cluster.name}'")
            
            # 3. ìµœì†Œ ê¸°ì‚¬ ìˆ˜ ì²´í¬ (10ê°œ ì´ìƒ)
            if len(cluster.articles) < 10:
                issues.append(f"ê¸°ì‚¬ ìˆ˜ ë¶€ì¡±: {len(cluster.articles)}ê°œ")
                continue  # 10ê°œ ë¯¸ë§Œì€ ì œì™¸
            
            # 4. ê²½ê³ ì‚¬í•­ ì¶œë ¥
            if issues:
                print(f"âš ï¸  {cluster.name}: {', '.join(issues)}")
            else:
                print(f"âœ… {cluster.name}: {len(cluster.articles)}ê°œ ê¸°ì‚¬")
            
            validated_clusters.append(cluster)
        
        print(f"ğŸ” ê²€ì¦ ì™„ë£Œ: {len(clusters)} â†’ {len(validated_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        return validated_clusters
    
    def assign_issue_numbers(self, clusters: List[Cluster]) -> List[Cluster]:
        """ê¸°ì‚¬ ìˆ˜ ìˆœìœ¼ë¡œ ì´ìŠˆ ë²ˆí˜¸ í• ë‹¹"""
        # ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        sorted_clusters = sorted(clusters, key=lambda x: len(x.articles), reverse=True)
        
        for i, cluster in enumerate(sorted_clusters, 1):
            cluster.issue_number = i
            # ì´ìŠˆ ë²ˆí˜¸ëŠ” ê´„í˜¸ë¡œ ë³„ë„ í‘œì‹œ
            cluster.name = f"{cluster.name}"
        
        return sorted_clusters
    
    def count_media_bias(self, cluster: Cluster) -> Dict[str, int]:
        """ì–¸ë¡ ì‚¬ ì„±í–¥ë³„ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°"""
        bias_counts = {'left': 0, 'center': 0, 'right': 0}
        
        for article in cluster.articles:
            bias = self.media_bias_mapping.get(article.media_id, 'center')
            if bias in bias_counts:
                bias_counts[bias] += 1
        
        return bias_counts
    
    def save_to_issues_table(self, clusters: List[Cluster]) -> bool:
        """í´ëŸ¬ìŠ¤í„°ë¥¼ issues í…Œì´ë¸”ì— ì €ì¥"""
        try:
            print(f"ğŸ’¾ {len(clusters)}ê°œ ì´ìŠˆë¥¼ issues í…Œì´ë¸”ì— ì €ì¥ ì¤‘...")
            
            issues_data = []
            for cluster in clusters:
                if not cluster.articles:  # ê¸°ì‚¬ê°€ ì—†ëŠ” í´ëŸ¬ìŠ¤í„°ëŠ” ê±´ë„ˆë›°ê¸°
                    continue
                
                # ì–¸ë¡ ì‚¬ ì„±í–¥ë³„ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
                bias_counts = self.count_media_bias(cluster)
                
                issue_data = {
                    "title": cluster.name,
                    "source": len(cluster.articles),
                    "left_source": bias_counts['left'],
                    "center_source": bias_counts['center'],
                    "right_source": bias_counts['right'],
                    "created_at": datetime.now().isoformat()
                }
                issues_data.append(issue_data)
            
            if not issues_data:
                print("âš ï¸ ì €ì¥í•  ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # issues í…Œì´ë¸”ì— ì €ì¥
            result = self.supabase_manager.client.table('issues').insert(issues_data).execute()
            
            if result.data:
                print(f"âœ… {len(result.data)}ê°œ ì´ìŠˆ ì €ì¥ ì™„ë£Œ")
                
                # issue_articles í…Œì´ë¸”ì— ì—°ê²° ì •ë³´ ì €ì¥
                self.save_issue_articles_connections(clusters, result.data)
                return True
            else:
                print("âŒ ì´ìŠˆ ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì´ìŠˆ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def save_issue_articles_connections(self, clusters: List[Cluster], saved_issues: List[Dict]) -> bool:
        """issue_articles í…Œì´ë¸”ì— ì—°ê²° ì •ë³´ ì €ì¥"""
        try:
            print("ğŸ”— ì´ìŠˆ-ê¸°ì‚¬ ì—°ê²° ì •ë³´ ì €ì¥ ì¤‘...")
            
            connections = []
            for i, cluster in enumerate(clusters):
                if i >= len(saved_issues) or not cluster.articles:
                    continue
                
                issue_id = saved_issues[i]['id']
                for article in cluster.articles:
                    connection = {
                        "issue_id": issue_id,
                        "article_id": article.id
                    }
                    connections.append(connection)
            
            if connections:
                result = self.supabase_manager.client.table('issue_articles').insert(connections).execute()
                if result.data:
                    print(f"âœ… {len(result.data)}ê°œ ì—°ê²° ì •ë³´ ì €ì¥ ì™„ë£Œ")
                    return True
            
            print("âš ï¸ ì—°ê²° ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ ì—°ê²° ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def run_improved_clustering(self, start_date: str, end_date: str) -> bool:
        """ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"""
        try:
            print("ğŸ¯ ê°œì„ ëœ ì´ìŠˆ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
            print("="*60)
            
            # 1. ê¸°ì‚¬ ì¡°íšŒ
            articles = self.fetch_articles_by_date_range(start_date, end_date)
            if not articles:
                return False
            
            # 2. ì§ì ‘ í´ëŸ¬ìŠ¤í„°ë§ (ì œëª© ê¸°ë°˜, ë°°ì¹˜ ë¶ˆí•„ìš”)
            print(f"ğŸ”„ {len(articles)}ê°œ ê¸°ì‚¬ ì œëª© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
            clusters = self.cluster_with_precise_llm(articles)
            if not clusters:
                print("âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
                return False
            
            # 3. í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê²€ì¦
            validated_clusters = self.validate_clusters(clusters)
            if not validated_clusters:
                print("âŒ ê²€ì¦ëœ í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 4. ì´ìŠˆ ë²ˆí˜¸ í• ë‹¹
            numbered_clusters = self.assign_issue_numbers(validated_clusters)
            
            # 5. ê²°ê³¼ ë¶„ì„
            print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
            print("="*60)
            for i, cluster in enumerate(numbered_clusters, 1):
                bias_counts = self.count_media_bias(cluster)
                print(f"ì´ìŠˆ {i:2d}: {cluster.name}")
                print(f"        ê¸°ì‚¬ ìˆ˜: {len(cluster.articles)}ê°œ (ì¢Œ:{bias_counts['left']}, ì¤‘:{bias_counts['center']}, ìš°:{bias_counts['right']})")
                print(f"        í‚¤ì›Œë“œ: {', '.join(cluster.keywords)}")
                print()
            
            # 6. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            save_success = self.save_to_issues_table(numbered_clusters)
            if not save_success:
                print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨")
                return False
            
            print(f"âœ… ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
            print(f"ğŸ“ˆ {len(articles)}ê°œ ê¸°ì‚¬ â†’ {len(numbered_clusters)}ê°œ ëª…í™•í•œ ì´ìŠˆ")
            return True
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ ê°œì„ ëœ LLM ê¸°ë°˜ ì´ìŠˆ í´ëŸ¬ìŠ¤í„°ë§")
    print("ğŸš¨ 'ë°' ë¬¸ì œ í•´ê²° ë²„ì „")
    print("="*60)
    
    try:
        # ë‚ ì§œ ë²”ìœ„ ì…ë ¥
        print("\nğŸ“… í´ëŸ¬ìŠ¤í„°ë§í•  ë‚ ì§œ ë²”ìœ„ë¥¼ ì„ íƒí•˜ì„¸ìš” (KST ê¸°ì¤€)")
        print("ì˜ˆ: 0910~0920, 0901~0930, 0915~0915")
        
        while True:
            date_input = input("\në‚ ì§œ ë²”ìœ„ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if not date_input:
                print("âŒ ë‚ ì§œ ë²”ìœ„ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            try:
                clustering = ImprovedIssueClustering()
                start_date, end_date = clustering.parse_date_range(date_input)
                break
            except ValueError as e:
                print(f"âŒ {e}")
                continue
        
        # ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        success = clustering.run_improved_clustering(start_date, end_date)
        
        if success:
            print("\nğŸ‰ ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ ì„±ê³µ!")
            print("âœ… ê° ì´ìŠˆëŠ” ëª…í™•í•œ ë‹¨ì¼ ì£¼ì œë¥¼ ê°€ì§‘ë‹ˆë‹¤")
        else:
            print("\nâŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
