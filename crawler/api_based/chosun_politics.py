#!/usr/bin/env python3
"""
ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (API ê¸°ë°˜)
- story-card-by-id APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë³„ ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘
- ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ë§Œ í•„í„°ë§
- ë³¸ë¬¸ì€ Playwrightë¡œ ë³„ë„ ìˆ˜ì§‘
"""

import asyncio
import json
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin, quote
import httpx
import pytz
from bs4 import BeautifulSoup
from rich.console import Console
from rich.table import Table
from playwright.async_api import async_playwright

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")


class ChosunPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.chosun.com"
        self.media_name = "ì¡°ì„ ì¼ë³´"
        self.media_bias = "right"
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        self._playwright = None
        self._browser = None

    async def _get_politics_article_ids(self, max_articles: int = 50) -> List[str]:
        """ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ ID ëª©ë¡ ìˆ˜ì§‘"""
        console.print("ğŸ”Œ ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ ID ìˆ˜ì§‘ ì‹œì‘...")
        
        api_base = "https://www.chosun.com/pf/api/v3/content/fetch/story-feed"
        article_ids = []
        offset = 0
        size = 50
        
        async with httpx.AsyncClient(timeout=5.0) as client:  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
            while len(article_ids) < max_articles:
                try:
                    console.print(f"ğŸ“¡ API í˜¸ì¶œ (offset: {offset})")
                    
                    query_params = {
                        "query": json.dumps({
                            "excludeContentTypes": "gallery, video",
                            "includeContentTypes": "story",
                            "includeSections": "/politics",
                            "offset": offset,
                            "size": size
                        }),
                        "_website": "chosun"
                    }
                    
                    resp = await client.get(api_base, params=query_params)
                    resp.raise_for_status()
                    data = resp.json()

                    content_elements = data.get("content_elements", [])
                    console.print(f"ğŸ“Š API ì‘ë‹µ: {len(content_elements)}ê°œ ìš”ì†Œ ìˆ˜ì‹ ")
                    
                    if not content_elements:
                        console.print("âš ï¸ ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                        break
                    
                    for element in content_elements:
                        if len(article_ids) >= max_articles:
                            break
                        article_id = element.get("_id")
                        if article_id:
                            article_ids.append(article_id)
                    
                    console.print(f"ğŸ“ˆ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ID: {len(article_ids)}ê°œ")
                    offset += size
                    # ëŒ€ê¸° ì‹œê°„ ì œê±°
                    
                except Exception as e:
                    console.print(f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                    break

        console.print(f"ğŸ¯ ì´ {len(article_ids)}ê°œ ê¸°ì‚¬ ID ìˆ˜ì§‘ ì™„ë£Œ")
        return article_ids

    async def _get_article_details(self, article_id: str) -> Optional[Dict]:
        """ê°œë³„ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        api_url = "https://www.chosun.com/pf/api/v3/content/fetch/story-card-by-id"
        
        # API í•„í„° ì„¤ì • (ì œê³µí•´ì£¼ì‹  ê²ƒê³¼ ë™ì¼)
        filter_data = {
            "_id": "",
            "canonical_url": "",
            "credits": {
                "by": {
                    "_id": "",
                    "additional_properties": {
                        "original": {
                            "affiliations": "",
                            "byline": ""
                        }
                    },
                    "name": "",
                    "org": "",
                    "url": ""
                }
            },
            "description": {
                "basic": ""
            },
            "display_date": "",
            "first_publish_date": "",
            "headlines": {
                "basic": "",
                "mobile": ""
            },
            "label": {
                "membership_icon": {
                    "text": ""
                },
                "shoulder_title": {
                    "text": "",
                    "url": ""
                },
                "video_icon": {
                    "text": ""
                }
            },
            "last_updated_date": "",
            "liveblogging_content": {
                "basic": {
                    "date": "",
                    "headline": "",
                    "id": "",
                    "url": "",
                    "website": ""
                }
            },
            "promo_items": {
                "basic": {
                    "_id": "",
                    "additional_properties": {
                        "focal_point": {
                            "max": "",
                            "min": ""
                        }
                    },
                    "alt_text": "",
                    "caption": "",
                    "content": "",
                    "content_elements": {
                        "_id": "",
                        "alignment": "",
                        "alt_text": "",
                        "caption": "",
                        "content": "",
                        "credits": {
                            "affiliation": {
                                "name": ""
                            },
                            "by": {
                                "_id": "",
                                "byline": "",
                                "name": "",
                                "org": ""
                            }
                        },
                        "height": "",
                        "resizedUrls": {
                            "16x9_lg": "",
                            "16x9_md": "",
                            "16x9_sm": "",
                            "16x9_xxl": ""
                        },
                        "subtype": "",
                        "type": "",
                        "url": "",
                        "width": ""
                    },
                    "credits": {
                        "affiliation": {
                            "byline": "",
                            "name": ""
                        },
                        "by": {
                            "byline": "",
                            "name": ""
                        }
                    },
                    "description": {
                        "basic": ""
                    },
                    "embed_html": "",
                    "focal_point": {
                        "x": "",
                        "y": ""
                    },
                    "headlines": {
                        "basic": ""
                    },
                    "height": "",
                    "promo_items": {
                        "basic": {
                            "_id": "",
                            "height": "",
                            "resizedUrls": {
                                "16x9_lg": "",
                                "16x9_md": "",
                                "16x9_sm": "",
                                "16x9_xxl": ""
                            },
                            "subtype": "",
                            "type": "",
                            "url": "",
                            "width": ""
                        }
                    },
                    "resizedUrls": {
                        "16x9_lg": "",
                        "16x9_md": "",
                        "16x9_sm": "",
                        "16x9_xxl": ""
                    },
                    "streams": {
                        "height": "",
                        "width": ""
                    },
                    "subtype": "",
                    "type": "",
                    "url": "",
                    "websites": "",
                    "width": ""
                },
                "lead_art": {
                    "duration": "",
                    "type": ""
                }
            },
            "related_content": {
                "basic": {
                    "_id": "",
                    "absolute_canonical_url": "",
                    "headlines": {
                        "basic": "",
                        "mobile": ""
                    },
                    "referent": {
                        "id": "",
                        "type": ""
                    },
                    "type": ""
                }
            },
            "subheadlines": {
                "basic": ""
            },
            "subtype": "",
            "taxonomy": {
                "primary_section": {
                    "_id": "",
                    "name": ""
                },
                "tags": {
                    "slug": "",
                    "text": ""
                }
            },
            "type": "",
            "website_url": ""
        }
        
        query_data = {
            "arr": "",
            "expandLiveBlogging": False,
            "expandRelated": False,
            "id": article_id,
            "published": ""
        }
        
        async with httpx.AsyncClient(timeout=5.0) as client:  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
            try:
                params = {
                    "query": json.dumps(query_data),
                    "filter": json.dumps(filter_data),
                    "d": "1925",
                    "mxId": "00000000",
                    "_website": "chosun"
                }
                
                resp = await client.get(api_url, params=params)
                resp.raise_for_status()
                data = resp.json()
                
                return self._parse_article_data(data)
                
            except Exception as e:
                console.print(f"âŒ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ({article_id}): {e}")
                return None

    def _parse_article_data(self, data: Dict) -> Optional[Dict]:
        """API ì‘ë‹µ ë°ì´í„° íŒŒì‹±"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            title = data.get("headlines", {}).get("basic")
            if not title:
                return None

            canonical_url = data.get("canonical_url")
            if not canonical_url:
                return None
            url = urljoin(self.base_url, canonical_url) if canonical_url.startswith("/") else canonical_url

            # ë‚ ì§œ ì²˜ë¦¬ - ì—¬ëŸ¬ í•„ë“œ ì¤‘ ê°€ì¥ ì ì ˆí•œ ê²ƒ ì„ íƒ
            published_at = None
            
            # last_updated_dateë§Œ ì‚¬ìš© (ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„)
            date_fields = [
                data.get("last_updated_date")
            ]
            
            for date_field in date_fields:
                if date_field:
                    try:
                        # ISO í˜•ì‹ ì •ê·œí™”
                        if date_field.endswith("Z"):
                            date_field = date_field.replace("Z", "+00:00")
                        
                        # ì†Œìˆ˜ì  ìë¦¿ìˆ˜ ì •ê·œí™” (ìµœëŒ€ 6ìë¦¬)
                        if "." in date_field and "+" in date_field:
                            # Tì™€ + ì‚¬ì´ì˜ ì‹œê°„ ë¶€ë¶„ ì°¾ê¸°
                            t_index = date_field.find("T")
                            plus_index = date_field.find("+")
                            if t_index != -1 and plus_index != -1:
                                time_part = date_field[t_index+1:plus_index]
                                if "." in time_part:
                                    seconds_part = time_part.split(".")[0]
                                    decimal_part = time_part.split(".")[1]
                                    # ì†Œìˆ˜ì ì„ 6ìë¦¬ë¡œ ë§ì¶¤
                                    decimal_part = decimal_part.ljust(6, "0")[:6]
                                    normalized_time = seconds_part + "." + decimal_part
                                    date_field = date_field[:t_index+1] + normalized_time + date_field[plus_index:]
                        
                        # APIì—ì„œ ì œê³µí•˜ëŠ” ì‹œê°„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³€í™˜í•˜ì§€ ì•ŠìŒ)
                        published_at = date_field
                        break  # ì²« ë²ˆì§¸ë¡œ ì„±ê³µí•œ ë‚ ì§œ ì‚¬ìš©
                            
                    except Exception as e:
                        console.print(f"âš ï¸ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {date_field} - {e}")
                        continue

            # ê¸°ì ì •ë³´
            credits = data.get("credits", {}).get("by", [])
            author = ""
            if credits and len(credits) > 0:
                author = credits[0].get("name", "")

            # ì„¹ì…˜ ì •ë³´
            taxonomy = data.get("taxonomy", {})
            section = taxonomy.get("primary_section", {}).get("name", "")

            # íƒœê·¸ ì •ë³´
            tags = taxonomy.get("tags", [])
            tag_list = [tag.get("text", "") for tag in tags if tag.get("text")]

            return {
                "title": title.strip(),
                "url": url,
                "content": "",  # ë³¸ë¬¸ì€ ë‚˜ì¤‘ì— Playwrightë¡œ ì±„ì›€
                "published_at": published_at,  # APIì—ì„œ ì œê³µí•˜ëŠ” ì‹œê°„ (ì—†ì„ ìˆ˜ë„ ìˆìŒ)
                "created_at": datetime.now(KST).isoformat(),  # ìˆ˜ì§‘ ì‹œì ì˜ í˜„ì¬ ì‹œê°„ (í•­ìƒ ì¡´ì¬)
                "author": author,
                "section": section,
                "tags": tag_list,
                "description": data.get("description", {}).get("basic", ""),
            }
        except Exception as e:
            console.print(f"âŒ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    async def _collect_articles(self, max_articles: int = 100):
        """ê¸°ì‚¬ ìˆ˜ì§‘ (ID ìˆ˜ì§‘ â†’ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘) - ë³‘ë ¬ ì²˜ë¦¬"""
        console.print(f"ğŸš€ ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_articles}ê°œ)")
        
        # 1ë‹¨ê³„: ê¸°ì‚¬ ID ìˆ˜ì§‘
        article_ids = await self._get_politics_article_ids(max_articles)
        
        if not article_ids:
            console.print("âŒ ìˆ˜ì§‘í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2ë‹¨ê³„: ë³‘ë ¬ë¡œ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (20ê°œì”© ë°°ì¹˜)
        console.print(f"ğŸ“– {len(article_ids)}ê°œ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘... (ë³‘ë ¬ ì²˜ë¦¬)")
        
        batch_size = 20
        success_count = 0
        
        for i in range(0, len(article_ids), batch_size):
            batch_ids = article_ids[i:i + batch_size]
            console.print(f"ğŸ“„ ë°°ì¹˜ {i//batch_size + 1}/{(len(article_ids) + batch_size - 1)//batch_size} ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [self._get_article_details(article_id) for article_id in batch_ids]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for j, result in enumerate(results):
                if isinstance(result, Exception):
                    console.print(f"âŒ [{i + j + 1}/{len(article_ids)}] ì˜¤ë¥˜: {str(result)[:50]}")
                elif result:
                    self.articles.append(result)
                    success_count += 1
                    console.print(f"âœ… [{i + j + 1}/{len(article_ids)}] {result['title'][:30]}...")
                else:
                    console.print(f"âš ï¸ [{i + j + 1}/{len(article_ids)}] ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨")
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸°
            if i + batch_size < len(article_ids):
                await asyncio.sleep(0.1)

        console.print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(article_ids)}ê°œ ì„±ê³µ")

    async def _extract_content(self, url: str) -> str:
        """Playwrightë¡œ ë³¸ë¬¸ ì „ë¬¸ ì¶”ì¶œ"""
        page = None
        try:
            if not self._browser:
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox', 
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--memory-pressure-off'
                    ]
                )

            page = await self._browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 720})
            await page.goto(url, wait_until="domcontentloaded", timeout=10000)

            # ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì¶”ì¶œ
            content = ""
            
            try:
                content = await page.evaluate('''() => {
                    const selectors = [
                        'section.article-body p',
                        'div#article-body p',
                        'div.article-body p',
                        'article.article-body p',
                        '.story-news p',
                        '.article-content p',
                        'main p',
                        'article p'
                    ];
                    
                    for (const selector of selectors) {
                        const paragraphs = document.querySelectorAll(selector);
                        if (paragraphs.length > 0) {
                            const texts = Array.from(paragraphs)
                                .map(p => p.textContent.trim())
                                .filter(text => text.length > 20)
                                .slice(0, 20);
                            
                            if (texts.length > 0) {
                                return texts.join('\\n\\n');
                            }
                        }
                    }
                    
                    return "";
                }''')
                
                if content and len(content.strip()) > 50:
                    return content.strip()
                    
            except Exception as e:
                console.print(f"âš ï¸ JavaScript ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:50]}")
            
            return content.strip()
            
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url[:50]}...): {str(e)[:50]}")
            return ""
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass

    async def collect_contents(self):
        """ë³¸ë¬¸ ì „ë¬¸ ìˆ˜ì§‘ - ë³‘ë ¬ ì²˜ë¦¬"""
        if not self.articles:
            return

        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬ (ë³‘ë ¬ ì²˜ë¦¬)")
        
        batch_size = 10  # ë” í° ë°°ì¹˜ í¬ê¸°
        success_count = 0
        
        for i in range(0, len(self.articles), batch_size):
            batch = self.articles[i:i + batch_size]
            console.print(f"ğŸ“„ ë°°ì¹˜ {i//batch_size + 1}/{(len(self.articles) + batch_size - 1)//batch_size} ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [self._extract_content(art["url"]) for art in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for j, (art, result) in enumerate(zip(batch, results)):
                if isinstance(result, Exception):
                    console.print(f"âŒ [{i + j + 1}/{len(self.articles)}] ì˜¤ë¥˜: {str(result)[:50]}")
                elif result:
                    self.articles[i + j]["content"] = result
                    success_count += 1
                    console.print(f"âœ… [{i + j + 1}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
                else:
                    console.print(f"âš ï¸ [{i + j + 1}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸°
            if i + batch_size < len(self.articles):
                await asyncio.sleep(0.5)

        console.print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(self.articles)}ê°œ ì„±ê³µ")

    async def save_to_supabase(self):
        """DB ì €ì¥"""
        if not self.articles:
            console.print("âŒ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")

        # ì–¸ë¡ ì‚¬ í™•ì¸
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
        else:
            media_id = media["id"]

        # ì¤‘ë³µ ì²´í¬
        urls = [art["url"] for art in self.articles]
        existing_urls = set()
        
        try:
            for url in urls:
                exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                if exists.data:
                    existing_urls.add(url)
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")

        success, failed, skipped = 0, 0, 0
        
        for i, art in enumerate(self.articles, 1):
            try:
                if art["url"] in existing_urls:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {art['title'][:30]}...")
                    skipped += 1
                    continue

                published_at_str = None
                created_at_str = None
                
                if isinstance(art["published_at"], datetime):
                    published_at_str = art["published_at"].strftime('%Y-%m-%d %H:%M:%S')
                    created_at_str = art["created_at"].strftime('%Y-%m-%d %H:%M:%S') if art.get("created_at") else published_at_str
                elif art.get("published_at"):
                    published_at_str = art["published_at"]
                    created_at_str = art.get("created_at", published_at_str)

                article_data = {
                    "media_id": media_id,
                    "title": art["title"],
                    "content": art["content"],
                    "url": art["url"],
                    "published_at": published_at_str,
                    "created_at": created_at_str,
                }

                if self.supabase_manager.insert_article(article_data):
                    success += 1
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {art['title'][:30]}...")
                else:
                    failed += 1
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {art['title'][:30]}...")
                    
            except Exception as e:
                failed += 1
                console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì˜¤ë¥˜: {str(e)[:50]}")

        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success}ê°œ")
        console.print(f"  âŒ ì‹¤íŒ¨: {failed}ê°œ") 
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skipped}ê°œ")
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(success / len(self.articles) * 100):.1f}%")

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self._browser:
                await self._browser.close()
                self._browser = None
            if self._playwright:
                await self._playwright.stop()
                self._playwright = None
            console.print("ğŸ§¹ Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            console.print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}")

    async def run(self, max_articles: int = 50):
        """ì‹¤í–‰"""
        try:
            console.print(f"ğŸš€ ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_articles}ê°œ)")
            
            await self._collect_articles(max_articles)
            await self.collect_contents()
            await self.save_to_supabase()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            await self.cleanup()


async def main():
    collector = ChosunPoliticsCollector()
    await collector.run(max_articles=100)

if __name__ == "__main__":
    asyncio.run(main())