#!/usr/bin/env python3
"""
Ï°∞ÏÑ†ÏùºÎ≥¥ Ï†ïÏπò Í∏∞ÏÇ¨ ÌÅ¨Î°§Îü¨ (API Í∏∞Î∞ò)
- story-card-by-id APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Í∞úÎ≥Ñ Í∏∞ÏÇ¨ Ï†ïÎ≥¥ ÏàòÏßë
- Ï†ïÏπò ÏÑπÏÖò Í∏∞ÏÇ¨Îßå ÌïÑÌÑ∞ÎßÅ
- Î≥∏Î¨∏ÏùÄ PlaywrightÎ°ú Î≥ÑÎèÑ ÏàòÏßë
"""

import asyncio
import json
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin, quote
import httpx
import pytz
from bs4 import BeautifulSoup
from rich.console import Console
from rich.table import Table
from playwright.async_api import async_playwright

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Ï∂îÍ∞Ä
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ÎÇ¥Î∂Ä Î™®Îìà
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")


class ChosunPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.chosun.com"
        self.media_name = "Ï°∞ÏÑ†ÏùºÎ≥¥"
        self.media_bias = "right"
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        self._playwright = None
        self._browser = None

    async def _get_politics_article_ids(self, max_articles: int = 50) -> List[str]:
        """Ï†ïÏπò ÏÑπÏÖò Í∏∞ÏÇ¨ ID Î™©Î°ù ÏàòÏßë"""
        console.print("üîå Ï†ïÏπò ÏÑπÏÖò Í∏∞ÏÇ¨ ID ÏàòÏßë ÏãúÏûë...")
        
        api_base = "https://www.chosun.com/pf/api/v3/content/fetch/story-feed"
        article_ids = []
        offset = 0
        size = 50
        
        async with httpx.AsyncClient(timeout=5.0) as client:  # ÌÉÄÏûÑÏïÑÏõÉ Îã®Ï∂ï
            while len(article_ids) < max_articles:
                try:
                    console.print(f"üì° API Ìò∏Ï∂ú (offset: {offset})")
                    
                    query_params = {
                        "query": json.dumps({
                            "excludeContentTypes": "gallery, video",
                            "includeContentTypes": "story",
                            "includeSections": "/politics",
                            "offset": offset,
                            "size": size
                        }),
                        "_website": "chosun"
                    }
                    
                    resp = await client.get(api_base, params=query_params)
                    resp.raise_for_status()
                    data = resp.json()

                    content_elements = data.get("content_elements", [])
                    console.print(f"üìä API ÏùëÎãµ: {len(content_elements)}Í∞ú ÏöîÏÜå ÏàòÏã†")
                    
                    if not content_elements:
                        console.print("‚ö†Ô∏è Îçî Ïù¥ÏÉÅ Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§")
                        break
                    
                    for element in content_elements:
                        if len(article_ids) >= max_articles:
                            break
                        article_id = element.get("_id")
                        if article_id:
                            article_ids.append(article_id)
                    
                    console.print(f"üìà ÏàòÏßëÎêú Í∏∞ÏÇ¨ ID: {len(article_ids)}Í∞ú")
                    offset += size
                    # ÎåÄÍ∏∞ ÏãúÍ∞Ñ Ï†úÍ±∞
                    
                except Exception as e:
                    console.print(f"‚ùå API Ìò∏Ï∂ú Ïò§Î•ò: {e}")
                    break

        console.print(f"üéØ Ï¥ù {len(article_ids)}Í∞ú Í∏∞ÏÇ¨ ID ÏàòÏßë ÏôÑÎ£å")
        return article_ids

    async def _get_article_details(self, article_id: str) -> Optional[Dict]:
        """Í∞úÎ≥Ñ Í∏∞ÏÇ¨ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÏàòÏßë"""
        api_url = "https://www.chosun.com/pf/api/v3/content/fetch/story-card-by-id"
        
        # API ÌïÑÌÑ∞ ÏÑ§Ï†ï (Ï†úÍ≥µÌï¥Ï£ºÏã† Í≤ÉÍ≥º ÎèôÏùº)
        filter_data = {
            "_id": "",
            "canonical_url": "",
            "credits": {
                "by": {
                    "_id": "",
                    "additional_properties": {
                        "original": {
                            "affiliations": "",
                            "byline": ""
                        }
                    },
                    "name": "",
                    "org": "",
                    "url": ""
                }
            },
            "description": {
                "basic": ""
            },
            "display_date": "",
            "first_publish_date": "",
            "headlines": {
                "basic": "",
                "mobile": ""
            },
            "label": {
                "membership_icon": {
                    "text": ""
                },
                "shoulder_title": {
                    "text": "",
                    "url": ""
                },
                "video_icon": {
                    "text": ""
                }
            },
            "last_updated_date": "",
            "liveblogging_content": {
                "basic": {
                    "date": "",
                    "headline": "",
                    "id": "",
                    "url": "",
                    "website": ""
                }
            },
            "promo_items": {
                "basic": {
                    "_id": "",
                    "additional_properties": {
                        "focal_point": {
                            "max": "",
                            "min": ""
                        }
                    },
                    "alt_text": "",
                    "caption": "",
                    "content": "",
                    "content_elements": {
                        "_id": "",
                        "alignment": "",
                        "alt_text": "",
                        "caption": "",
                        "content": "",
                        "credits": {
                            "affiliation": {
                                "name": ""
                            },
                            "by": {
                                "_id": "",
                                "byline": "",
                                "name": "",
                                "org": ""
                            }
                        },
                        "height": "",
                        "resizedUrls": {
                            "16x9_lg": "",
                            "16x9_md": "",
                            "16x9_sm": "",
                            "16x9_xxl": ""
                        },
                        "subtype": "",
                        "type": "",
                        "url": "",
                        "width": ""
                    },
                    "credits": {
                        "affiliation": {
                            "byline": "",
                            "name": ""
                        },
                        "by": {
                            "byline": "",
                            "name": ""
                        }
                    },
                    "description": {
                        "basic": ""
                    },
                    "embed_html": "",
                    "focal_point": {
                        "x": "",
                        "y": ""
                    },
                    "headlines": {
                        "basic": ""
                    },
                    "height": "",
                    "promo_items": {
                        "basic": {
                            "_id": "",
                            "height": "",
                            "resizedUrls": {
                                "16x9_lg": "",
                                "16x9_md": "",
                                "16x9_sm": "",
                                "16x9_xxl": ""
                            },
                            "subtype": "",
                            "type": "",
                            "url": "",
                            "width": ""
                        }
                    },
                    "resizedUrls": {
                        "16x9_lg": "",
                        "16x9_md": "",
                        "16x9_sm": "",
                        "16x9_xxl": ""
                    },
                    "streams": {
                        "height": "",
                        "width": ""
                    },
                    "subtype": "",
                    "type": "",
                    "url": "",
                    "websites": "",
                    "width": ""
                },
                "lead_art": {
                    "duration": "",
                    "type": ""
                }
            },
            "related_content": {
                "basic": {
                    "_id": "",
                    "absolute_canonical_url": "",
                    "headlines": {
                        "basic": "",
                        "mobile": ""
                    },
                    "referent": {
                        "id": "",
                        "type": ""
                    },
                    "type": ""
                }
            },
            "subheadlines": {
                "basic": ""
            },
            "subtype": "",
            "taxonomy": {
                "primary_section": {
                    "_id": "",
                    "name": ""
                },
                "tags": {
                    "slug": "",
                    "text": ""
                }
            },
            "type": "",
            "website_url": ""
        }
        
        query_data = {
            "arr": "",
            "expandLiveBlogging": False,
            "expandRelated": False,
            "id": article_id,
            "published": ""
        }
        
        async with httpx.AsyncClient(timeout=5.0) as client:  # ÌÉÄÏûÑÏïÑÏõÉ Îã®Ï∂ï
            try:
                params = {
                    "query": json.dumps(query_data),
                    "filter": json.dumps(filter_data),
                    "d": "1925",
                    "mxId": "00000000",
                    "_website": "chosun"
                }
                
                resp = await client.get(api_url, params=params)
                resp.raise_for_status()
                data = resp.json()
                
                return self._parse_article_data(data)
                
            except Exception as e:
                console.print(f"‚ùå Í∏∞ÏÇ¨ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå® ({article_id}): {e}")
                return None

    def _parse_article_data(self, data: Dict) -> Optional[Dict]:
        """API ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ ÌååÏã±"""
        try:
            # ÌïÑÏàò ÌïÑÎìú ÌôïÏù∏
            title = data.get("headlines", {}).get("basic")
            if not title:
                return None

            canonical_url = data.get("canonical_url")
            if not canonical_url:
                return None
            url = urljoin(self.base_url, canonical_url) if canonical_url.startswith("/") else canonical_url

            # ÎÇ†Ïßú Ï≤òÎ¶¨ - Ïó¨Îü¨ ÌïÑÎìú Ï§ë Í∞ÄÏû• Ï†ÅÏ†àÌïú Í≤É ÏÑ†ÌÉù
            published_at = None
            
            # last_updated_dateÎßå ÏÇ¨Ïö© (ÎßàÏßÄÎßâ ÏóÖÎç∞Ïù¥Ìä∏ ÏãúÍ∞Ñ)
            date_fields = [
                data.get("last_updated_date")
            ]
            
            for date_field in date_fields:
                if date_field:
                    try:
                        # ISO ÌòïÏãù Ï†ïÍ∑úÌôî
                        if date_field.endswith("Z"):
                            date_field = date_field.replace("Z", "+00:00")
                        
                        # ÏÜåÏàòÏ†ê ÏûêÎ¶øÏàò Ï†ïÍ∑úÌôî (ÏµúÎåÄ 6ÏûêÎ¶¨)
                        if "." in date_field and "+" in date_field:
                            # TÏôÄ + ÏÇ¨Ïù¥Ïùò ÏãúÍ∞Ñ Î∂ÄÎ∂Ñ Ï∞æÍ∏∞
                            t_index = date_field.find("T")
                            plus_index = date_field.find("+")
                            if t_index != -1 and plus_index != -1:
                                time_part = date_field[t_index+1:plus_index]
                                if "." in time_part:
                                    seconds_part = time_part.split(".")[0]
                                    decimal_part = time_part.split(".")[1]
                                    # ÏÜåÏàòÏ†êÏùÑ 6ÏûêÎ¶¨Î°ú ÎßûÏ∂§
                                    decimal_part = decimal_part.ljust(6, "0")[:6]
                                    normalized_time = seconds_part + "." + decimal_part
                                    date_field = date_field[:t_index+1] + normalized_time + date_field[plus_index:]
                        
                        # APIÏóêÏÑú Ï†úÍ≥µÌïòÎäî ÏãúÍ∞ÑÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö© (Î≥ÄÌôòÌïòÏßÄ ÏïäÏùå)
                        published_at = date_field
                        break  # Ï≤´ Î≤àÏß∏Î°ú ÏÑ±Í≥µÌïú ÎÇ†Ïßú ÏÇ¨Ïö©
                            
                    except Exception as e:
                        console.print(f"‚ö†Ô∏è ÎÇ†Ïßú Î≥ÄÌôò Ïã§Ìå®: {date_field} - {e}")
                        continue

            # Í∏∞Ïûê Ï†ïÎ≥¥
            credits = data.get("credits", {}).get("by", [])
            author = ""
            if credits and len(credits) > 0:
                author = credits[0].get("name", "")

            # ÏÑπÏÖò Ï†ïÎ≥¥
            taxonomy = data.get("taxonomy", {})
            section = taxonomy.get("primary_section", {}).get("name", "")

            # ÌÉúÍ∑∏ Ï†ïÎ≥¥
            tags = taxonomy.get("tags", [])
            tag_list = [tag.get("text", "") for tag in tags if tag.get("text")]

            return {
                "title": title.strip(),
                "url": url,
                "content": "",  # Î≥∏Î¨∏ÏùÄ ÎÇòÏ§ëÏóê PlaywrightÎ°ú Ï±ÑÏõÄ
                "published_at": published_at,  # APIÏóêÏÑú Ï†úÍ≥µÌïòÎäî ÏãúÍ∞Ñ (ÏóÜÏùÑ ÏàòÎèÑ ÏûàÏùå)
                "created_at": datetime.now(KST).isoformat(),  # ÏàòÏßë ÏãúÏ†êÏùò ÌòÑÏû¨ ÏãúÍ∞Ñ (Ìï≠ÏÉÅ Ï°¥Ïû¨)
                "author": author,
                "section": section,
                "tags": tag_list,
                "description": data.get("description", {}).get("basic", ""),
            }
        except Exception as e:
            console.print(f"‚ùå Îç∞Ïù¥ÌÑ∞ ÌååÏã± Ïã§Ìå®: {e}")
            return None

    async def _collect_articles(self, max_articles: int = 100):
        """Í∏∞ÏÇ¨ ÏàòÏßë (ID ÏàòÏßë ‚Üí ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÏàòÏßë) - Î≥ëÎ†¨ Ï≤òÎ¶¨"""
        console.print(f"üöÄ Ï°∞ÏÑ†ÏùºÎ≥¥ Ï†ïÏπò Í∏∞ÏÇ¨ ÏàòÏßë ÏãúÏûë (ÏµúÎåÄ {max_articles}Í∞ú)")
        
        # 1Îã®Í≥Ñ: Í∏∞ÏÇ¨ ID ÏàòÏßë
        article_ids = await self._get_politics_article_ids(max_articles)
        
        if not article_ids:
            console.print("‚ùå ÏàòÏßëÌï† Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return

        # 2Îã®Í≥Ñ: Î≥ëÎ†¨Î°ú Í∏∞ÏÇ¨ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÏàòÏßë (20Í∞úÏî© Î∞∞Ïπò)
        console.print(f"üìñ {len(article_ids)}Í∞ú Í∏∞ÏÇ¨ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÏàòÏßë Ï§ë... (Î≥ëÎ†¨ Ï≤òÎ¶¨)")
        
        batch_size = 20
        success_count = 0
        
        for i in range(0, len(article_ids), batch_size):
            batch_ids = article_ids[i:i + batch_size]
            console.print(f"üìÑ Î∞∞Ïπò {i//batch_size + 1}/{(len(article_ids) + batch_size - 1)//batch_size} Ï≤òÎ¶¨ Ï§ë...")
            
            # Î∞∞Ïπò ÎÇ¥ÏóêÏÑú Î≥ëÎ†¨ Ï≤òÎ¶¨
            tasks = [self._get_article_details(article_id) for article_id in batch_ids]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for j, result in enumerate(results):
                if isinstance(result, Exception):
                    console.print(f"‚ùå [{i + j + 1}/{len(article_ids)}] Ïò§Î•ò: {str(result)[:50]}")
                elif result:
                    self.articles.append(result)
                    success_count += 1
                    console.print(f"‚úÖ [{i + j + 1}/{len(article_ids)}] {result['title'][:30]}...")
                else:
                    console.print(f"‚ö†Ô∏è [{i + j + 1}/{len(article_ids)}] Í∏∞ÏÇ¨ Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå®")
            
            # Î∞∞Ïπò Í∞Ñ ÏßßÏùÄ ÎåÄÍ∏∞
            if i + batch_size < len(article_ids):
                await asyncio.sleep(0.1)

        console.print(f"üìä ÏàòÏßë ÏôÑÎ£å: {success_count}/{len(article_ids)}Í∞ú ÏÑ±Í≥µ")

    async def _extract_content(self, url: str) -> str:
        """PlaywrightÎ°ú Î≥∏Î¨∏ Ï†ÑÎ¨∏ Ï∂îÏ∂ú"""
        page = None
        try:
            if not self._browser:
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox', 
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--memory-pressure-off'
                    ]
                )

            page = await self._browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 720})
            await page.goto(url, wait_until="domcontentloaded", timeout=10000)

            # Ï°∞ÏÑ†ÏùºÎ≥¥ Î≥∏Î¨∏ Ï∂îÏ∂ú
            content = ""
            
            try:
                content = await page.evaluate('''() => {
                    const selectors = [
                        'section.article-body p',
                        'div#article-body p',
                        'div.article-body p',
                        'article.article-body p',
                        '.story-news p',
                        '.article-content p',
                        'main p',
                        'article p'
                    ];
                    
                    for (const selector of selectors) {
                        const paragraphs = document.querySelectorAll(selector);
                        if (paragraphs.length > 0) {
                            const texts = Array.from(paragraphs)
                                .map(p => p.textContent.trim())
                                .filter(text => text.length > 20)
                                .slice(0, 20);
                            
                            if (texts.length > 0) {
                                return texts.join('\\n\\n');
                            }
                        }
                    }
                    
                    return "";
                }''')
                
                if content and len(content.strip()) > 50:
                    return content.strip()
                    
            except Exception as e:
                console.print(f"‚ö†Ô∏è JavaScript Î≥∏Î¨∏ Ï∂îÏ∂ú Ïã§Ìå®: {str(e)[:50]}")
            
            return content.strip()
            
        except Exception as e:
            console.print(f"‚ùå Î≥∏Î¨∏ Ï∂îÏ∂ú Ïã§Ìå® ({url[:50]}...): {str(e)[:50]}")
            return ""
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass

    async def collect_contents(self):
        """Î≥∏Î¨∏ Ï†ÑÎ¨∏ ÏàòÏßë - Î≥ëÎ†¨ Ï≤òÎ¶¨"""
        if not self.articles:
            return

        console.print(f"üìñ Î≥∏Î¨∏ ÏàòÏßë ÏãúÏûë: {len(self.articles)}Í∞ú Í∏∞ÏÇ¨ (Î≥ëÎ†¨ Ï≤òÎ¶¨)")
        
        batch_size = 10  # Îçî ÌÅ∞ Î∞∞Ïπò ÌÅ¨Í∏∞
        success_count = 0
        
        for i in range(0, len(self.articles), batch_size):
            batch = self.articles[i:i + batch_size]
            console.print(f"üìÑ Î∞∞Ïπò {i//batch_size + 1}/{(len(self.articles) + batch_size - 1)//batch_size} Ï≤òÎ¶¨ Ï§ë...")
            
            # Î∞∞Ïπò ÎÇ¥ÏóêÏÑú Î≥ëÎ†¨ Ï≤òÎ¶¨
            tasks = [self._extract_content(art["url"]) for art in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for j, (art, result) in enumerate(zip(batch, results)):
                if isinstance(result, Exception):
                    console.print(f"‚ùå [{i + j + 1}/{len(self.articles)}] Ïò§Î•ò: {str(result)[:50]}")
                elif result:
                    self.articles[i + j]["content"] = result
                    success_count += 1
                    console.print(f"‚úÖ [{i + j + 1}/{len(self.articles)}] Î≥∏Î¨∏ ÏàòÏßë ÏÑ±Í≥µ")
                else:
                    console.print(f"‚ö†Ô∏è [{i + j + 1}/{len(self.articles)}] Î≥∏Î¨∏ ÏàòÏßë Ïã§Ìå®")
            
            # Î∞∞Ïπò Í∞Ñ ÏßßÏùÄ ÎåÄÍ∏∞
            if i + batch_size < len(self.articles):
                await asyncio.sleep(0.5)

        console.print(f"‚úÖ Î≥∏Î¨∏ ÏàòÏßë ÏôÑÎ£å: {success_count}/{len(self.articles)}Í∞ú ÏÑ±Í≥µ")

    async def save_to_supabase(self):
        """DB Ï†ÄÏû•"""
        if not self.articles:
            console.print("‚ùå Ï†ÄÏû•Ìï† Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return

        console.print(f"üíæ SupabaseÏóê {len(self.articles)}Í∞ú Í∏∞ÏÇ¨ Ï†ÄÏû• Ï§ë...")

        # Ïñ∏Î°†ÏÇ¨ ÌôïÏù∏
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
        else:
            media_id = media["id"]

        # Ï§ëÎ≥µ Ï≤¥ÌÅ¨
        urls = [art["url"] for art in self.articles]
        existing_urls = set()
        
        try:
            for url in urls:
                exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                if exists.data:
                    existing_urls.add(url)
        except Exception as e:
            console.print(f"‚ö†Ô∏è Ï§ëÎ≥µ Ï≤¥ÌÅ¨ Ï§ë Ïò§Î•ò: {e}")

        success, failed, skipped = 0, 0, 0
        
        for i, art in enumerate(self.articles, 1):
            try:
                if art["url"] in existing_urls:
                    console.print(f"‚ö†Ô∏è [{i}/{len(self.articles)}] Ï§ëÎ≥µ Í∏∞ÏÇ¨ Ïä§ÌÇµ: {art['title'][:30]}...")
                    skipped += 1
                    continue

                published_at_str = None
                created_at_str = None
                
                if isinstance(art["published_at"], datetime):
                    published_at_str = art["published_at"].strftime('%Y-%m-%d %H:%M:%S')
                    created_at_str = art["created_at"].strftime('%Y-%m-%d %H:%M:%S') if art.get("created_at") else published_at_str
                elif art.get("published_at"):
                    published_at_str = art["published_at"]
                    created_at_str = art.get("created_at", published_at_str)

                article_data = {
                    "media_id": media_id,
                    "title": art["title"],
                    "content": art["content"],
                    "url": art["url"],
                    "published_at": published_at_str,
                    "created_at": created_at_str,
                }

                if self.supabase_manager.insert_article(article_data):
                    success += 1
                    console.print(f"‚úÖ [{i}/{len(self.articles)}] Ï†ÄÏû• ÏÑ±Í≥µ: {art['title'][:30]}...")
                else:
                    failed += 1
                    console.print(f"‚ùå [{i}/{len(self.articles)}] Ï†ÄÏû• Ïã§Ìå®: {art['title'][:30]}...")
                    
            except Exception as e:
                failed += 1
                console.print(f"‚ùå [{i}/{len(self.articles)}] Ï†ÄÏû• Ïò§Î•ò: {str(e)[:50]}")

        console.print(f"\nüìä Ï†ÄÏû• Í≤∞Í≥º:")
        console.print(f"  ‚úÖ ÏÑ±Í≥µ: {success}Í∞ú")
        console.print(f"  ‚ùå Ïã§Ìå®: {failed}Í∞ú") 
        console.print(f"  ‚ö†Ô∏è Ï§ëÎ≥µ Ïä§ÌÇµ: {skipped}Í∞ú")
        console.print(f"  üìà ÏÑ±Í≥µÎ•†: {(success / len(self.articles) * 100):.1f}%")

    async def cleanup(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            if self._browser:
                await self._browser.close()
                self._browser = None
            if self._playwright:
                await self._playwright.stop()
                self._playwright = None
            console.print("üßπ Playwright Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
        except Exception as e:
            console.print(f"‚ö†Ô∏è Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)[:50]}")

    async def run(self, max_articles: int = 50):
        """Ïã§Ìñâ"""
        try:
            console.print(f"üöÄ Ï°∞ÏÑ†ÏùºÎ≥¥ Ï†ïÏπò Í∏∞ÏÇ¨ ÌÅ¨Î°§ÎßÅ ÏãúÏûë (ÏµúÎåÄ {max_articles}Í∞ú)")
            
            await self._collect_articles(max_articles)
            await self.collect_contents()
            await self.save_to_supabase()
            
            console.print("üéâ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å!")
            
        except KeyboardInterrupt:
            console.print("‚èπÔ∏è ÏÇ¨Ïö©ÏûêÏóê ÏùòÌï¥ Ï§ëÎã®ÎêòÏóàÏäµÎãàÎã§")
        except Exception as e:
            console.print(f"‚ùå ÌÅ¨Î°§ÎßÅ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
        finally:
            await self.cleanup()


async def main():
    collector = ChosunPoliticsCollector()
    await collector.run(max_articles=100)

if __name__ == "__main__":
    asyncio.run(main())