#!/usr/bin/env python3
"""
í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
APIë¥¼ í†µí•´ ì •ì¹˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import asyncio
import httpx
import json
from datetime import datetime
from urllib.parse import urljoin
from rich.console import Console
from playwright.async_api import async_playwright
import pytz

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.supabase_manager import SupabaseManager

console = Console()

class HaniPoliticsCollector:
    """í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.media_name = "í•œê²¨ë ˆ"
        self.base_url = "https://www.hani.co.kr"
        self.api_base = "https://www.hani.co.kr/_next/data/EM02RniQA0XrP2aTiUFUG/arti/politics.json"
        self.articles = []
        self.supabase_manager = SupabaseManager()
        
    async def _get_page_articles(self, page_num: int) -> list:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        try:
            url = f"{self.api_base}?section=politics&page={page_num}"
            console.print(f"ğŸ“¡ í˜ì´ì§€ ìˆ˜ì§‘: {url}")
            
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                response.raise_for_status()
                
                data = response.json()
                
                # articleListì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                article_list = data.get('pageProps', {}).get('listData', {}).get('articleList', [])
                console.print(f"ğŸ” APIì—ì„œ {len(article_list)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                
                articles = []
                if article_list:
                    # ê° í˜ì´ì§€ì—ì„œ ìµœëŒ€ 15ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
                    max_articles_per_page = 15
                    collected_count = 0
                    
                    for article in article_list:
                        if collected_count >= max_articles_per_page:
                            break
                            
                        title = article.get('title', '').strip()
                        article_url = article.get('url', '')
                        create_date = article.get('createDate', '')
                        
                        if title and article_url:
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if article_url.startswith('/'):
                                full_url = urljoin(self.base_url, article_url)
                            else:
                                full_url = article_url
                            
                            articles.append({
                                'title': title,
                                'url': full_url,
                                'published_at': create_date
                            })
                            collected_count += 1
                            console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬ [{collected_count}]: {title[:50]}...")
                
                console.print(f"ğŸ“Š í˜ì´ì§€ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                return articles
                
        except Exception as e:
            console.print(f"âŒ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_content(self, url: str) -> dict:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
                content_data = await page.evaluate("""
                    () => {
                        // ê¸°ì‚¬ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
                        const contentArea = document.querySelector('.article-text');
                        if (!contentArea) return { content: '', published_at: '' };
                        
                        // ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        const elementsToRemove = [
                            '.ArticleDetailAudioPlayer_wrap__',
                            '.ArticleDetailContent_imageContainer__',
                            '.ArticleDetailContent_adWrap__',
                            '.ArticleDetailContent_adFlex__',
                            '.BaseAd_adWrapper__'
                        ];
                        
                        elementsToRemove.forEach(selector => {
                            const elements = contentArea.querySelectorAll(selector);
                            elements.forEach(el => el.remove());
                        });
                        
                        // <p class="text"> íƒœê·¸ë§Œ ì¶”ì¶œ
                        const textParagraphs = contentArea.querySelectorAll('p.text');
                        const contentLines = [];
                        
                        textParagraphs.forEach(p => {
                            const text = p.textContent || p.innerText || '';
                            const trimmedText = text.trim();
                            
                            // ê¸°ì ì •ë³´ ì œì™¸ (ì´ë©”ì¼ í¬í•¨)
                            if (trimmedText && 
                                !trimmedText.includes('@') && 
                                !trimmedText.includes('ê¸°ì') &&
                                !trimmedText.includes('íŠ¹íŒŒì›') &&
                                !trimmedText.includes('í†µì‹ ì›')) {
                                contentLines.push(trimmedText);
                            }
                        });
                        
                        // ê° <p>ë¥¼ ê°œí–‰ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê²°í•©
                        const content = contentLines.join('\\n\\n');
                        
                        // ë°œí–‰ ì‹œê°„ ì¶”ì¶œ (í•œê²¨ë ˆ íŠ¹ì • ì„ íƒì)
                        const timeElement = document.querySelector('li.ArticleDetailView_dateListItem__mRc3d span');
                        const published_at = timeElement ? timeElement.textContent.trim() : '';
                        
                        return {
                            content: content,
                            published_at: published_at
                        };
                    }
                """)
                
                await browser.close()
                return content_data
                
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return {"content": "", "published_at": ""}
    
    async def _parse_article_data(self, article: dict, content_data: dict) -> dict:
        """ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ë° ì •ë¦¬"""
        try:
            # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
            published_at_str = content_data.get('published_at', '') or article.get('published_at', '')
            
            if published_at_str:
                # í•œê²¨ë ˆ ë‚ ì§œ í˜•ì‹ íŒŒì‹± (ì˜ˆ: "2025-09-03 16:05")
                try:
                    if 'T' in published_at_str:
                        # ISO í˜•ì‹ì¸ ê²½ìš°
                        published_at = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))
                    else:
                        # "YYYY-MM-DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                        published_at = datetime.strptime(published_at_str, "%Y-%m-%d %H:%M")
                        # KSTë¡œ ì¸ì‹í•˜ê³  UTCë¡œ ë³€í™˜
                        kst = pytz.timezone("Asia/Seoul")
                        published_at = kst.localize(published_at).astimezone(pytz.UTC)
                except Exception as e:
                    console.print(f"âš ï¸ ë°œí–‰ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {published_at_str} - {e}")
                    published_at = datetime.now(pytz.UTC)
            else:
                published_at = datetime.now(pytz.UTC)
            
            return {
                'title': article['title'],
                'url': article['url'],
                'content': content_data.get('content', ''),
                'published_at': published_at.isoformat(),
                'media_outlet': self.media_name
            }
            
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    async def collect_articles(self, num_pages: int = 10):
        """ê¸°ì‚¬ ìˆ˜ì§‘"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
        
        for page in range(1, num_pages + 1):
            console.print(f"ğŸ“„ í˜ì´ì§€ {page}/{num_pages} ì²˜ë¦¬ ì¤‘...")
            articles = await self._get_page_articles(page)
            self.articles.extend(articles)
        
        console.print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: {len(self.articles)}ê°œ ì„±ê³µ")
    
    async def collect_contents(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘"""
        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬")
        
        for i, article in enumerate(self.articles, 1):
            console.print(f"ğŸ“– [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘: {article['title'][:50]}...")
            
            content_data = await self._extract_content(article['url'])
            
            # ê¸°ì‚¬ ë°ì´í„°ì— ë³¸ë¬¸ê³¼ ë°œí–‰ì‹œê°„ ì¶”ê°€
            article['content'] = content_data.get('content', '')
            article['published_at'] = content_data.get('published_at', article.get('published_at', ''))
            
            console.print(f"âœ… [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
    
    async def save_articles(self):
        """ê¸°ì‚¬ ì €ì¥"""
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ì–¸ë¡ ì‚¬ í™•ì¸
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(
                name=self.media_name,
                bias="center-left",
                website=self.base_url
            )
        else:
            media_id = media["id"]
        
        # ì¤‘ë³µ ì²´í¬
        urls = [art["url"] for art in self.articles]
        existing_urls = set()
        
        try:
            for url in urls:
                exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                if exists.data:
                    existing_urls.add(url)
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        success_count = 0
        skip_count = 0
        
        for i, article in enumerate(self.articles, 1):
            try:
                # ì¤‘ë³µ ì²´í¬
                if article["url"] in existing_urls:
                    skip_count += 1
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")
                    continue
                
                # ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
                parsed_article = await self._parse_article_data(article, article)
                
                if not parsed_article:
                    continue
                
                # media_id ì¶”ê°€
                if media_id:
                    parsed_article['media_id'] = media_id
                
                # media_outlet í•„ë“œ ì œê±° (ìŠ¤í‚¤ë§ˆì— ì—†ìŒ)
                if 'media_outlet' in parsed_article:
                    del parsed_article['media_outlet']
                
                # Supabaseì— ì €ì¥
                result = self.supabase_manager.insert_article(parsed_article)
                
                if result:
                    success_count += 1
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {parsed_article['title'][:50]}...")
                else:
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨")
                    
            except Exception as e:
                console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  âŒ ì‹¤íŒ¨: {len(self.articles) - success_count - skip_count}ê°œ")
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skip_count}ê°œ")
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_count/len(self.articles)*100:.1f}%")
    
    async def run(self, num_pages: int = 10):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            console.print("ğŸš€ í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
            
            # 1. ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
            await self.collect_articles(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
            await self.collect_contents()
            
            # 3. ê¸°ì‚¬ ì €ì¥
            await self.save_articles()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

async def main():
    collector = HaniPoliticsCollector()
    await collector.run(num_pages=10)  # 10í˜ì´ì§€ì—ì„œ ê°ê° 15ê°œì”© ì´ 150ê°œ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())
