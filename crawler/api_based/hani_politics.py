#!/usr/bin/env python3
"""
í•œê²¨ë ˆ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬
- API ê¸°ë°˜ìœ¼ë¡œ ê¸°ì‚¬ ëª©ë¡ ë° ë³¸ë¬¸ ìˆ˜ì§‘
- í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›
- HTML íŒŒì‹±ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ
"""

import sys
import os
import re
import json
import time
import requests
from datetime import datetime
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import html

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from utils.supabase_manager import SupabaseManager

class HaniPoliticsCrawler:
    """í•œê²¨ë ˆ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.base_url = "https://www.hani.co.kr"
        self.politics_url = "https://www.hani.co.kr/arti/politics"
        self.api_base_url = "https://www.hani.co.kr/_next/data/RJLhj-Yk0mGbw6YoyvYyz/arti/politics.json"
        
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise Exception("Supabase ì—°ê²° ì‹¤íŒ¨")
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def get_media_outlet_id(self) -> Optional[int]:
        """í•œê²¨ë ˆ ì–¸ë¡ ì‚¬ ID ì¡°íšŒ"""
        try:
            result = self.supabase_manager.client.table('media_outlets').select('id').eq('name', 'í•œê²¨ë ˆ').execute()
            if result.data:
                return result.data[0]['id']
            return None
        except Exception as e:
            print(f"âŒ ì–¸ë¡ ì‚¬ ID ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def fetch_articles_page(self, page: int) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • í˜ì´ì§€ì˜ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ
        
        Args:
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            
        Returns:
            List[Dict]: ê¸°ì‚¬ ëª©ë¡
        """
        try:
            # API URL êµ¬ì„±
            api_url = f"{self.api_base_url}?section=politics&page={page}"
            
            print(f"ğŸ“¡ í˜ì´ì§€ {page} ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ ì¤‘...")
            
            response = self.session.get(api_url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
            articles = []
            if 'pageProps' in data and 'listData' in data['pageProps'] and 'articleList' in data['pageProps']['listData']:
                articles_data = data['pageProps']['listData']['articleList']
                
                for article_data in articles_data:
                    try:
                        # published_at ì¶”ì¶œ
                        published_at = article_data.get('updateDate', '')
                        if published_at:
                            # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                            published_at = published_at.replace('Z', '+00:00')
                        
                        article = {
                            'title': article_data.get('title', ''),
                            'url': article_data.get('url', ''),
                            'published_at': published_at,
                            'media_id': self.get_media_outlet_id()
                        }
                        
                        # URLì´ ìƒëŒ€ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
                        if article['url'] and not article['url'].startswith('http'):
                            article['url'] = urljoin(self.base_url, article['url'])
                        
                        articles.append(article)
                        
                    except Exception as e:
                        print(f"âš ï¸ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                        continue
            
            print(f"âœ… í˜ì´ì§€ {page}: {len(articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
            return articles
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page} ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ë¦¬
        
        Args:
            text: ì •ë¦¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = html.unescape(text)
        
        # <br> íƒœê·¸ë¥¼ \nìœ¼ë¡œ ë³€í™˜
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    def extract_article_content(self, article_url: str) -> str:
        """
        ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
        
        Args:
            article_url: ê¸°ì‚¬ URL
            
        Returns:
            str: ì¶”ì¶œëœ ë³¸ë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {article_url}")
            
            response = self.session.get(article_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # div.article-text ì°¾ê¸°
            article_text_div = soup.find('div', class_='article-text')
            if not article_text_div:
                print(f"âš ï¸ article-text divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {article_url}")
                return ""
            
            # ì œê±°í•  ìš”ì†Œë“¤
            unwanted_selectors = [
                '[class*="ArticleDetailAudioPlayer"]',
                '[class*="ArticleDetailContent_adWrap"]',
                '[class*="ArticleDetailContent_adFlex"]',
                '[class*="BaseAd_"]',
                'figure',
                'script',
                'style',
                'noscript',
                'iframe'
            ]
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for selector in unwanted_selectors:
                for element in article_text_div.select(selector):
                    element.decompose()
            
            # p.text ìš”ì†Œë“¤ ì¶”ì¶œ
            paragraphs = []
            for p in article_text_div.find_all('p', class_='text'):
                text = self.clean_text(p.get_text())
                if text and text.strip():  # ê³µë°±ë§Œ ìˆëŠ” ë‹¨ë½ ì œê±°
                    paragraphs.append(text.strip())
            
            # ë‹¨ë½ë“¤ì„ \n\në¡œ ì—°ê²°
            content = '\n\n'.join(paragraphs)
            
            print(f"âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ: {len(content)}ì")
            return content
            
        except Exception as e:
            print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {article_url} - {str(e)}")
            return ""
    
    def process_article(self, article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ê°œë³„ ê¸°ì‚¬ ì²˜ë¦¬
        
        Args:
            article: ê¸°ì‚¬ ë°ì´í„°
            
        Returns:
            Optional[Dict]: ì²˜ë¦¬ëœ ê¸°ì‚¬ ë°ì´í„°
        """
        try:
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self.extract_article_content(article['url'])
            if not content:
                print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ê±´ë„ˆëœ€: {article['title']}")
                return None
            
            # ìµœì¢… ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
            processed_article = {
                'title': article['title'],
                'content': content,
                'url': article['url'],
                'published_at': article['published_at'],
                'media_id': article['media_id'],
                'is_preprocessed': False  # ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ìƒíƒœë¡œ ì €ì¥
            }
            
            return processed_article
            
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {article['title']} - {str(e)}")
            return None
    
    def crawl_articles(self, max_pages: int = 5) -> List[Dict[str, Any]]:
        """
        ê¸°ì‚¬ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            List[Dict]: í¬ë¡¤ë§ëœ ê¸°ì‚¬ ëª©ë¡
        """
        try:
            print(f"ğŸš€ í•œê²¨ë ˆ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ë§ ì‹œì‘... (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
            
            all_articles = []
            
            for page in range(1, max_pages + 1):
                # í˜ì´ì§€ë³„ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ
                articles = self.fetch_articles_page(page)
                if not articles:
                    print(f"ğŸ“ í˜ì´ì§€ {page}ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ê° ê¸°ì‚¬ ì²˜ë¦¬
                for article in articles:
                    processed_article = self.process_article(article)
                    if processed_article:
                        all_articles.append(processed_article)
                
                # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                time.sleep(1)
            
            print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ ê¸°ì‚¬")
            return all_articles
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def save_articles(self, articles: List[Dict[str, Any]]) -> int:
        """
        ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        
        Args:
            articles: ì €ì¥í•  ê¸°ì‚¬ ëª©ë¡
            
        Returns:
            int: ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜
        """
        if not articles:
            print("ğŸ“ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        try:
            print(f"ğŸ’¾ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
            
            success_count = 0
            for article in articles:
                if self.supabase_manager.insert_article(article):
                    success_count += 1
            
            print(f"âœ… {success_count}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
            return success_count
            
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return 0

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ“° í•œê²¨ë ˆ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬")
    print("=" * 60)
    
    try:
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = HaniPoliticsCrawler()
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        articles = crawler.crawl_articles(max_pages=10)
        
        if articles:
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            saved_count = crawler.save_articles(articles)
            print(f"\nğŸ‰ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!")
            print(f"ğŸ“Š í¬ë¡¤ë§: {len(articles)}ê°œ, ì €ì¥: {saved_count}ê°œ")
        else:
            print("\nâŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
