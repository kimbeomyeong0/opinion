#!/usr/bin/env python3
"""
í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
APIë¥¼ í†µí•´ ì •ì¹˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import asyncio
import httpx
import json
from datetime import datetime
from urllib.parse import urljoin
from rich.console import Console
from playwright.async_api import async_playwright
import pytz

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.supabase_manager import SupabaseManager

console = Console()

class HaniPoliticsCollector:
    """í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.media_name = "í•œê²¨ë ˆ"
        self.base_url = "https://www.hani.co.kr"
        self.politics_url = "https://www.hani.co.kr/arti/politics"
        self.articles = []
        self.supabase_manager = SupabaseManager()
        self._playwright = None
        self._browser = None
        self._semaphore = asyncio.Semaphore(3)  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
        
    async def _get_page_articles(self, page_num: int) -> list:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        try:
            # í˜ì´ì§€ URL êµ¬ì„±
            if page_num == 1:
                url = self.politics_url
            else:
                url = f"{self.politics_url}?page={page_num}"
            
            console.print(f"ğŸ“¡ í˜ì´ì§€ ìˆ˜ì§‘: {url}")
            
            # ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©
            if not self._browser:
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--memory-pressure-off'
                    ]
                )
            
            page = await self._browser.new_page()
            await page.goto(url, wait_until='domcontentloaded', timeout=10000)
            
            # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
            articles = await page.evaluate("""
                    () => {
                        const articleElements = document.querySelectorAll('a[href*="/arti/politics/"][href$=".html"]');
                        const articles = [];
                        
                        articleElements.forEach((link, index) => {
                            const title = link.textContent.trim();
                            const href = link.href;
                            
                            // ì œëª©ì´ ìˆê³ , ì‹¤ì œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸
                            if (title && href && href.includes('/arti/politics/') && href.endsWith('.html')) {
                                articles.push({
                                    title: title,
                                    url: href
                                });
                            }
                        });
                        
                        return articles.slice(0, 20); // í˜ì´ì§€ë‹¹ ìµœëŒ€ 20ê°œ
                    }
                """)
            
            await page.close()
            
            console.print(f"ğŸ” í˜ì´ì§€ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            for i, article in enumerate(articles, 1):
                console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬ [{i}]: {article['title'][:50]}...")
                
                return articles
                
        except Exception as e:
            console.print(f"âŒ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_content(self, url: str) -> dict:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        async with self._semaphore:  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
            try:
                # ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©
                if not self._browser:
                    self._playwright = await async_playwright().start()
                    self._browser = await self._playwright.chromium.launch(
                        headless=True,
                        args=[
                            '--no-sandbox',
                            '--disable-dev-shm-usage',
                            '--disable-gpu',
                            '--disable-web-security',
                            '--disable-features=VizDisplayCompositor',
                            '--memory-pressure-off'
                        ]
                    )
                
                page = await self._browser.new_page()
                await page.goto(url, wait_until='domcontentloaded', timeout=10000)
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - ìµœì í™”ëœ ì„ íƒì
                content_data = await page.evaluate("""
                    () => {
                        const result = { content: '', published_at: '' };
                        
                        // 1. ë°œí–‰ ì‹œê°„ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ë³„)
                        const timeSelectors = [
                            'li.ArticleDetailView_dateListItem__mRc3d span',
                            '.article-date span',
                            '.date span',
                            'time'
                        ];
                        
                        for (const selector of timeSelectors) {
                            const element = document.querySelector(selector);
                            if (element && element.textContent.trim()) {
                                result.published_at = element.textContent.trim();
                                break;
                            }
                        }
                        
                        // 2. ë³¸ë¬¸ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ë³„)
                        const contentSelectors = [
                            '.article-text',
                            '.article-body',
                            '.content',
                            'article'
                        ];
                        
                        let contentArea = null;
                        for (const selector of contentSelectors) {
                            contentArea = document.querySelector(selector);
                            if (contentArea) break;
                        }
                        
                        if (contentArea) {
                            // ê´‘ê³  ìš”ì†Œ ì œê±°
                            const adSelectors = [
                                '.ArticleDetailAudioPlayer_wrap__',
                                '.ArticleDetailContent_imageContainer__',
                                '.ArticleDetailContent_adWrap__',
                                '.ArticleDetailContent_adFlex__',
                                '.BaseAd_adWrapper__',
                                '[class*="ad"]',
                                '[class*="Ad"]'
                            ];
                            
                            adSelectors.forEach(selector => {
                                const elements = contentArea.querySelectorAll(selector);
                                elements.forEach(el => el.remove());
                            });
                            
                            // ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            const paragraphs = contentArea.querySelectorAll('p.text, p, div.text');
                            const contentLines = [];
                            
                            paragraphs.forEach(p => {
                                const text = p.textContent?.trim() || '';
                                
                                // í•„í„°ë§: ê¸°ì ì •ë³´, ì´ë©”ì¼, ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                                if (text && 
                                    text.length > 20 && 
                                    !text.includes('@') && 
                                    !text.includes('ê¸°ì') &&
                                    !text.includes('íŠ¹íŒŒì›') &&
                                    !text.includes('í†µì‹ ì›') &&
                                    !text.match(/^\\s*$/)) {
                                    contentLines.push(text);
                                }
                            });
                            
                            result.content = contentLines.join('\\n\\n');
                        }
                        
                        return result;
                    }
                """)
                
                await page.close()
                return content_data
                
            except Exception as e:
                console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
                return {"content": "", "published_at": ""}
    
    async def _parse_article_data(self, article: dict, content_data: dict) -> dict:
        """ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ë° ì •ë¦¬"""
        try:
            # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
            published_at_str = content_data.get('published_at', '') or article.get('published_at', '')
            
            if published_at_str:
                # í•œê²¨ë ˆ ë‚ ì§œ í˜•ì‹ íŒŒì‹± (ì˜ˆ: "2025-09-03 16:05")
                try:
                    if 'T' in published_at_str:
                        # ISO í˜•ì‹ì¸ ê²½ìš°
                        published_at = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))
                    else:
                        # "YYYY-MM-DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                        published_at = datetime.strptime(published_at_str, "%Y-%m-%d %H:%M")
                        # KSTë¡œ ì¸ì‹í•˜ê³  UTCë¡œ ë³€í™˜
                        kst = pytz.timezone("Asia/Seoul")
                        published_at = kst.localize(published_at).astimezone(pytz.UTC)
                except Exception as e:
                    console.print(f"âš ï¸ ë°œí–‰ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {published_at_str} - {e}")
                    published_at = datetime.now(pytz.UTC)
            else:
                published_at = datetime.now(pytz.UTC)
            
            return {
                'title': article['title'],
                'url': article['url'],
                'content': content_data.get('content', ''),
                'published_at': published_at.isoformat(),
                'media_outlet': self.media_name
            }
            
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    async def collect_articles(self, num_pages: int = 10):
        """ê¸°ì‚¬ ìˆ˜ì§‘ - ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìµœì í™”"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í˜ì´ì§€ ìˆ˜ì§‘
        tasks = [self._get_page_articles(page) for page in range(1, num_pages + 1)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i} ìˆ˜ì§‘ ì‹¤íŒ¨: {result}")
            else:
                self.articles.extend(result)
                console.print(f"âœ… í˜ì´ì§€ {i} ìˆ˜ì§‘ ì™„ë£Œ: {len(result)}ê°œ ê¸°ì‚¬")
        
        console.print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: {len(self.articles)}ê°œ ì„±ê³µ")
    
    async def collect_contents(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ - ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìµœì í™”"""
        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬")
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë³¸ë¬¸ ìˆ˜ì§‘
        tasks = [self._extract_content(article['url']) for article in self.articles]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì²˜ë¦¬
        success_count = 0
        for i, (article, result) in enumerate(zip(self.articles, results), 1):
            if isinstance(result, Exception):
                console.print(f"âŒ [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {result}")
                article['content'] = ''
                article['published_at'] = article.get('published_at', '')
            else:
                article['content'] = result.get('content', '')
                article['published_at'] = result.get('published_at', article.get('published_at', ''))
                success_count += 1
                console.print(f"âœ… [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
        
        console.print(f"ğŸ“Š ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(self.articles)}ê°œ ì„±ê³µ")
    
    async def save_articles(self):
        """ê¸°ì‚¬ ì €ì¥"""
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ì–¸ë¡ ì‚¬ í™•ì¸
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(
                name=self.media_name,
                bias="center-left",
                website=self.base_url
            )
        else:
            media_id = media["id"]
        
        # ì¤‘ë³µ ì²´í¬
        urls = [art["url"] for art in self.articles]
        existing_urls = set()
        
        try:
            for url in urls:
                exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                if exists.data:
                    existing_urls.add(url)
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        success_count = 0
        skip_count = 0
        
        for i, article in enumerate(self.articles, 1):
            try:
                # ì¤‘ë³µ ì²´í¬
                if article["url"] in existing_urls:
                    skip_count += 1
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")
                    continue
                
                # ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
                parsed_article = await self._parse_article_data(article, article)
                
                if not parsed_article:
                    continue
                
                # media_id ì¶”ê°€
                if media_id:
                    parsed_article['media_id'] = media_id
                
                # media_outlet í•„ë“œ ì œê±° (ìŠ¤í‚¤ë§ˆì— ì—†ìŒ)
                if 'media_outlet' in parsed_article:
                    del parsed_article['media_outlet']
                
                # Supabaseì— ì €ì¥
                result = self.supabase_manager.insert_article(parsed_article)
                
                if result:
                    success_count += 1
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {parsed_article['title'][:50]}...")
                else:
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨")
                    
            except Exception as e:
                console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  âŒ ì‹¤íŒ¨: {len(self.articles) - success_count - skip_count}ê°œ")
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skip_count}ê°œ")
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_count/len(self.articles)*100:.1f}%")
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self._browser:
                await self._browser.close()
                self._browser = None
            if self._playwright:
                await self._playwright.stop()
                self._playwright = None
            console.print("ğŸ§¹ í•œê²¨ë ˆ í¬ë¡¤ëŸ¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            console.print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}")
    
    async def run(self, num_pages: int = 10):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            console.print("ğŸš€ í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
            
            # 1. ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
            await self.collect_articles(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
            await self.collect_contents()
            
            # 3. ê¸°ì‚¬ ì €ì¥
            await self.save_articles()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            await self.cleanup()

async def main():
    collector = HaniPoliticsCollector()
    await collector.run(num_pages=10)  # 10í˜ì´ì§€ì—ì„œ ê°ê° 15ê°œì”© ì´ 150ê°œ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())
