#!/usr/bin/env python3
"""
ë‰´ìŠ¤ì› ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (API ê¸°ë°˜)
- News1 APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ ìˆ˜ì§‘
- ê¸°ì‚¬ 1ê°œ ìˆ˜ì§‘ ê¸°ëŠ¥
"""

import asyncio
import json
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin
import httpx
import pytz
from rich.console import Console
from playwright.async_api import async_playwright

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")


class NewsonePoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.news1.kr"
        self.api_url = "https://rest.news1.kr/v6/section/politics/latest"
        self.media_name = "ë‰´ìŠ¤ì›"
        self.media_bias = "center"
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        self._playwright = None
        self._browser = None

    async def _get_politics_articles(self, total_limit: int = 130) -> List[Dict]:
        """ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›)"""
        console.print(f"ğŸ”Œ ë‰´ìŠ¤ì› ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {total_limit}ê°œ)")
        
        all_articles = []
        async with httpx.AsyncClient(timeout=10.0) as client:
            # start=1ë¶€í„° start=13ê¹Œì§€ ê°ê° 10ê°œì”© ìˆ˜ì§‘
            for start_page in range(1, 14):  # 1ë¶€í„° 13ê¹Œì§€
                if len(all_articles) >= total_limit:
                    break
                    
                try:
                    params = {
                        "start": start_page,
                        "limit": 10  # ê° í˜ì´ì§€ì—ì„œ 10ê°œì”©
                    }
                    
                    console.print(f"ğŸ“¡ API í˜¸ì¶œ: {self.api_url} (start={start_page}, limit=10)")
                    resp = await client.get(self.api_url, params=params)
                    resp.raise_for_status()
                    data = resp.json()
                    
                    console.print(f"ğŸ“Š API ì‘ë‹µ (start={start_page}): {len(data)}ê°œ ê¸°ì‚¬ ìˆ˜ì‹ ")
                    all_articles.extend(data)
                    
                    # í˜ì´ì§€ ê°„ ì§§ì€ ëŒ€ê¸° (API ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    console.print(f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜ (start={start_page}): {e}")
                    continue
        
        console.print(f"ğŸ“ˆ ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(all_articles)}ê°œ")
        return all_articles

    def _parse_article_data(self, article_data: Dict) -> Optional[Dict]:
        """API ì‘ë‹µ ë°ì´í„° íŒŒì‹±"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            title = article_data.get("title")
            if not title:
                return None

            # URL ì²˜ë¦¬
            url_path = article_data.get("url", "")
            if url_path.startswith("/"):
                url = urljoin(self.base_url, url_path)
            else:
                url = url_path

            # ë‚ ì§œ ì²˜ë¦¬
            published_at = None
            pubdate = article_data.get("pubdate")
            if pubdate:
                try:
                    # "2025-09-04 22:31:19" í˜•ì‹ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    dt = datetime.strptime(pubdate, "%Y-%m-%d %H:%M:%S")
                    # KST ì‹œê°„ëŒ€ë¡œ ë³€í™˜
                    dt_kst = KST.localize(dt)
                    published_at = dt_kst.isoformat()
                except Exception as e:
                    console.print(f"âš ï¸ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {pubdate} - {e}")
                    published_at = None

            # ê¸°ì ì •ë³´
            author = article_data.get("author", "")

            # ì„¹ì…˜ ì •ë³´ (ì •ì¹˜ë¡œ ê³ ì •)
            section = "ì •ì¹˜"

            # íƒœê·¸ ì •ë³´ (ì—†ìŒ)
            tag_list = []

            # ìš”ì•½ ì •ë³´
            summary = article_data.get("summary", "")

            return {
                "title": title.strip(),
                "url": url,
                "content": "",  # ë³¸ë¬¸ì€ ë‚˜ì¤‘ì— Playwrightë¡œ ì±„ì›€
                "published_at": published_at,
                "created_at": datetime.now(KST).isoformat(),
                "author": author,
                "section": section,
                "tags": tag_list,
                "description": summary,
            }
        except Exception as e:
            console.print(f"âŒ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    async def _collect_articles(self, total_limit: int = 130):
        """ê¸°ì‚¬ ìˆ˜ì§‘"""
        console.print(f"ğŸš€ ë‰´ìŠ¤ì› ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {total_limit}ê°œ)")
        
        # APIì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
        articles_data = await self._get_politics_articles(total_limit)
        
        if not articles_data:
            console.print("âŒ ìˆ˜ì§‘í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ê° ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
        success_count = 0
        for i, article_data in enumerate(articles_data, 1):
            parsed_article = self._parse_article_data(article_data)
            if parsed_article:
                self.articles.append(parsed_article)
                success_count += 1
                # ì§„í–‰ë¥  í‘œì‹œ (10ê°œë§ˆë‹¤)
                if i % 10 == 0 or i == len(articles_data):
                    console.print(f"âœ… [{i}/{len(articles_data)}] {parsed_article['title'][:50]}...")
            else:
                console.print(f"âŒ [{i}/{len(articles_data)}] ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨")

        console.print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(articles_data)}ê°œ ì„±ê³µ")

    async def _extract_content(self, url: str) -> str:
        """Playwrightë¡œ ë³¸ë¬¸ ì „ë¬¸ ì¶”ì¶œ"""
        page = None
        try:
            if not self._browser:
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox', 
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--memory-pressure-off'
                    ]
                )

            page = await self._browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 720})
            await page.goto(url, wait_until="domcontentloaded", timeout=10000)

            # ë‰´ìŠ¤ì› ë³¸ë¬¸ ì¶”ì¶œ
            content = ""
            
            try:
                content = await page.evaluate('''() => {
                    const selectors = [
                        'div.article-body p',
                        'div#article-body p',
                        'section.article-body p',
                        'article.article-body p',
                        '.story-news p',
                        '.article-content p',
                        'main p',
                        'article p'
                    ];
                    
                    for (const selector of selectors) {
                        const paragraphs = document.querySelectorAll(selector);
                        if (paragraphs.length > 0) {
                            const texts = Array.from(paragraphs)
                                .map(p => p.textContent.trim())
                                .filter(text => text.length > 20)
                                .slice(0, 20);
                            
                            if (texts.length > 0) {
                                return texts.join('\\n\\n');
                            }
                        }
                    }
                    
                    return "";
                }''')
                
                if content and len(content.strip()) > 50:
                    return content.strip()
                    
            except Exception as e:
                console.print(f"âš ï¸ JavaScript ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:50]}")
            
            return content.strip()
            
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url[:50]}...): {str(e)[:50]}")
            return ""
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass

    async def collect_contents(self):
        """ë³¸ë¬¸ ì „ë¬¸ ìˆ˜ì§‘"""
        if not self.articles:
            return

        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬")
        
        success_count = 0
        for i, art in enumerate(self.articles, 1):
            content = await self._extract_content(art["url"])
            if content:
                self.articles[i-1]["content"] = content
                success_count += 1
                console.print(f"âœ… [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
            else:
                console.print(f"âš ï¸ [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨")

        console.print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(self.articles)}ê°œ ì„±ê³µ")

    async def save_to_supabase(self):
        """DB ì €ì¥"""
        if not self.articles:
            console.print("âŒ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")

        # ì–¸ë¡ ì‚¬ í™•ì¸
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
        else:
            media_id = media["id"]

        # ì¤‘ë³µ ì²´í¬
        urls = [art["url"] for art in self.articles]
        existing_urls = set()
        
        try:
            for url in urls:
                exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                if exists.data:
                    existing_urls.add(url)
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")

        success, failed, skipped = 0, 0, 0
        
        for i, art in enumerate(self.articles, 1):
            try:
                if art["url"] in existing_urls:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {art['title'][:30]}...")
                    skipped += 1
                    continue

                published_at_str = art.get("published_at")
                created_at_str = art.get("created_at", published_at_str)

                article_data = {
                    "media_id": media_id,
                    "title": art["title"],
                    "content": art["content"],
                    "url": art["url"],
                    "published_at": published_at_str,
                    "created_at": created_at_str,
                }

                if self.supabase_manager.insert_article(article_data):
                    success += 1
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {art['title'][:30]}...")
                else:
                    failed += 1
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {art['title'][:30]}...")
                    
            except Exception as e:
                failed += 1
                console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì˜¤ë¥˜: {str(e)[:50]}")

        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success}ê°œ")
        console.print(f"  âŒ ì‹¤íŒ¨: {failed}ê°œ") 
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skipped}ê°œ")
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(success / len(self.articles) * 100):.1f}%")

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self._browser:
                await self._browser.close()
                self._browser = None
            if self._playwright:
                await self._playwright.stop()
                self._playwright = None
            console.print("ğŸ§¹ Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            console.print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}")

    async def run(self, total_limit: int = 130):
        """ì‹¤í–‰"""
        try:
            console.print(f"ğŸš€ ë‰´ìŠ¤ì› ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {total_limit}ê°œ)")
            
            await self._collect_articles(total_limit)
            await self.collect_contents()
            await self.save_to_supabase()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            await self.cleanup()


async def main():
    collector = NewsonePoliticsCollector()
    await collector.run(total_limit=130)  # start=1ë¶€í„° start=13ê¹Œì§€ ê°ê° 10ê°œì”© ì´ 130ê°œ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())
