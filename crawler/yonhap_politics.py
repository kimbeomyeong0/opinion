#!/usr/bin/env python3
"""
ì—°í•©ë‰´ìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
 - ê¸°ì‚¬ ëª©ë¡: https://www.yna.co.kr/politics/all/{page}
 - ë³¸ë¬¸: .story-news.article ë‚´ë¶€ <p>
 - ë°œí–‰ì‹œê°„: p.txt-time01
"""

import asyncio
import re
import sys
import os
from datetime import datetime
import pytz
import httpx
from bs4 import BeautifulSoup
from rich.console import Console

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ utils ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.supabase_manager import SupabaseManager

console = Console()


class YonhapPoliticsCollector:
    def __init__(self):
        self.media_name = "ì—°í•©ë‰´ìŠ¤"
        self.base_url = "https://www.yna.co.kr"
        self.list_url = "https://www.yna.co.kr/politics/all"
        self.supabase_manager = SupabaseManager()
        self.articles = []

    async def run(self, num_pages=10):
        console.print(f"ğŸš€ {self.media_name} ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
        await self.collect_articles(num_pages)
        await self.collect_contents()
        await self.save_articles()
        console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")

    async def collect_articles(self, num_pages):
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...")
        for page in range(1, num_pages + 1):
            url = f"{self.list_url}/{page}"
            page_articles = await self._get_page_articles(url)
            self.articles.extend(page_articles)
            console.print(f"ğŸ“Š í˜ì´ì§€ {page}: {len(page_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def _get_page_articles(self, url: str):
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                r = await client.get(url, headers={"User-Agent": "Mozilla/5.0"})
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")

            articles = []
            for item in soup.select("div.item-box01")[:15]:  # ê° í˜ì´ì§€ 15ê°œ
                title_tag = item.select_one("a.tit-news span.title01")
                link_tag = item.select_one("a.tit-news")

                if not title_tag or not link_tag:
                    continue

                title = title_tag.get_text(strip=True)
                href = link_tag.get("href")
                article_url = href if href.startswith("http") else self.base_url + href

                articles.append({
                    "title": title,
                    "url": article_url,
                    "content": "",
                    "published_at": ""
                })
                console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬: {title[:40]}...")

            return articles
        except Exception as e:
            console.print(f"âŒ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def collect_contents(self):
        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ ({len(self.articles)}ê°œ)")
        async with httpx.AsyncClient(timeout=30) as client:
            for i, article in enumerate(self.articles, 1):
                try:
                    r = await client.get(article["url"], headers={"User-Agent": "Mozilla/5.0"})
                    r.raise_for_status()
                    soup = BeautifulSoup(r.text, "html.parser")

                    article["content"] = self.extract_content(soup)
                    article["published_at"] = self.extract_published_at(soup)

                    console.print(f"âœ… [{i}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ: {article['title'][:40]}...")
                except Exception as e:
                    console.print(f"âŒ [{i}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def extract_content(self, soup: BeautifulSoup) -> str:
        """ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        article = soup.select_one(".story-news.article")
        if not article:
            return ""

        # ê´‘ê³ /ì‚¬ì§„/aside/ì €ì‘ê¶Œ ì œê±°
        for tag in article.select("aside, figure, div.comp-box, p.txt-copyright"):
            tag.decompose()

        paragraphs = []
        for p in article.find_all("p"):
            text = p.get_text(" ", strip=True)
            if not text:
                continue
            # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
            if "ì €ì‘ê¶Œì" in text or "ë¬´ë‹¨ ì „ì¬" in text:
                continue
            if "ì œë³´ëŠ” ì¹´ì¹´ì˜¤í†¡" in text:
                continue
            if re.match(r"^\[.*\]$", text):
                continue
            if re.search(r"[a-zA-Z0-9._%+-]+@yna\.co\.kr", text):
                continue
            paragraphs.append(text)

        return "\n\n".join(paragraphs).strip()

    def extract_published_at(self, soup: BeautifulSoup) -> str:
        """ì†¡ê³  ì‹œê° UTC ë³€í™˜"""
        tag = soup.select_one("p.txt-time01")
        if not tag:
            return datetime.now(pytz.UTC).isoformat()

        text = tag.get_text(" ", strip=True)
        m = re.search(r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}", text)
        if not m:
            return datetime.now(pytz.UTC).isoformat()

        kst = pytz.timezone("Asia/Seoul")
        dt = datetime.strptime(m.group(), "%Y-%m-%d %H:%M")
        dt = kst.localize(dt)
        return dt.astimezone(pytz.UTC).isoformat()

    async def save_articles(self):
        console.print(f"ğŸ’¾ DB ì €ì¥ ì‹œì‘ ({len(self.articles)}ê°œ)")
        media_outlet = self.supabase_manager.get_media_outlet(self.media_name)
        media_id = media_outlet["id"] if media_outlet else self.supabase_manager.create_media_outlet(self.media_name)

        existing_urls = set()
        try:
            result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
            existing_urls = {a["url"] for a in result.data}
        except Exception as e:
            console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {e}")

        for i, article in enumerate(self.articles, 1):
            if article["url"] in existing_urls:
                console.print(f"âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {article['title'][:40]}")
                continue

            data = {
                "title": article["title"],
                "url": article["url"],
                "content": article["content"],
                "published_at": article["published_at"],
                "created_at": datetime.now(pytz.UTC).isoformat(),
                "media_id": media_id,
            }
            success = self.supabase_manager.insert_article(data)
            if success:
                console.print(f"âœ… ì €ì¥ ì™„ë£Œ: {article['title'][:40]}")
            else:
                console.print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {article['title'][:40]}")


async def main():
    collector = YonhapPoliticsCollector()
    await collector.run(num_pages=10)


if __name__ == "__main__":
    asyncio.run(main())
