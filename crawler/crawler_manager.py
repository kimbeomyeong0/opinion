#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ ë³‘ë ¬ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
4ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ í¬ë¡¤ëŸ¬ë“¤ì„ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import sys
import os
import time
from datetime import datetime
from typing import List, Dict, Optional
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
import pytz

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ì„¤ì • ë° í¬ë¡¤ëŸ¬ ëª¨ë“ˆë“¤ import
from config.crawler_config import CRAWLER_PARAMS, CRAWLER_GROUPS, PLAYWRIGHT_CRAWLERS, STAGE_DELAYS, RETRY_CONFIG
from .html_parsing.ohmynews_politics import OhmyNewsPoliticsCollector
from .html_parsing.yonhap_politics import YonhapPoliticsCollector
from .api_based.hani_politics import HaniPoliticsCollector
from .api_based.newsone_politics import NewsonePoliticsCollector
from .api_based.khan_politics import KhanPoliticsCollector
from .html_parsing.donga_politics import DongaPoliticsCollector
from .html_parsing.joongang_politics import JoongangPoliticsCollector
from .html_parsing.newsis_politics import NewsisPoliticsCollector
from .api_based.chosun_politics import ChosunPoliticsCollector

console = Console()
KST = pytz.timezone("Asia/Seoul")


class CrawlerResult:
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, crawler_name: str):
        self.crawler_name = crawler_name
        self.status = "pending"
        self.start_time = None
        self.end_time = None
        self.duration = None
        self.error_message = None
        self.articles_collected = 0
        
    def start(self):
        """ì‹¤í–‰ ì‹œì‘"""
        self.start_time = datetime.now(KST)
        self.status = "running"
        
    def finish(self, success: bool = True, error_message: str = None, articles_count: int = 0):
        """ì‹¤í–‰ ì™„ë£Œ"""
        self.end_time = datetime.now(KST)
        self.duration = (self.end_time - self.start_time).total_seconds()
        self.status = "success" if success else "failed"
        self.error_message = error_message
        self.articles_collected = articles_count


class CrawlerManager:
    """í¬ë¡¤ëŸ¬ ë³‘ë ¬ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.results: Dict[str, CrawlerResult] = {}
        self.semaphore = asyncio.Semaphore(3)  # ì¼ë°˜ í¬ë¡¤ëŸ¬ ë™ì‹œ ì‹¤í–‰ ì œí•œ
        self.playwright_semaphore = asyncio.Semaphore(2)  # Playwright í¬ë¡¤ëŸ¬ ë™ì‹œ ì‹¤í–‰ ì œí•œ
        
        # í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ë§¤í•‘
        self.crawler_classes = {
            "ohmynews_politics": OhmyNewsPoliticsCollector,
            "yonhap_politics": YonhapPoliticsCollector,
            "hani_politics": HaniPoliticsCollector,
            "newsone_politics": NewsonePoliticsCollector,
            "khan_politics": KhanPoliticsCollector,
            "donga_politics": DongaPoliticsCollector,
            "joongang_politics": JoongangPoliticsCollector,
            "newsis_politics": NewsisPoliticsCollector,
            "chosun_politics": ChosunPoliticsCollector,
        }
        
        # ì„¤ì •ì—ì„œ í¬ë¡¤ëŸ¬ ê·¸ë£¹ ë° ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        self.crawler_groups = CRAWLER_GROUPS
        self.playwright_crawlers = PLAYWRIGHT_CRAWLERS
    
    def _get_crawler_params(self, crawler_name: str) -> Dict:
        """í¬ë¡¤ëŸ¬ë³„ ì‹¤í–‰ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return CRAWLER_PARAMS.get(crawler_name, {})
    
    async def run_crawler(self, crawler_name: str) -> CrawlerResult:
        """ë‹¨ì¼ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        result = CrawlerResult(crawler_name)
        self.results[crawler_name] = result
        
        try:
            console.print(f"ğŸš€ {crawler_name} í¬ë¡¤ëŸ¬ ì‹œì‘")
            result.start()
            
            # í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            crawler_class = self.crawler_classes.get(crawler_name)
            if not crawler_class:
                raise ValueError(f"í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {crawler_name}")
            
            crawler = crawler_class()
            params = self._get_crawler_params(crawler_name)
            
            # í¬ë¡¤ëŸ¬ ì‹¤í–‰
            await crawler.run(**params)
            
            # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ
            articles_count = len(getattr(crawler, 'articles', []))
            result.finish(success=True, articles_count=articles_count)
            console.print(f"âœ… {crawler_name} ì™„ë£Œ - {articles_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            
        except Exception as e:
            error_msg = str(e)[:100] + "..." if len(str(e)) > 100 else str(e)
            result.finish(success=False, error_message=error_msg)
            console.print(f"âŒ {crawler_name} ì‹¤íŒ¨: {error_msg}")
            
        return result
    
    async def run_crawler_with_semaphore(self, crawler_name: str) -> CrawlerResult:
        """ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        if crawler_name in self.playwright_crawlers:
            async with self.playwright_semaphore:
                return await self.run_crawler(crawler_name)
        else:
            async with self.semaphore:
                return await self.run_crawler(crawler_name)
    
    async def run_simple_crawlers(self):
        """ë‹¨ìˆœí•œ í¬ë¡¤ëŸ¬ë“¤ ë³‘ë ¬ ì‹¤í–‰"""
        stage_info = self.crawler_groups["simple"]
        console.print(Panel.fit(f"ğŸ¯ 1ë‹¨ê³„: {stage_info['description']}", style="bold blue"))
        
        crawlers = stage_info["crawlers"]
        console.print(f"ì‹¤í–‰í•  í¬ë¡¤ëŸ¬: {', '.join(crawlers)}")
        
        # ë³‘ë ¬ ì‹¤í–‰
        tasks = [self.run_crawler_with_semaphore(crawler) for crawler in crawlers]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì¶œë ¥
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                console.print(f"âŒ {crawlers[i]} ì˜ˆì™¸ ë°œìƒ: {result}")
            else:
                status = "âœ… ì„±ê³µ" if result.status == "success" else "âŒ ì‹¤íŒ¨"
                console.print(f"{status} {result.crawler_name} - {result.articles_collected}ê°œ ê¸°ì‚¬")
    
    async def run_complex_crawlers(self):
        """ë³µì¡í•œ í¬ë¡¤ëŸ¬ë“¤ ìˆœì°¨ ì‹¤í–‰ (Playwright ì‚¬ìš©)"""
        stage_info = self.crawler_groups["complex"]
        console.print(Panel.fit(f"ğŸ¯ 2ë‹¨ê³„: {stage_info['description']}", style="bold yellow"))
        
        crawlers = stage_info["crawlers"]
        console.print(f"ì‹¤í–‰í•  í¬ë¡¤ëŸ¬: {', '.join(crawlers)} (ìˆœì°¨ ì‹¤í–‰)")
        
        # ìˆœì°¨ ì‹¤í–‰ (Playwright ë¦¬ì†ŒìŠ¤ ì¶©ëŒ ë°©ì§€)
        for crawler in crawlers:
            console.print(f"ğŸ”„ {crawler} ì‹¤í–‰ ì¤‘...")
            result = await self.run_crawler_with_semaphore(crawler)
            
            status = "âœ… ì„±ê³µ" if result.status == "success" else "âŒ ì‹¤íŒ¨"
            console.print(f"{status} {result.crawler_name} - {result.articles_collected}ê°œ ê¸°ì‚¬")
            
            # í¬ë¡¤ëŸ¬ ê°„ ëŒ€ê¸° ì‹œê°„
            await asyncio.sleep(2)
    
    def print_summary(self):
        """ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        console.print(Panel.fit("ğŸ“Š í¬ë¡¤ëŸ¬ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½", style="bold magenta"))
        
        table = Table(title="í¬ë¡¤ëŸ¬ ì‹¤í–‰ ê²°ê³¼")
        table.add_column("í¬ë¡¤ëŸ¬", style="cyan")
        table.add_column("ìƒíƒœ", style="green")
        table.add_column("ìˆ˜ì§‘ ê¸°ì‚¬", style="blue")
        table.add_column("ì‹¤í–‰ ì‹œê°„", style="yellow")
        table.add_column("ì˜¤ë¥˜ ë©”ì‹œì§€", style="red")
        
        total_articles = 0
        success_count = 0
        
        for crawler_name, result in self.results.items():
            status_icon = "âœ…" if result.status == "success" else "âŒ"
            duration_str = f"{result.duration:.1f}ì´ˆ" if result.duration else "N/A"
            error_str = result.error_message[:30] + "..." if result.error_message and len(result.error_message) > 30 else result.error_message or ""
            
            table.add_row(
                crawler_name,
                f"{status_icon} {result.status}",
                str(result.articles_collected),
                duration_str,
                error_str
            )
            
            total_articles += result.articles_collected
            if result.status == "success":
                success_count += 1
        
        console.print(table)
        
        # ì „ì²´ í†µê³„
        console.print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        console.print(f"  ì´ í¬ë¡¤ëŸ¬: {len(self.results)}ê°œ")
        console.print(f"  ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  ì‹¤íŒ¨: {len(self.results) - success_count}ê°œ")
        console.print(f"  ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {total_articles}ê°œ")
        console.print(f"  ì„±ê³µë¥ : {(success_count / len(self.results) * 100):.1f}%")
    
    async def run_full_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = datetime.now(KST)
        console.print(Panel.fit("ğŸš€ í¬ë¡¤ëŸ¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘", style="bold white"))
        console.print(f"ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # 1ë‹¨ê³„: ë‹¨ìˆœí•œ í¬ë¡¤ëŸ¬ë“¤
            await self.run_simple_crawlers()
            await asyncio.sleep(STAGE_DELAYS["simple"])
            
            # 2ë‹¨ê³„: ë³µì¡í•œ í¬ë¡¤ëŸ¬ë“¤
            await self.run_complex_crawlers()
            await asyncio.sleep(STAGE_DELAYS["complex"])
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            end_time = datetime.now(KST)
            total_duration = (end_time - start_time).total_seconds()
            
            console.print(f"\nğŸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            console.print(f"ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
            console.print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            self.print_summary()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    manager = CrawlerManager()
    await manager.run_full_pipeline()


if __name__ == "__main__":
    asyncio.run(main())
