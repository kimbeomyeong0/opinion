#!/usr/bin/env python3
"""
ì˜¤ë§ˆì´ë‰´ìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (5í˜ì´ì§€ Ã— ê° 1ê°œ ê¸°ì‚¬)
ê°œì„ ì‚¬í•­:
- ë¦¬ë‹¤ì´ë ‰íŠ¸ ìë™ ì²˜ë¦¬
- ë” ê²¬ê³ í•œ ì—ëŸ¬ í•¸ë“¤ë§
- ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
- ë¡œê¹… ê°œì„ 
"""

import asyncio
import sys
import os
from datetime import datetime
import pytz
import httpx
from bs4 import BeautifulSoup
from rich.console import Console
import time
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ utils ë¶ˆëŸ¬ì˜¤ê¸°
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.supabase_manager import SupabaseManager

console = Console()

class OhmyNewsPoliticsCollector:
    def __init__(self):
        self.media_name = "ì˜¤ë§ˆì´ë‰´ìŠ¤"
        self.base_url = "https://www.ohmynews.com"
        self.list_url = "https://www.ohmynews.com/NWS_Web/Articlepage/Total_Article.aspx?PAGE_CD=C0400&pageno={}"
        self.supabase_manager = SupabaseManager()
        self.articles = []
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

    async def run(self, num_pages=10):
        console.print(f"ğŸš€ {self.media_name} ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")

        await self.collect_articles(num_pages)
        await self.collect_contents()
        await self.save_articles()

        console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")

    async def collect_articles(self, num_pages):
        """ëª©ë¡ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        console.print(f"ğŸ“„ {num_pages} í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
        
        for page_num in range(1, num_pages + 1):
            url = self.list_url.format(page_num)
            console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: {url}")

            try:
                soup = await self._fetch_soup(url)
                if not soup:
                    console.print(f"âš ï¸ í˜ì´ì§€ {page_num} ë¡œë“œ ì‹¤íŒ¨")
                    continue

                # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœëŒ€ 20ê°œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
                news_list = soup.select(".news_list")
                if not news_list:
                    console.print(f"âš ï¸ í˜ì´ì§€ {page_num}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    continue

                page_articles = 0
                for news_item in news_list[:20]:  # ìµœëŒ€ 20ê°œ
                    link = news_item.select_one("dt a")
                    if not link:
                        continue

                    title = link.get_text(strip=True)
                    href = link.get("href")

                    if href.startswith("/"):
                        href = f"{self.base_url}{href}"

                    article = {
                        "title": title,
                        "url": href,
                        "published_at": "",
                        "content": "",
                    }
                    self.articles.append(article)
                    page_articles += 1
                    console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")

                console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {page_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                
                # ìš”ì²­ ê°„ ë”œë ˆì´
                await asyncio.sleep(1)

            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue

        console.print(f"ğŸ“Š ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def collect_contents(self):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ + ë°œí–‰ì‹œê°„ ìˆ˜ì§‘"""
        console.print(f"ğŸ“– ìƒì„¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ ({len(self.articles)}ê°œ)")

        for i, article in enumerate(self.articles, 1):
            console.print(f"ğŸ“– [{i}/{len(self.articles)}] {article['title'][:40]}...")
            
            try:
                data = await self._get_article_content(article["url"])
                article["published_at"] = data.get("published_at", "")
                article["content"] = data.get("content", "")
                
                # ìš”ì²­ ê°„ ë”œë ˆì´
                await asyncio.sleep(1.5)
                
            except Exception as e:
                console.print(f"âŒ [{i}] ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                continue

    async def _fetch_soup(self, url: str, max_retries=3) -> BeautifulSoup:
        """URLì—ì„œ BeautifulSoup ê°ì²´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        for attempt in range(max_retries):
            try:
                async with httpx.AsyncClient(
                    timeout=30.0,
                    follow_redirects=True,  # ë¦¬ë‹¤ì´ë ‰íŠ¸ ìë™ ì²˜ë¦¬
                    limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
                ) as client:
                    response = await client.get(url, headers=self.headers)
                    response.raise_for_status()
                    return BeautifulSoup(response.text, "html.parser")
                    
            except (httpx.HTTPStatusError, httpx.TimeoutException, httpx.ConnectError) as e:
                console.print(f"âš ï¸ ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨: {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    console.print(f"âŒ {url} ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    return None
            except Exception as e:
                console.print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
                return None

    async def _get_article_content(self, url: str):
        """ë³¸ë¬¸ê³¼ ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        soup = await self._fetch_soup(url)
        if not soup:
            return {"published_at": "", "content": ""}

        result = {"published_at": "", "content": ""}

        try:
            # 1. ë°œí–‰ ì‹œê°„ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            result["published_at"] = self._extract_publish_date(soup)

            # 2. ë³¸ë¬¸ ì¶”ì¶œ
            result["content"] = self._extract_content(soup)
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # if not result["content"]:
            #     console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url}")
            #     console.print(f"HTML êµ¬ì¡° í™•ì¸ í•„ìš”")

        except Exception as e:
            console.print(f"âš ï¸ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        return result

    def _extract_publish_date(self, soup: BeautifulSoup) -> str:
        """ë°œí–‰ ë‚ ì§œ ì¶”ì¶œ"""
        date_selectors = [
            "span.date",
            ".article_date",
            ".date_time",
            ".publish_time"
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                time_str = date_elem.get_text(strip=True)
                # "ìµœì¢… ì—…ë°ì´íŠ¸", "ê¸°ì‚¬ì…ë ¥" ë“±ì˜ í…ìŠ¤íŠ¸ ì œê±°
                time_str = re.sub(r'(ìµœì¢…\s*ì—…ë°ì´íŠ¸|ê¸°ì‚¬ì…ë ¥|ìˆ˜ì •)', '', time_str).strip()
                
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹± ì‹œë„
                date_formats = [
                    "%y.%m.%d %H:%M",
                    "%Y.%m.%d %H:%M",
                    "%Y-%m-%d %H:%M",
                    "%m/%d %H:%M",
                ]
                
                for fmt in date_formats:
                    try:
                        dt = datetime.strptime(time_str, fmt)
                        # ì—°ë„ê°€ ì—†ëŠ” ê²½ìš° í˜„ì¬ ì—°ë„ ì‚¬ìš©
                        if dt.year == 1900:
                            dt = dt.replace(year=datetime.now().year)
                            
                        kst = pytz.timezone("Asia/Seoul")
                        dt = kst.localize(dt)
                        return dt.astimezone(pytz.UTC).isoformat()
                    except ValueError:
                        continue
                        
                console.print(f"âš ï¸ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {time_str}")
                break
                
        return ""

    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ì¶”ì¶œ"""
        # ì˜¤ë§ˆì´ë‰´ìŠ¤ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        content_div = soup.select_one('div.at_contents[itemprop="articleBody"]')
        
        if not content_div:
            return ""
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in content_div.find_all(['figure', 'script', 'style', 'iframe', 'button', 'img', 'video', 'audio']):
            tag.decompose()
        
        # ê´‘ê³ ì„± div ì œê±°
        for tag in content_div.find_all('div', class_=lambda x: x and ('ad' in x.lower() or 'advertisement' in x.lower())):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        text = content_div.get_text(separator="\n", strip=True)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
        
        return text

    def _is_unwanted_text(self, text: str) -> bool:
        """ì›í•˜ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸ íŒ¨í„´ í™•ì¸"""
        unwanted_patterns = [
            r'^AD\s*$',
            r'ê´‘ê³ ',
            r'â–¶.*ë”ë³´ê¸°',
            r'Copyright.*',
            r'ì €ì‘ê¶Œ.*',
            r'ë¬´ë‹¨.*ì „ì¬.*',
            r'êµ¬ë….*ì‹ ì²­',
        ]
        
        for pattern in unwanted_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False

    async def save_articles(self):
        """DB ì €ì¥"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")

        try:
            media_outlet = self.supabase_manager.get_media_outlet(self.media_name)
            if media_outlet:
                media_id = media_outlet["id"]
            else:
                media_id = self.supabase_manager.create_media_outlet(self.media_name)

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            success_count, skip_count, fail_count = 0, 0, 0
            
            for i, article in enumerate(self.articles, 1):
                if article["url"] in existing_urls:
                    console.print(f"âš ï¸ [{i}] ì¤‘ë³µ ìŠ¤í‚µ: {article['title'][:40]}")
                    skip_count += 1
                    continue

                # ë¹ˆ ì½˜í…ì¸ ë„ ì €ì¥ (ìŠ¤í‚µí•˜ì§€ ì•ŠìŒ)

                article_data = {
                    "title": article["title"],
                    "url": article["url"],
                    "content": article["content"],
                    "published_at": article["published_at"] or datetime.now(pytz.UTC).isoformat(),
                    "created_at": datetime.now(pytz.UTC).isoformat(),
                    "media_id": media_id,
                }

                success = self.supabase_manager.insert_article(article_data)
                if success:
                    console.print(f"âœ… [{i}] ì €ì¥ ì„±ê³µ: {article['title'][:40]}")
                    success_count += 1
                else:
                    console.print(f"âŒ [{i}] ì €ì¥ ì‹¤íŒ¨: {article['title'][:40]}")
                    fail_count += 1

            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {success_count}, ìŠ¤í‚µ {skip_count}, ì‹¤íŒ¨ {fail_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

async def main():
    collector = OhmyNewsPoliticsCollector()
    await collector.run(num_pages=10)

if __name__ == "__main__":
    asyncio.run(main())
