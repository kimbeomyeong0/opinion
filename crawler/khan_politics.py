#!/usr/bin/env python3
"""
ê²½í–¥ì‹ ë¬¸ ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
"""

import asyncio
import sys
import os
from datetime import datetime
import httpx
import pytz
from playwright.async_api import async_playwright
from rich.console import Console

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.supabase_manager import SupabaseManager

console = Console()

class KhanPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.khan.co.kr"
        self.api_url = "https://www.khan.co.kr/SecListData.html"
        self.media_name = "ê²½í–¥ì‹ ë¬¸"
        self.articles = []
        self.supabase_manager = SupabaseManager()
        
    async def run(self, num_pages: int = 5):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            console.print(f"ğŸš€ {self.media_name} ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
            
            # ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
            await self.collect_articles(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ë³¸ë¬¸ ìˆ˜ì§‘
            await self.collect_contents()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            await self.save_articles()
            
            console.print("\\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    async def collect_articles(self, num_pages: int = 5):
        """ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
        
        for page in range(1, num_pages + 1):
            console.print(f"\\nğŸ“„ í˜ì´ì§€ {page}/{num_pages} ì²˜ë¦¬ ì¤‘...")
            articles = await self._get_page_articles(page)
            self.articles.extend(articles)
        
        console.print(f"\\nğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: {len(self.articles)}ê°œ ì„±ê³µ")
    
    async def _get_page_articles(self, page_num: int):
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # í˜„ì¬ ë…„ì›” ê³„ì‚°
            now = datetime.now()
            year = now.year
            month = now.month
            
            # API ìš”ì²­ ë°ì´í„°
            payload = {
                "syncType": "async",
                "type": "politics", 
                "year": str(year),
                "month": str(month).zfill(2),
                "page": str(page_num)
            }
            
            console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num} API í˜¸ì¶œ: {payload}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.api_url,
                    data=payload,
                    headers={
                        "Content-Type": "application/x-www-form-urlencoded",
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                    }
                )
                response.raise_for_status()
                
                data = response.json()
                items = data.get("items", [])
                console.print(f"ğŸ“Š í˜ì´ì§€ {page_num}ì—ì„œ {len(items)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                
                articles = []
                for i, item in enumerate(items[:10]):  # ê° í˜ì´ì§€ì—ì„œ ìµœëŒ€ 10ê°œ ìˆ˜ì§‘
                    article = {
                        "art_id": item.get("art_id"),
                        "title": item.get("art_title", ""),
                        "summary": item.get("summary", ""),
                        "publish_date": item.get("publish_date", ""),
                        "url": item.get("url", f"{self.base_url}/article/{item.get('art_id')}")
                    }
                    articles.append(article)
                    console.print(f"ğŸ“° ê¸°ì‚¬ {i+1}: {article['title'][:50]}...")
                
                return articles
                
        except Exception as e:
            console.print(f"âŒ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []

    async def collect_contents(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘"""
        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬")
        
        # ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©ì„ ìœ„í•´ í•œ ë²ˆë§Œ ì‹¤í–‰
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            for i, article in enumerate(self.articles, 1):
                console.print(f"ğŸ“– [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘: {article['title'][:50]}...")
                
                content_data = await self._extract_content_with_browser(page, article['url'])
                
                # ê¸°ì‚¬ ë°ì´í„°ì— ë³¸ë¬¸ê³¼ ë°œí–‰ì‹œê°„ ì¶”ê°€
                article['content'] = content_data.get('content', '')
                article['published_at'] = content_data.get('published_at', article.get('publish_date', ''))
                
                console.print(f"âœ… [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
            
            await browser.close()
        
    async def _extract_content_with_browser(self, page, url: str):
        """ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
        try:
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            # ë³¸ë¬¸ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            try:
                await page.wait_for_selector('div.art_body#articleBody', timeout=5000)
            except:
                try:
                    await page.wait_for_selector('div.art_body', timeout=5000)
                except:
                    await page.wait_for_selector('div[class*="art_body"]', timeout=5000)
            
            # JavaScriptë¡œ ë°ì´í„° ì¶”ì¶œ
            content_data = await page.evaluate("""
                () => {
                    const result = {
                        published_at: '',
                        content: ''
                    };
                    
                    // 1. ë°œí–‰ì‹œê° ì¶”ì¶œ (ìˆ˜ì • ì‹œê°„ ìš°ì„ )
                    const timeContainer = document.querySelector('a[title*="ê¸°ì‚¬ ì…ë ¥/ìˆ˜ì •ì¼"]');
                    if (timeContainer) {
                        const paragraphs = timeContainer.querySelectorAll('p');
                        let inputTime = '';
                        let modifyTime = '';
                        
                        paragraphs.forEach(p => {
                            const text = p.textContent || '';
                            if (text.includes('ì…ë ¥')) {
                                inputTime = text.replace('ì…ë ¥', '').trim();
                            } else if (text.includes('ìˆ˜ì •')) {
                                modifyTime = text.replace('ìˆ˜ì •', '').trim();
                            }
                        });
                        
                        // ìˆ˜ì • ì‹œê°„ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì…ë ¥ ì‹œê°„ ì‚¬ìš©
                        result.published_at = modifyTime || inputTime;
                    }
                    
                    // 2. ë³¸ë¬¸ ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                    let articleBody = document.querySelector('div.art_body#articleBody');
                    if (!articleBody) {
                        articleBody = document.querySelector('div.art_body');
                    }
                    if (!articleBody) {
                        articleBody = document.querySelector('div[class*="art_body"]');
                    }
                    if (!articleBody) {
                        // ëŒ€ì²´ ì„ íƒìë“¤ ì‹œë„
                        articleBody = document.querySelector('div[class*="article"]');
                    }
                    if (!articleBody) {
                        articleBody = document.querySelector('div[class*="content"]');
                    }
                    
                    if (articleBody) {
                        // ê´‘ê³ /ë°°ë„ˆ ì œê±°
                        const banners = articleBody.querySelectorAll('div[class*="banner"], div[class*="ad"], div[class*="advertisement"]');
                        banners.forEach(banner => banner.remove());
                        
                        // ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                        let contentParagraphs = articleBody.querySelectorAll('p.content_text.text-l');
                        if (contentParagraphs.length === 0) {
                            contentParagraphs = articleBody.querySelectorAll('p.content_text');
                        }
                        if (contentParagraphs.length === 0) {
                            contentParagraphs = articleBody.querySelectorAll('p.text-l');
                        }
                        if (contentParagraphs.length === 0) {
                            contentParagraphs = articleBody.querySelectorAll('p');
                        }
                        
                        const contentTexts = [];
                        
                        contentParagraphs.forEach(p => {
                            let text = p.textContent || '';
                            
                            // ê¸°ìëª…, ì´ë©”ì¼, ì¶œì²˜ ì œê±°
                            text = text.replace(/[ê°€-í£]+\s*ê¸°ì/g, '');
                            text = text.replace(/[ê°€-í£]+\s*íŠ¹íŒŒì›/g, '');
                            text = text.replace(/[ê°€-í£]+\s*í†µì‹ ì›/g, '');
                            text = text.replace(/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g, '');
                            text = text.replace(/\[ì¶œì²˜:[^\]]+\]/g, '');
                            text = text.replace(/\[ê²½í–¥ì‹ ë¬¸\]/g, '');
                            
                            text = text.trim();
                            if (text && text.length > 10) {  // ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                                contentTexts.push(text);
                            }
                        });
                        
                        result.content = contentTexts.join('\\n\\n');
                    } else {
                        // ë””ë²„ê¹…ì„ ìœ„í•´ í˜ì´ì§€ êµ¬ì¡° í™•ì¸
                        console.log('ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ divë“¤:');
                        const allDivs = document.querySelectorAll('div[class*="art"], div[class*="article"], div[class*="content"]');
                        allDivs.forEach(div => {
                            console.log('í´ë˜ìŠ¤:', div.className, 'ID:', div.id);
                        });
                    }
                    
                    return result;
                }
            """)
            
            return content_data
            
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            return {"published_at": "", "content": ""}
    
    async def _extract_content(self, url: str):
        """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ë°œí–‰ì‹œê°, ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                await page.goto(url, wait_until='domcontentloaded', timeout=30000)
                # ë³¸ë¬¸ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                await page.wait_for_selector('div.art_body#articleBody', timeout=10000)
                
                # JavaScriptë¡œ ë°ì´í„° ì¶”ì¶œ
                content_data = await page.evaluate("""
                    () => {
                        const result = {
                            published_at: '',
                            content: ''
                        };
                        
                        // 1. ë°œí–‰ì‹œê° ì¶”ì¶œ (ìˆ˜ì • ì‹œê°„ ìš°ì„ )
                        const timeContainer = document.querySelector('a[title*="ê¸°ì‚¬ ì…ë ¥/ìˆ˜ì •ì¼"]');
                        if (timeContainer) {
                            const paragraphs = timeContainer.querySelectorAll('p');
                            let inputTime = '';
                            let modifyTime = '';
                            
                            paragraphs.forEach(p => {
                                const text = p.textContent || '';
                                if (text.includes('ì…ë ¥')) {
                                    inputTime = text.replace('ì…ë ¥', '').trim();
                                } else if (text.includes('ìˆ˜ì •')) {
                                    modifyTime = text.replace('ìˆ˜ì •', '').trim();
                                }
                            });
                            
                            // ìˆ˜ì • ì‹œê°„ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì…ë ¥ ì‹œê°„ ì‚¬ìš©
                            result.published_at = modifyTime || inputTime;
                        }
                        
                        // 2. ë³¸ë¬¸ ì¶”ì¶œ
                        const articleBody = document.querySelector('div.art_body#articleBody');
                        if (articleBody) {
                            // ê´‘ê³ /ë°°ë„ˆ ì œê±°
                            const banners = articleBody.querySelectorAll('div[class*="banner"]');
                            banners.forEach(banner => banner.remove());
                            
                            // ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            const contentParagraphs = articleBody.querySelectorAll('p.content_text.text-l');
                            const contentTexts = [];
                            
                            contentParagraphs.forEach(p => {
                                let text = p.textContent || '';
                                
                                // ê¸°ìëª…, ì´ë©”ì¼, ì¶œì²˜ ì œê±°
                                text = text.replace(/[ê°€-í£]+\s*ê¸°ì/g, '');
                                text = text.replace(/[ê°€-í£]+\s*íŠ¹íŒŒì›/g, '');
                                text = text.replace(/[ê°€-í£]+\s*í†µì‹ ì›/g, '');
                                text = text.replace(/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g, '');
                                text = text.replace(/\[ì¶œì²˜:[^\]]+\]/g, '');
                                text = text.replace(/\[ê²½í–¥ì‹ ë¬¸\]/g, '');
                                
                                text = text.trim();
                                if (text && text.length > 10) {  // ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                                    contentTexts.push(text);
                                }
                            });
                            
                            result.content = contentTexts.join('\\n\\n');
                        }
                        
                        return result;
                    }
                """)
                
                await browser.close()
                return content_data
                
                        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            return {"published_at": "", "content": ""}
    
    def parse_published_time(self, time_str: str) -> datetime:
        """ë°œí–‰ì‹œê°„ ë¬¸ìì—´ì„ UTC datetimeìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not time_str or not time_str.strip():
            return datetime.now(pytz.UTC)
        
        try:
            # "2025.01.05 21:43" í˜•ì‹ íŒŒì‹±
            clean_time = time_str.strip()
            if '.' in clean_time and ':' in clean_time:
                # KST ì‹œê°„ìœ¼ë¡œ íŒŒì‹±
                published_at = datetime.strptime(clean_time, "%Y.%m.%d %H:%M")
                kst = pytz.timezone("Asia/Seoul")
                published_at = kst.localize(published_at)
                # UTCë¡œ ë³€í™˜
                return published_at.astimezone(pytz.UTC)
            else:
                console.print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‹œê°„ í˜•ì‹: {clean_time}")
                return datetime.now(pytz.UTC)
            
        except Exception as e:
            console.print(f"âš ï¸ ë°œí–‰ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {time_str} - {e}")
            return datetime.now(pytz.UTC)
    
    async def save_articles(self):
        """ê¸°ì‚¬ ì €ì¥"""
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ì–¸ë¡ ì‚¬ í™•ì¸
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(
                name=self.media_name,
                bias="center-left",
                website=self.base_url
            )
        else:
            media_id = media["id"]
        
        # ì¤‘ë³µ ì²´í¬
        urls = [art["url"] for art in self.articles]
        existing_urls = set()
        
        try:
            for url in urls:
                exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                if exists.data:
                    existing_urls.add(url)
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        success_count = 0
        skip_count = 0
        
        for i, article in enumerate(self.articles, 1):
            try:
                # ì¤‘ë³µ ì²´í¬
                if article["url"] in existing_urls:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")
                    skip_count += 1
                    continue
                
                # ë°œí–‰ì‹œê°„ íŒŒì‹±
                published_at = self.parse_published_time(article["published_at"])
                
                # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
                article_data = {
                    "title": article["title"],
                    "url": article["url"],
                    "content": article["content"],
                    "published_at": published_at.isoformat(),
                    "created_at": datetime.now(pytz.UTC).isoformat(),
                    "media_id": media_id
                }
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                success = self.supabase_manager.insert_article(article_data)
                if success:
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                    success_count += 1
                else:
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}...")
                    
            except Exception as e:
                console.print(f"âŒ [{i}/{len(self.articles)}] ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        console.print(f"\\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skip_count}ê°œ")
        total_processed = success_count + skip_count
        success_rate = (success_count / total_processed) * 100 if total_processed > 0 else 0
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")

async def main():
    collector = KhanPoliticsCollector()
    await collector.run(num_pages=10)  # 10í˜ì´ì§€ì—ì„œ ê°ê° 10ê°œì”© ì´ 100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())