#!/usr/bin/env python3
"""
ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹)
ì¡°ì„ ì¼ë³´/ë‰´ìŠ¤1ê³¼ ë™ì¼í•œ ê·œì¹™ ì ìš©: 100ê°œ, ë¹ ë¥´ê²Œ, ìƒˆë¡œìš´ articles í…Œì´ë¸”, ì‹œê°„ ì²˜ë¦¬
"""

import asyncio
import json
import sys
import os
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
from urllib.parse import urljoin
import httpx
import pytz
from bs4 import BeautifulSoup
from rich.console import Console
from rich.table import Table
from playwright.async_api import async_playwright
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.supabase_manager import SupabaseManager

load_dotenv()

console = Console()
KST = pytz.timezone("Asia/Seoul")

class DongaPoliticsCollector:
    """ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹)"""
    
    def __init__(self):
        self.base_url = "https://www.donga.com"
        self.politics_url = "https://www.donga.com/news/Politics"
        self.media_name = "ë™ì•„ì¼ë³´"
        self.media_bias = "right"  # ë™ì•„ì¼ë³´ëŠ” ìš°í¸í–¥
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        self._playwright = None
        self._browser = None

    async def _collect_latest_articles(self):
        """í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ ìµœì‹  100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì¤‘ë³µ ì œì™¸)"""
        console.print("ğŸ”Œ í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ ìµœì‹  ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘ ì‹œì‘...")
        target_count = 100
        page = 1
        page_offset = 0
        max_attempts = 20  # ìµœëŒ€ 20í˜ì´ì§€ ì‹œë„

        async with httpx.AsyncClient(timeout=10.0) as client:
            for attempt in range(max_attempts):
                if len(self.articles) >= target_count:
                    break
                
                # í˜ì´ì§€ URL êµ¬ì„±
                if page == 1:
                    url = self.politics_url
                else:
                    # ë™ì•„ì¼ë³´ í˜ì´ì§€ë„¤ì´ì…˜: p=11, p=21, p=31...
                    page_offset = (page - 1) * 10
                    url = f"{self.politics_url}?p={page_offset + 1}&prod=news&ymd=&m="
                
                console.print(f"ğŸ“¡ í˜ì´ì§€ {page}/20 ì²˜ë¦¬ ì¤‘ (offset: {page_offset})")
                
                try:
                    response = await client.get(url)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ê¸°ì‚¬ ì¹´ë“œ ì°¾ê¸°
                    articles = soup.select('.news_card')
                    console.print(f"ğŸ“Š í˜ì´ì§€ {page} ì‘ë‹µ: {len(articles)}ê°œ ìš”ì†Œ ìˆ˜ì‹ ")
                    
                    parsed_count = 0
                    added_count = 0
                    
                    for article in articles:
                        if len(self.articles) >= target_count:
                            break
                            
                        parsed_article = self._parse_html_article(article)
                        if parsed_article:
                            parsed_count += 1
                            if self._add_article(parsed_article):
                                added_count += 1
                    
                    console.print(f"ğŸ“ˆ í˜ì´ì§€ {page} íŒŒì‹± ì„±ê³µ: {parsed_count}ê°œ, ìµœì¢… ì¶”ê°€: {added_count}ê°œ")
                    console.print(f"ğŸ“Š í˜„ì¬ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(self.articles)}ê°œ")
                    
                    if len(articles) == 0:
                        console.print(f"âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        break
                    
                    page += 1
                    await asyncio.sleep(0.5)  # í˜ì´ì§€ ê°„ ë”œë ˆì´
                    
                except Exception as e:
                    console.print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    break
        
        console.print(f"ğŸ¯ ìˆ˜ì§‘ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ (ëª©í‘œ: {target_count}ê°œ)")

    def _parse_html_article(self, article_element) -> Optional[Dict]:
        """HTML ê¸°ì‚¬ ìš”ì†Œ íŒŒì‹±"""
        try:
            # ì œëª© ë§í¬ ì°¾ê¸°
            title_link = article_element.select_one('.tit a')
            if not title_link or not title_link.get('href'):
                return None
            
            title = title_link.get_text(strip=True)
            if not title or len(title) < 5:
                return None
            
            # URL ì²˜ë¦¬
            href = title_link.get('href')
            if href.startswith('/'):
                url = urljoin(self.base_url, href)
            else:
                url = href
            
            # ë™ì•„ì¼ë³´ ê¸°ì‚¬ URL íŒ¨í„´ í™•ì¸ (ì •ì¹˜ ì„¹ì…˜ë§Œ)
            if '/article/' not in url or 'donga.com' not in url:
                return None
            
            # Opinion ì„¹ì…˜ ì œì™¸ (ì •ì¹˜ ê¸°ì‚¬ë§Œ)
            if '/Opinion/' in url:
                return None
            
            # ë°œí–‰ ì‹œê°„ ì¶”ì¶œ (ë™ì•„ì¼ë³´ëŠ” ìƒëŒ€ì  ì‹œê°„ ì‚¬ìš©)
            published_at = self._extract_published_time(article_element)
            
            return {
                "title": title.strip(),
                "url": url,
                "content": "",  # ë³¸ë¬¸ì€ ë‚˜ì¤‘ì— Playwrightë¡œ ì±„ì›€
                "published_at": published_at,
                "created_at": published_at,  # ë°œí–‰ ì‹œê°„ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
            }
            
        except Exception as e:
            console.print(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return None

    def _extract_published_time(self, article_element) -> Optional[datetime]:
        """ê¸°ì‚¬ ìš”ì†Œì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ"""
        try:
            # ë™ì•„ì¼ë³´ëŠ” ìƒëŒ€ì  ì‹œê°„ í‘œì‹œ ("1ì‹œê°„ ì „", "2ì‹œê°„ ì „" ë“±)
            time_elem = article_element.select_one('.date, .time, .publish_date')
            if time_elem:
                time_text = time_elem.get_text(strip=True)
                
                # "1ì‹œê°„ ì „", "2ì‹œê°„ ì „" ë“±ì˜ ìƒëŒ€ì  ì‹œê°„ ì²˜ë¦¬
                if 'ì‹œê°„ ì „' in time_text:
                    import re
                    hours_match = re.search(r'(\d+)ì‹œê°„ ì „', time_text)
                    if hours_match:
                        hours = int(hours_match.group(1))
                        return datetime.now(KST).replace(tzinfo=None) - timedelta(hours=hours)
                
                # "1ì¼ ì „", "2ì¼ ì „" ë“±ì˜ ì²˜ë¦¬
                elif 'ì¼ ì „' in time_text:
                    import re
                    days_match = re.search(r'(\d+)ì¼ ì „', time_text)
                    if days_match:
                        days = int(days_match.group(1))
                        return datetime.now(KST).replace(tzinfo=None) - timedelta(days=days)
                
                # "1ë¶„ ì „", "2ë¶„ ì „" ë“±ì˜ ì²˜ë¦¬
                elif 'ë¶„ ì „' in time_text:
                    import re
                    minutes_match = re.search(r'(\d+)ë¶„ ì „', time_text)
                    if minutes_match:
                        minutes = int(minutes_match.group(1))
                        return datetime.now(KST).replace(tzinfo=None) - timedelta(minutes=minutes)
            
            # ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°„
            return datetime.now(KST).replace(tzinfo=None)
            
        except Exception as e:
            console.print(f"âš ï¸ ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return datetime.now(KST).replace(tzinfo=None)

    def _add_article(self, article: Dict) -> bool:
        """ê¸°ì‚¬ë¥¼ ëª©ë¡ì— ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)"""
        url = article.get("url")
        if not url:
            return False
        
        # URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        for existing_article in self.articles:
            if existing_article.get("url") == url:
                return False
        
        self.articles.append(article)
        return True

    async def _extract_content(self, url: str) -> str:
        """Playwrightë¥¼ ì‚¬ìš©í•œ ë³¸ë¬¸ ì¶”ì¶œ (ë§¥ë¶ ì—ì–´ M2 ìµœì í™”)"""
        if not self._browser:
            return ""
        
        try:
            page = await self._browser.new_page()
            
            # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
            await page.set_viewport_size({"width": 1280, "height": 720})
            
            await page.goto(url, wait_until="domcontentloaded", timeout=10000)
            
            # JavaScriptë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (ë™ì•„ì¼ë³´ íŠ¹í™”)
            content = await page.evaluate("""
                () => {
                    // ë™ì•„ì¼ë³´ ë³¸ë¬¸ ì„ íƒìë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ) - section.news_viewê°€ ì‹¤ì œ ë³¸ë¬¸
                    const selectors = [
                        'section.news_view',
                        'section.news_view .article_txt',
                        'section.news_view .content',
                        'section.news_view .article_body',
                        '.article_txt',
                        '.content',
                        '.article_body',
                        'main .article_txt',
                        'article .article_txt'
                    ];
                    
                    for (const selector of selectors) {
                        const element = document.querySelector(selector);
                        if (element) {
                            // ê´‘ê³ ì™€ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° (ë™ì•„ì¼ë³´ íŠ¹í™”)
                            const unwanted = element.querySelectorAll('.ad, .advertisement, .view_ad06, .view_m_adA, .view_m_adB, .view_m_adC, .view_m_adK, .a1, script, .related_news, .social_share, .recommend_keyword, .keyword_list, .company_info, .footer_info, .contact_info, .copyright, .publish_info, .img_cont, .articlePhotoC, .sub_tit');
                            unwanted.forEach(el => el.remove());
                            
                            // p íƒœê·¸ì™€ ì§ì ‘ í…ìŠ¤íŠ¸ ëª¨ë‘ ì¶”ì¶œ
                            const paragraphs = element.querySelectorAll('p');
                            let content = '';
                            
                            if (paragraphs.length > 0) {
                                content = Array.from(paragraphs)
                                    .slice(0, 20)  // ìµœëŒ€ 20ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ì œí•œ
                                    .map(p => p.textContent.trim())
                                    .filter(text => {
                                        // ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ í•„í„°ë§
                                        if (text.length < 10) return false;
                                        if (text.includes('ì¶”ì²œ ê²€ìƒ‰ì–´')) return false;
                                        if (text.includes('ì…ë ¥ 2025-')) return false;
                                        if (text.includes('ê¸€ìí¬ê¸° ì„¤ì •')) return false;
                                        if (text.includes('ë””ì§€í„¸ë© ë””ì§€í„¸ë‰´ìŠ¤íŒ€')) return false;
                                        if (text.includes('ì‚¬ì‹¤ë§Œ ì“°ë ¤ê³  ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤')) return false;
                                        if (text.includes('ëŒ“ê¸€ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”')) return false;
                                        if (text.includes('ì£¼ì†Œ ì„œìš¸íŠ¹ë³„ì‹œ')) return false;
                                        if (text.includes('ì „í™”ë²ˆí˜¸ 02-')) return false;
                                        if (text.includes('ë“±ë¡ë²ˆí˜¸ ì„œìš¸ì•„')) return false;
                                        if (text.includes('ë°œí–‰ì¼ì 1996')) return false;
                                        if (text.includes('ë“±ë¡ì¼ì 2009')) return false;
                                        if (text.includes('ë°œí–‰Â·í¸ì§‘ì¸')) return false;
                                        if (text.includes('ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬')) return false;
                                        if (text.includes('ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬')) return false;
                                        if (text.includes('ê¸°ì ì‚¬ì§„')) return false;
                                        if (text.includes('ë””ì§€í„¸ë© ë””ì§€í„¸ë‰´ìŠ¤íŒ€')) return false;
                                        if (text.includes('êµ¬ë…')) return false;
                                        if (text.includes('ì¶”ì²œ')) return false;
                                        if (text.includes('ì¼ìƒì´ ì—­ì‚¬ê°€ ë˜ëŠ” ì‹œê°„')) return false;
                                        if (text.includes('ì—°ì´ ë‹¿ì•„ ì‹œê°„ì„ ê³µìœ í•´ì£¼ì‹ ')) return false;
                                        if (text.includes('ê¹Šì´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤')) return false;
                                        return true;
                                    })
                                    .join('\\n\\n');
                            }
                            
                            // p íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ (br íƒœê·¸ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸)
                            if (!content || content.length < 50) {
                                const textContent = element.textContent || element.innerText || '';
                                const lines = textContent.split(/\\n|\\r\\n|\\r/).map(line => line.trim()).filter(line => line.length > 10);
                                content = lines.join('\\n\\n');
                            }
                            
                            return content;
                        }
                    }
                    
                    // fallback: ëª¨ë“  p íƒœê·¸ì—ì„œ ì¶”ì¶œ (ë” ê´€ëŒ€í•œ í•„í„°ë§)
                    const allParagraphs = document.querySelectorAll('p');
                    return Array.from(allParagraphs)
                        .slice(0, 30)  // ë” ë§ì€ ë¬¸ë‹¨ í—ˆìš©
                        .map(p => p.textContent.trim())
                        .filter(text => {
                            // ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ í•„í„°ë§
                            if (text.length < 5) return false;
                            if (text.includes('ì¶”ì²œ ê²€ìƒ‰ì–´')) return false;
                            if (text.includes('ì…ë ¥ 2025-')) return false;
                            if (text.includes('ê¸€ìí¬ê¸° ì„¤ì •')) return false;
                            if (text.includes('ë””ì§€í„¸ë© ë””ì§€í„¸ë‰´ìŠ¤íŒ€')) return false;
                            if (text.includes('ì‚¬ì‹¤ë§Œ ì“°ë ¤ê³  ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤')) return false;
                            if (text.includes('ëŒ“ê¸€ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”')) return false;
                            if (text.includes('ì£¼ì†Œ ì„œìš¸íŠ¹ë³„ì‹œ')) return false;
                            if (text.includes('ì „í™”ë²ˆí˜¸ 02-')) return false;
                            if (text.includes('ë“±ë¡ë²ˆí˜¸ ì„œìš¸ì•„')) return false;
                            if (text.includes('ë°œí–‰ì¼ì 1996')) return false;
                            if (text.includes('ë“±ë¡ì¼ì 2009')) return false;
                            if (text.includes('ë°œí–‰Â·í¸ì§‘ì¸')) return false;
                            if (text.includes('ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬')) return false;
                            if (text.includes('ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬')) return false;
                            if (text.includes('ê¸°ì ì‚¬ì§„')) return false;
                            if (text.includes('ë””ì§€í„¸ë© ë””ì§€í„¸ë‰´ìŠ¤íŒ€')) return false;
                            if (text.includes('êµ¬ë…')) return false;
                            if (text.includes('ì¶”ì²œ')) return false;
                            if (text.includes('ì¼ìƒì´ ì—­ì‚¬ê°€ ë˜ëŠ” ì‹œê°„')) return false;
                            if (text.includes('ì—°ì´ ë‹¿ì•„ ì‹œê°„ì„ ê³µìœ í•´ì£¼ì‹ ')) return false;
                            if (text.includes('ê¹Šì´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤')) return false;
                            return true;
                        })
                        .join('\\n\\n');
                }
            """)
            
            await page.close()
            
            if content and len(content) > 30:  # ë” ê´€ëŒ€í•œ ê¸¸ì´ ê¸°ì¤€
                return content.strip()
            else:
                # BeautifulSoup fallback ì‹œë„
                try:
                    html = await page.content()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ë™ì•„ì¼ë³´ ë³¸ë¬¸ ì„ íƒìë“¤ (section.news_viewê°€ ì‹¤ì œ ë³¸ë¬¸)
                    content_selectors = [
                        'section.news_view',
                        'section.news_view .article_txt',
                        'section.news_view .content',
                        'section.news_view .article_body',
                        '.article_txt',
                        '.content',
                        '.article_body'
                    ]
                    
                    for selector in content_selectors:
                        element = soup.select_one(selector)
                        if element:
                            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                            for unwanted in element.select('.ad, .advertisement, .view_ad06, .view_m_adA, .view_m_adB, .view_m_adC, .view_m_adK, .a1, script, .related_news, .social_share, .recommend_keyword, .keyword_list, .company_info, .footer_info, .contact_info, .copyright, .publish_info, .img_cont, .articlePhotoC, .sub_tit'):
                                unwanted.decompose()
                            
                            content = element.get_text(separator='\n', strip=True)
                            if content and len(content) > 30:
                                # í…ìŠ¤íŠ¸ í•„í„°ë§
                                lines = content.split('\n')
                                filtered_lines = []
                                for line in lines:
                                    line = line.strip()
                                    if (len(line) > 10 and 
                                        'ì¶”ì²œ ê²€ìƒ‰ì–´' not in line and
                                        'ì…ë ¥ 2025-' not in line and
                                        'ê¸€ìí¬ê¸° ì„¤ì •' not in line and
                                        'ë””ì§€í„¸ë© ë””ì§€í„¸ë‰´ìŠ¤íŒ€' not in line and
                                        'ì‚¬ì‹¤ë§Œ ì“°ë ¤ê³  ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤' not in line and
                                        'ëŒ“ê¸€ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”' not in line and
                                        'ì£¼ì†Œ ì„œìš¸íŠ¹ë³„ì‹œ' not in line and
                                        'ì „í™”ë²ˆí˜¸ 02-' not in line and
                                        'ë“±ë¡ë²ˆí˜¸ ì„œìš¸ì•„' not in line and
                                        'ë°œí–‰ì¼ì 1996' not in line and
                                        'ë“±ë¡ì¼ì 2009' not in line and
                                        'ë°œí–‰Â·í¸ì§‘ì¸' not in line and
                                        'ê¸°ì ì‚¬ì§„' not in line and
                                        'ë””ì§€í„¸ë© ë””ì§€í„¸ë‰´ìŠ¤íŒ€' not in line and
                                        'êµ¬ë…' not in line and
                                        'ì¶”ì²œ' not in line and
                                        'ì¼ìƒì´ ì—­ì‚¬ê°€ ë˜ëŠ” ì‹œê°„' not in line and
                                        'ì—°ì´ ë‹¿ì•„ ì‹œê°„ì„ ê³µìœ í•´ì£¼ì‹ ' not in line and
                                        'ê¹Šì´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤' not in line):
                                        filtered_lines.append(line)
                                
                                if filtered_lines:
                                    return '\n\n'.join(filtered_lines)
                    
                    return ""
                except:
                    return ""
                
        except Exception as e:
            console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {str(e)}")
            try:
                await page.close()
            except:
                pass
            return ""

    async def collect_contents(self):
        """ë³¸ë¬¸ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”)"""
        if not self.articles:
            console.print("âš ï¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬ (ë°°ì¹˜ ì²˜ë¦¬)")
        
        # Playwright ì´ˆê¸°í™” (ë§¥ë¶ ì—ì–´ M2 ìµœì í™”)
        self._playwright = await async_playwright().start()
        self._browser = await self._playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--memory-pressure-off'
            ]
        )
        
        try:
            batch_size = 2  # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½, ì•ˆì •ì„± í–¥ìƒ)
            total_batches = (len(self.articles) + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(self.articles))
                batch_articles = self.articles[start_idx:end_idx]
                
                console.print(f"ğŸ“„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘...")
                
                for i, article in enumerate(batch_articles):
                    article_idx = start_idx + i + 1
                    url = article["url"]
                    
                    content = await self._extract_content(url)
                    if content:
                        article["content"] = content
                        console.print(f"âœ… [{article_idx}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
                    else:
                        console.print(f"âš ï¸ [{article_idx}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
                    
                    # ê¸°ì‚¬ ê°„ ë”œë ˆì´
                    await asyncio.sleep(0.5)
                
                # ë°°ì¹˜ ê°„ ë”œë ˆì´ (ë©”ëª¨ë¦¬ ì •ë¦¬)
                if batch_idx < total_batches - 1:
                    console.print("â³ ë°°ì¹˜ ê°„ ëŒ€ê¸° ì¤‘... (ë©”ëª¨ë¦¬ ì •ë¦¬)")
                    await asyncio.sleep(2)
            
            console.print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {len([a for a in self.articles if a.get('content')])}/{len(self.articles)}ê°œ ì„±ê³µ")
            
        finally:
            # Playwright ì •ë¦¬
            if self._browser:
                await self._browser.close()
            if self._playwright:
                await self._playwright.stop()
            self._browser = None
            self._playwright = None

    async def save_to_supabase(self):
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ì–¸ë¡ ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        media_info = self.supabase_manager.get_media_outlet(self.media_name)
        if not media_info:
            media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias, self.base_url)
            if not media_id:
                console.print("âŒ ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
        else:
            media_id = media_info['id']
        
        # ê¸°ì¡´ ê¸°ì‚¬ URL ì¤‘ë³µ ì²´í¬
        existing_urls = set()
        try:
            for article in self.articles:
                url = article.get("url")
                if url:
                    existing_urls.add(url)
            
            if existing_urls:
                # Supabaseì—ì„œ ê¸°ì¡´ URLë“¤ ì¡°íšŒ
                result = self.supabase_manager.client.table('articles').select('url').in_('url', list(existing_urls)).execute()
                existing_urls_in_db = {row['url'] for row in result.data}
                console.print(f"ğŸ” ê¸°ì¡´ ê¸°ì‚¬ ì¤‘ë³µ ì²´í¬ ì¤‘...")
                console.print(f"ğŸ“Š ì¤‘ë³µ ì²´í¬ ì™„ë£Œ: {len(existing_urls_in_db)}ê°œ ì¤‘ë³µ ë°œê²¬")
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {str(e)}")
            existing_urls_in_db = set()
        
        # ê¸°ì‚¬ ì €ì¥
        success_count = 0
        failed_count = 0
        skipped_count = 0
        
        for i, article in enumerate(self.articles, 1):
            try:
                url = article.get("url")
                
                # ì¤‘ë³µ ì²´í¬
                if url in existing_urls_in_db:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")
                    skipped_count += 1
                    continue
                
                # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
                article_data = {
                    "media_id": media_id,
                    "title": article["title"],
                    "content": article.get("content", ""),
                    "url": url,
                    "published_at": article["published_at"].strftime('%Y-%m-%d %H:%M:%S') if article["published_at"] else None,
                    "created_at": article["created_at"].strftime('%Y-%m-%d %H:%M:%S') if article["created_at"] else None
                }
                
                # Supabaseì— ì €ì¥
                if self.supabase_manager.insert_article(article_data):
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                    success_count += 1
                else:
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}...")
                    failed_count += 1
                    
            except Exception as e:
                console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                failed_count += 1
        
        # ê²°ê³¼ ì¶œë ¥
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skipped_count}ê°œ")
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_count/(success_count+failed_count)*100:.1f}%" if (success_count+failed_count) > 0 else "  ğŸ“ˆ ì„±ê³µë¥ : 0.0%")

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self._browser:
                await self._browser.close()
            if self._playwright:
                await self._playwright.stop()
        except Exception as e:
            console.print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}")

    async def run(self):
        """ì‹¤í–‰ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
        try:
            console.print(f"ğŸš€ ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹  100ê°œ)")
            console.print("ğŸ’¡ ë§¥ë¶ ì—ì–´ M2 ìµœì í™” ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
            
            await self._collect_latest_articles()
            await self.collect_contents()
            await self.save_to_supabase()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            await self.cleanup()
            console.print("ğŸ§¹ Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    collector = DongaPoliticsCollector()
    await collector.run()

if __name__ == "__main__":
    asyncio.run(main())