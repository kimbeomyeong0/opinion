#!/usr/bin/env python3
"""
ë‰´ì‹œìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ë³‘ë ¬ì²˜ë¦¬ ìµœì í™” ë²„ì „)
"""
import asyncio
import sys
import os
import httpx
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from datetime import datetime
import pytz
from rich.console import Console

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.supabase_manager import SupabaseManager

console = Console()

BASE_URL = "https://www.newsis.com"
LIST_URL = "https://www.newsis.com/pol/list/"


class NewsisPoliticsCollector:
    def __init__(self):
        self.articles = []
        self.semaphore = asyncio.Semaphore(5)  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
        self.supabase_manager = SupabaseManager()

    async def run(self, num_pages=1):
        console.print("ğŸš€ ë‰´ì‹œìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
        await self.collect_articles(num_pages)
        await self.collect_contents_parallel()  # ë³‘ë ¬ ì²˜ë¦¬!
        await self.save_articles()  # DB ì €ì¥
        console.print("ğŸ‰ ì™„ë£Œ")

    async def collect_articles(self, num_pages=1):
        for page in range(1, num_pages + 1):
            url = f"{LIST_URL}?cid=10300&scid=10301&page={page}"
            console.print(f"ğŸ“¡ ëª©ë¡ ìš”ì²­: {url}")

            async with httpx.AsyncClient() as client:
                r = await client.get(url)
                soup = BeautifulSoup(r.text, "html.parser")

                for el in soup.select(".txtCont")[:5]:  # ì•ì—ì„œ 5ê°œë§Œ
                    a = el.select_one(".tit a")
                    if not a:
                        continue

                    title = a.get_text(strip=True)
                    href = a["href"]
                    if href.startswith("/"):
                        href = BASE_URL + href

                    self.articles.append(
                        {"title": title, "url": href, "content": "", "published_at": ""}
                    )
                    console.print(f"ğŸ“° {title[:50]}...")

    async def collect_contents_parallel(self):
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë³¸ë¬¸ ìˆ˜ì§‘"""
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ë³‘ë ¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘...")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            
            # ë³‘ë ¬ ì‘ì—… ì‹¤í–‰
            tasks = []
            for i, article in enumerate(self.articles):
                task = self._extract_single_content(browser, article, i + 1)
                tasks.append(task)
            
            # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            await asyncio.gather(*tasks, return_exceptions=True)
            
            await browser.close()

    async def _extract_single_content(self, browser, article, index):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ)"""
        async with self.semaphore:  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            page = None
            try:
                page = await browser.new_page()
                console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
                
                # í˜ì´ì§€ ë¡œë“œ (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
                await page.goto(article["url"], wait_until="domcontentloaded", timeout=20000)

                # ë°œí–‰ ì‹œê°„ ì¶”ì¶œ
                try:
                    time_text = await page.inner_text("span:has-text('ìˆ˜ì •')", timeout=5000)
                    time_str = time_text.replace("ìˆ˜ì •", "").strip()
                    dt = datetime.strptime(time_str, "%Y.%m.%d %H:%M:%S")
                    kst = pytz.timezone("Asia/Seoul")
                    article["published_at"] = kst.localize(dt).astimezone(pytz.UTC).isoformat()
                except Exception:
                    article["published_at"] = datetime.now(pytz.UTC).isoformat()

                # ë³¸ë¬¸ ì¶”ì¶œ
                content = await page.evaluate(
                    """
                    () => {
                        const article = document.querySelector("article");
                        if (!article) return "";

                        // ê´‘ê³ /ì´ë¯¸ì§€/ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°
                        article.querySelectorAll("iframe, script, .banner, img, .ad").forEach(el => el.remove());

                        // ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        const text = article.innerText || article.textContent || "";
                        
                        // ê¸°ìëª…, ì´ë©”ì¼ ë“± ì œê±°
                        return text
                            .replace(/[ê°€-í£]+\\s*ê¸°ì/g, '')
                            .replace(/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}/g, '')
                            .replace(/\\[ë‰´ì‹œìŠ¤\\]/g, '')
                            .trim();
                    }
                """
                )
                
                article["content"] = content
                console.print(f"âœ… [{index}] ì™„ë£Œ: {len(content)}ì")
                
            except Exception as e:
                console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
                article["content"] = ""
                article["published_at"] = datetime.now(pytz.UTC).isoformat()
                
            finally:
                if page:
                    await page.close()

    async def save_articles(self):
        """ìˆ˜ì§‘í•œ ê¸°ì‚¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ë¯¸ë””ì–´ ì•„ì›ƒë › ID ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        media_outlet = self.supabase_manager.get_media_outlet("ë‰´ì‹œìŠ¤")
        if media_outlet:
            media_id = media_outlet['id']
        else:
            media_id = self.supabase_manager.create_media_outlet("ë‰´ì‹œìŠ¤")
        
        # ê¸°ì¡´ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬ìš©)
        existing_urls = set()
        try:
            result = self.supabase_manager.client.table('articles').select('url').eq('media_id', media_id).execute()
            existing_urls = {article['url'] for article in result.data}
        except Exception as e:
            console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        
        success_count = 0
        skip_count = 0
        
        for i, article in enumerate(self.articles, 1):
            try:
                # ì¤‘ë³µ ì²´í¬
                if article['url'] in existing_urls:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")
                    skip_count += 1
                    continue
                
                # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
                article_data = {
                    'title': article['title'],
                    'url': article['url'],
                    'content': article.get('content', ''),
                    'published_at': article.get('published_at', datetime.now(pytz.UTC).isoformat()),
                    'created_at': datetime.now(pytz.UTC).isoformat(),
                    'media_id': media_id
                }
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                success = self.supabase_manager.insert_article(article_data)
                
                if success:
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                    success_count += 1
                else:
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}...")
                
            except Exception as e:
                console.print(f"âŒ [{i}/{len(self.articles)}] ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skip_count}ê°œ")
        total_processed = success_count + skip_count
        success_rate = (success_count / total_processed) * 100 if total_processed > 0 else 0
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")


# ë” ë¹ ë¥¸ ë²„ì „: httpxë§Œ ì‚¬ìš© (Playwright ì—†ì´)
class NewsisFastCollector:
    """httpxë§Œ ì‚¬ìš©í•˜ëŠ” ì´ˆê³ ì† ë²„ì „"""
    
    def __init__(self):
        self.articles = []
        self.supabase_manager = SupabaseManager()

    async def run(self, num_pages=1):
        console.print("ğŸš€ ë‰´ì‹œìŠ¤ ì´ˆê³ ì† í¬ë¡¤ë§ ì‹œì‘")
        await self.collect_articles(num_pages)
        await self.collect_contents_httpx_only()
        await self.save_articles()  # DB ì €ì¥
        console.print("ğŸ‰ ì™„ë£Œ")

    async def collect_articles(self, num_pages=1):
        async with httpx.AsyncClient() as client:
            for page in range(1, num_pages + 1):
                url = f"{LIST_URL}?cid=10300&scid=10301&page={page}"
                console.print(f"ğŸ“¡ ëª©ë¡ ìš”ì²­: {url}")

                r = await client.get(url)
                soup = BeautifulSoup(r.text, "html.parser")

                for el in soup.select(".txtCont")[:5]:
                    a = el.select_one(".tit a")
                    if not a:
                        continue

                    title = a.get_text(strip=True)
                    href = a["href"]
                    if href.startswith("/"):
                        href = BASE_URL + href

                    self.articles.append({"title": title, "url": href})
                    console.print(f"ğŸ“° {title[:50]}...")

    async def collect_contents_httpx_only(self):
        """httpxë§Œìœ¼ë¡œ ë³‘ë ¬ ë³¸ë¬¸ ìˆ˜ì§‘ - ì´ˆê³ ì†!"""
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ì´ˆê³ ì† ë³‘ë ¬ ìˆ˜ì§‘...")
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = []
            for i, article in enumerate(self.articles):
                task = self._extract_with_httpx(client, article, i + 1)
                tasks.append(task)
            
            await asyncio.gather(*tasks, return_exceptions=True)

    async def _extract_with_httpx(self, client, article, index):
        """httpxë¡œ HTML íŒŒì‹±ë§Œìœ¼ë¡œ ì´ˆê³ ì† ì¶”ì¶œ"""
        try:
            console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
            
            response = await client.get(article["url"])
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë°œí–‰ì‹œê°„ ì¶”ì¶œ
            time_elem = soup.select_one("span:-soup-contains('ìˆ˜ì •')")
            if time_elem:
                time_str = time_elem.get_text().replace("ìˆ˜ì •", "").strip()
                try:
                    dt = datetime.strptime(time_str, "%Y.%m.%d %H:%M:%S")
                    kst = pytz.timezone("Asia/Seoul")
                    article["published_at"] = kst.localize(dt).astimezone(pytz.UTC).isoformat()
                except:
                    article["published_at"] = datetime.now(pytz.UTC).isoformat()
            else:
                article["published_at"] = datetime.now(pytz.UTC).isoformat()
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            article_elem = soup.select_one("article")
            if article_elem:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in article_elem.find_all(["script", "iframe", "img"]):
                    tag.decompose()
                
                content = article_elem.get_text(separator='\n', strip=True)
                # ì •ë¦¬
                content = content.replace('ê¸°ì', '').replace('[ë‰´ì‹œìŠ¤]', '').strip()
                article["content"] = content
            else:
                article["content"] = ""
            
            console.print(f"âœ… [{index}] ì™„ë£Œ: {len(article.get('content', ''))}ì")
            
        except Exception as e:
            console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
            article["content"] = ""
            article["published_at"] = datetime.now(pytz.UTC).isoformat()

    async def save_articles(self):
        """ìˆ˜ì§‘í•œ ê¸°ì‚¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ë¯¸ë””ì–´ ì•„ì›ƒë › ID ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        media_outlet = self.supabase_manager.get_media_outlet("ë‰´ì‹œìŠ¤")
        if media_outlet:
            media_id = media_outlet['id']
        else:
            media_id = self.supabase_manager.create_media_outlet("ë‰´ì‹œìŠ¤")
        
        # ê¸°ì¡´ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬ìš©)
        existing_urls = set()
        try:
            result = self.supabase_manager.client.table('articles').select('url').eq('media_id', media_id).execute()
            existing_urls = {article['url'] for article in result.data}
        except Exception as e:
            console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        
        success_count = 0
        skip_count = 0
        
        for i, article in enumerate(self.articles, 1):
            try:
                # ì¤‘ë³µ ì²´í¬
                if article['url'] in existing_urls:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")
                    skip_count += 1
                    continue
                
                # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
                article_data = {
                    'title': article['title'],
                    'url': article['url'],
                    'content': article.get('content', ''),
                    'published_at': article.get('published_at', datetime.now(pytz.UTC).isoformat()),
                    'created_at': datetime.now(pytz.UTC).isoformat(),
                    'media_id': media_id
                }
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                success = self.supabase_manager.insert_article(article_data)
                
                if success:
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                    success_count += 1
                else:
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}...")
                
            except Exception as e:
                console.print(f"âŒ [{i}/{len(self.articles)}] ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skip_count}ê°œ")
        total_processed = success_count + skip_count
        success_rate = (success_count / total_processed) * 100 if total_processed > 0 else 0
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")


async def main():
    console.print("ì„ íƒí•˜ì„¸ìš”:")
    console.print("1. ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „ (Playwright)")
    console.print("2. ì´ˆê³ ì† ë²„ì „ (httpxë§Œ ì‚¬ìš©)")
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ì´ˆê³ ì† ë²„ì „ ì‹¤í–‰
    collector = NewsisFastCollector()
    await collector.run(num_pages=1)
    
    # ê²°ê³¼ ì¶œë ¥
    for i, art in enumerate(collector.articles, 1):
        console.print(f"\n=== ê¸°ì‚¬ {i} ===")
        console.print(f"ì œëª©: {art['title']}")
        console.print(f"URL: {art['url']}")
        console.print(f"ë°œí–‰ì‹œê°„: {art.get('published_at', 'N/A')}")
        console.print(f"ë³¸ë¬¸ ê¸¸ì´: {len(art.get('content', ''))}ì")
        if art.get('content'):
            console.print(f"ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {art['content'][:100]}...")


if __name__ == "__main__":
    asyncio.run(main())