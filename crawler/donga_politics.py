#!/usr/bin/env python3
"""
ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ë°˜)
- ë™ì•„ì¼ë³´ ì •ì¹˜ ì„¹ì…˜ì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ ê¸°ì‚¬ ìˆ˜ì§‘
- ê° í˜ì´ì§€ì—ì„œ ìµœìƒë‹¨ ê¸°ì‚¬ 1ê°œì”© ìˆ˜ì§‘
- í˜ì´ì§€ ê·œì¹™: p=1, 11, 21, 31... (10ì”© ì¦ê°€)
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin
import httpx
import pytz
from rich.console import Console
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")


class DongaPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.donga.com"
        self.politics_url = "https://www.donga.com/news/Politics"
        self.media_name = "ë™ì•„ì¼ë³´"
        self.media_bias = "center"
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        self._playwright = None
        self._browser = None

    def _get_page_urls(self, num_pages: int = 4) -> List[str]:
        """í˜ì´ì§€ URL ëª©ë¡ ìƒì„± (p=1, 11, 21, 31...)"""
        urls = []
        for i in range(num_pages):
            page_num = i * 10 + 1  # 1, 11, 21, 31...
            url = f"{self.politics_url}?p={page_num}&prod=news&ymd=&m="
            urls.append(url)
        return urls

    async def _get_page_articles(self, page_url: str) -> List[Dict]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        console.print(f"ğŸ“¡ í˜ì´ì§€ ìˆ˜ì§‘: {page_url}")
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.get(page_url)
                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                articles = []
                
                # ë™ì•„ì¼ë³´ ì •ì¹˜ ì„¹ì…˜ì˜ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                # ë””ë²„ê¹…ì„ ìœ„í•´ HTML êµ¬ì¡° í™•ì¸
                console.print("ğŸ” HTML êµ¬ì¡° ë¶„ì„ ì¤‘...")
                
                # divide_area ì°¾ê¸°
                divide_area = soup.find('div', class_='divide_area')
                console.print(f"divide_area ë°œê²¬: {divide_area is not None}")
                
                if divide_area:
                    # ì˜¬ë°”ë¥¸ êµ¬ì¡°: divide_area > section.sub_news_sec > ul.row_list > li > article.news_card
                    sub_news_sec = divide_area.find('section', class_='sub_news_sec')
                    console.print(f"sub_news_sec (section) ë°œê²¬: {sub_news_sec is not None}")
                    
                    if sub_news_sec:
                        row_list = sub_news_sec.find('ul', class_='row_list')
                        console.print(f"row_list ë°œê²¬: {row_list is not None}")
                        
                        if row_list:
                            li_items = row_list.find_all('li')
                            console.print(f"ğŸ” row_listì—ì„œ {len(li_items)}ê°œ li ìš”ì†Œ ë°œê²¬")
                            
                            # ê° í˜ì´ì§€ì—ì„œ ìµœëŒ€ 10ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
                            collected_count = 0
                            max_articles_per_page = 10
                            
                            for i, li in enumerate(li_items):
                                if collected_count >= max_articles_per_page:
                                    break
                                    
                                # li ì•ˆì—ì„œ article.news_card ì°¾ê¸°
                                news_card = li.find('article', class_='news_card')
                                console.print(f"li[{i}]ì—ì„œ news_card ë°œê²¬: {news_card is not None}")
                                
                                if news_card:
                                    # news_card ì•ˆì˜ êµ¬ì¡° í™•ì¸
                                    news_head = news_card.find('header', class_='news_head')
                                    news_body = news_card.find('div', class_='news_body')
                                    console.print(f"  news_head: {news_head is not None}, news_body: {news_body is not None}")
                                    
                                    # news_card ì•ˆì—ì„œ ë§í¬ ì°¾ê¸° (news_bodyì˜ .tit aì—ì„œ)
                                    link = None
                                    
                                    # news_bodyì˜ .tit aì—ì„œ ë§í¬ ì°¾ê¸°
                                    if news_body:
                                        tit_link = news_body.find('h4', class_='tit')
                                        if tit_link:
                                            link = tit_link.find('a', href=True)
                                            console.print(f"  .tit aì—ì„œ ë§í¬ ë°œê²¬: {link is not None}")
                                    
                                    # ëŒ€ì•ˆ: news_headì—ì„œ ë§í¬ ì°¾ê¸°
                                    if not link and news_head:
                                        link = news_head.find('a', href=True)
                                        console.print(f"  news_headì—ì„œ ë§í¬ ë°œê²¬: {link is not None}")
                                    
                                    # ëŒ€ì•ˆ: news_cardì—ì„œ ì§ì ‘ ë§í¬ ì°¾ê¸°
                                    if not link:
                                        link = news_card.find('a', href=True)
                                        console.print(f"  news_cardì—ì„œ ì§ì ‘ ë§í¬ ë°œê²¬: {link is not None}")
                                    
                                    if link:
                                        href = link.get('href')
                                        category = link.get('data-ep_button_category')
                                        console.print(f"  ë§í¬ href: {href}")
                                        console.print(f"  ì¹´í…Œê³ ë¦¬: {category}")
                                        
                                        # ì •ì¹˜ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§ (data ì†ì„±ìœ¼ë¡œ)
                                        is_politics = False
                                        
                                        if href and '/news/' in href and '/article/' in href:
                                            # data ì†ì„±ìœ¼ë¡œ ì •ì¹˜ ì¹´í…Œê³ ë¦¬ í™•ì¸
                                            if category == 'ì •ì¹˜':
                                                is_politics = True
                                                console.print(f"  data ì†ì„±ìœ¼ë¡œ ì •ì¹˜ ê¸°ì‚¬ í™•ì¸: {category}")
                                        
                                        if is_politics:
                                            console.print(f"  ì •ì¹˜ ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ ë°œê²¬!")
                                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                                            if href.startswith('/'):
                                                full_url = urljoin(self.base_url, href)
                                            else:
                                                full_url = href
                                            
                                            # ì œëª© ì¶”ì¶œ (data-ep_button_name ì†ì„± ìš°ì„  ì‚¬ìš©)
                                            title = link.get('data-ep_button_name', '').strip()
                                            console.print(f"  ì œëª© ì¶”ì¶œ ì‹œë„ 1 - data-ep_button_name: '{title}'")
                                            
                                            # ëŒ€ì•ˆ: data-ep_contentdata_content_title ì†ì„±
                                            if not title:
                                                title = link.get('data-ep_contentdata_content_title', '').strip()
                                                console.print(f"  ì œëª© ì¶”ì¶œ ì‹œë„ 2 - data-ep_contentdata_content_title: '{title}'")
                                            
                                            # ëŒ€ì•ˆ: a íƒœê·¸ì˜ ì§ì ‘ í…ìŠ¤íŠ¸ ë…¸ë“œ
                                            if not title:
                                                title_text = link.find(text=True, recursive=False)
                                                if title_text:
                                                    title = title_text.strip()
                                                console.print(f"  a íƒœê·¸ ì§ì ‘ í…ìŠ¤íŠ¸: '{title}'")
                                            
                                            # ëŒ€ì•ˆ: img íƒœê·¸ì˜ alt ì†ì„±
                                            if not title:
                                                img_tag = link.find('img')
                                                if img_tag:
                                                    title = img_tag.get('alt', '').strip()
                                                console.print(f"  img alt ì†ì„±: '{title}'")
                                            
                                            console.print(f"  ìµœì¢… ì œëª©: '{title[:50]}...'")
                                            
                                            if title and len(title) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                                                articles.append({
                                                    'title': title,
                                                    'url': full_url
                                                })
                                                collected_count += 1
                                                console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬ [{collected_count}]: {title[:50]}...")
                                            else:
                                                if i == 0:
                                                    console.print(f"  ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ: '{title}'")
                                else:
                                    console.print(f"  li[{i}]ì—ì„œ news_cardë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    console.print("âš ï¸ divide_areaë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    # ì „ì²´ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
                    links = soup.find_all('a', href=True)
                    news_links = [link for link in links if link.get('href') and '/news/article/' in link.get('href')]
                    console.print(f"ì „ì²´ í˜ì´ì§€ì—ì„œ {len(news_links)}ê°œ ë‰´ìŠ¤ ë§í¬ ë°œê²¬")
                    
                    for link in news_links:
                        href = link.get('href')
                        if href.startswith('/'):
                            full_url = urljoin(self.base_url, href)
                        else:
                            full_url = href
                        
                        title = link.get_text(strip=True)
                        if title and len(title) > 10:
                            articles.append({
                                'title': title,
                                'url': full_url
                            })
                            console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬: {title[:50]}...")
                            break
                
                console.print(f"ğŸ“Š í˜ì´ì§€ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                return articles
                
            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                return []

    async def _collect_articles(self, num_pages: int = 4):
        """ê¸°ì‚¬ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜)"""
        console.print(f"ğŸš€ ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {num_pages}í˜ì´ì§€)")
        
        page_urls = self._get_page_urls(num_pages)
        all_articles = []
        
        for i, page_url in enumerate(page_urls, 1):
            console.print(f"ğŸ“„ í˜ì´ì§€ {i}/{len(page_urls)} ì²˜ë¦¬ ì¤‘...")
            articles = await self._get_page_articles(page_url)
            all_articles.extend(articles)
            
            # í˜ì´ì§€ ê°„ ëŒ€ê¸°
            await asyncio.sleep(0.5)
        
        # ê° ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
        success_count = 0
        for i, article_data in enumerate(all_articles, 1):
            parsed_article = self._parse_article_data(article_data)
            if parsed_article:
                self.articles.append(parsed_article)
                success_count += 1
                console.print(f"âœ… [{i}/{len(all_articles)}] {parsed_article['title'][:50]}...")
            else:
                console.print(f"âŒ [{i}/{len(all_articles)}] ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨")

        console.print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(all_articles)}ê°œ ì„±ê³µ")

    def _parse_article_data(self, article_data: Dict) -> Optional[Dict]:
        """ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±"""
        try:
            title = article_data.get("title")
            url = article_data.get("url")
            content_data = article_data.get("content_data", {})
            
            if not title or not url:
                return None

            # ë°œí–‰ ì‹œê°„ íŒŒì‹± ë° UTC ë³€í™˜
            published_at = None
            if content_data.get("published_at"):
                try:
                    # KST ì‹œê°„ íŒŒì‹±
                    kst_time = datetime.strptime(content_data["published_at"], "%Y-%m-%d %H:%M")
                    # KST íƒ€ì„ì¡´ ì ìš©
                    kst_tz = pytz.timezone("Asia/Seoul")
                    kst_dt = kst_tz.localize(kst_time)
                    # UTCë¡œ ë³€í™˜
                    published_at = kst_dt.astimezone(pytz.UTC).isoformat()
                except Exception as e:
                    console.print(f"âš ï¸ ë°œí–‰ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    published_at = None

            return {
                "title": title.strip(),
                "url": url,
                "content": content_data.get("content", ""),
                "published_at": published_at,
                "created_at": datetime.now(KST).isoformat(),
                "author": "",  # ë‚˜ì¤‘ì— ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ
                "section": "ì •ì¹˜",
                "tags": [],
                "description": "",
            }
        except Exception as e:
            console.print(f"âŒ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    async def _extract_content(self, url: str) -> Dict[str, str]:
        """Playwrightë¡œ ë³¸ë¬¸ ì „ë¬¸ ì¶”ì¶œ"""
        page = None
        try:
            if not self._browser:
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox', 
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--memory-pressure-off'
                    ]
                )

            page = await self._browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 720})
            await page.goto(url, wait_until="domcontentloaded", timeout=10000)

            # ë™ì•„ì¼ë³´ ë³¸ë¬¸ ë° ë°œí–‰ ì‹œê°„ ì¶”ì¶œ
            result = {"content": "", "published_at": ""}
            
            try:
                result = await page.evaluate('''() => {
                    // <section class="news_view"> ì°¾ê¸°
                    const newsView = document.querySelector('section.news_view');
                    if (!newsView) {
                        return {content: '', published_at: ''};
                    }
                    
                    // ë°œí–‰ ì‹œê°„ ì¶”ì¶œ (<span aria-hidden="true">2025-09-04 15:33</span>)
                    let publishedAt = '';
                    const timeSpan = document.querySelector('span[aria-hidden="true"]');
                    if (timeSpan) {
                        const timeText = timeSpan.textContent.trim();
                        // YYYY-MM-DD HH:MM í˜•ì‹ì¸ì§€ í™•ì¸
                        if (/^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}$/.test(timeText)) {
                            publishedAt = timeText;
                        }
                    }
                    
                    // ì œì™¸í•  ìš”ì†Œë“¤ ì œê±°
                    const excludeSelectors = [
                        'h2.sub_tit',  // ê¸°ì‚¬ ì œëª©
                        'figure',      // ì´ë¯¸ì§€ ê´€ë ¨
                        'img',         // ì´ë¯¸ì§€
                        'figcaption',  // ì´ë¯¸ì§€ ìº¡ì…˜
                        '.view_m_adK', // ê´‘ê³  ì˜ì—­
                        '.view_ad06',  // ê´‘ê³  ì˜ì—­
                        '.view_m_adA', // ê´‘ê³  ì˜ì—­
                        '.view_m_adB', // ê´‘ê³  ì˜ì—­
                        '.a1'          // ê´‘ê³  ì˜ì—­
                    ];
                    
                    // ì œì™¸í•  ìš”ì†Œë“¤ ì œê±°
                    excludeSelectors.forEach(selector => {
                        const elements = newsView.querySelectorAll(selector);
                        elements.forEach(el => el.remove());
                    });
                    
                    // í…ìŠ¤íŠ¸ ë…¸ë“œë“¤ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜ì§‘
                    const textNodes = [];
                    const walker = document.createTreeWalker(
                        newsView,
                        NodeFilter.SHOW_TEXT,
                        {
                            acceptNode: function(node) {
                                const text = node.textContent.trim();
                                if (text.length === 0) {
                                    return NodeFilter.FILTER_REJECT;
                                }
                                return NodeFilter.FILTER_ACCEPT;
                            }
                        }
                    );
                    
                    let node;
                    while (node = walker.nextNode()) {
                        textNodes.push(node.textContent.trim());
                    }
                    
                    // <br> íƒœê·¸ë¥¼ ë¬¸ë‹¨ êµ¬ë¶„ìë¡œ ì²˜ë¦¬í•˜ê³  í…ìŠ¤íŠ¸ ì—°ê²°
                    let content = textNodes.join(' ').replace(/\\s+/g, ' ').trim();
                    
                    // <br> íƒœê·¸ë¥¼ ë¬¸ë‹¨ êµ¬ë¶„ìë¡œ ë³€í™˜
                    content = content.replace(/<br\\s*\\/?>/gi, '\\n\\n');
                    
                    // ì—°ì†ëœ ê³µë°± ì •ë¦¬
                    content = content.replace(/\\s+/g, ' ').trim();
                    
                    return {content: content, published_at: publishedAt};
                }''')
                
                if result.get("content") and len(result["content"].strip()) > 50:
                    return result
                    
            except Exception as e:
                console.print(f"âš ï¸ JavaScript ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:50]}")
            
            return result
            
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url[:50]}...): {str(e)[:50]}")
            return {"content": "", "published_at": ""}
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass

    async def collect_contents(self):
        """ë³¸ë¬¸ ì „ë¬¸ ìˆ˜ì§‘"""
        if not self.articles:
            return

        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬")
        
        success_count = 0
        for i, art in enumerate(self.articles, 1):
            content_data = await self._extract_content(art["url"])
            if content_data and content_data.get("content"):
                # ë³¸ë¬¸ê³¼ ë°œí–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
                self.articles[i-1]["content"] = content_data["content"]
                if content_data.get("published_at"):
                    # ë°œí–‰ ì‹œê°„ íŒŒì‹± ë° UTC ë³€í™˜
                    try:
                        kst_time = datetime.strptime(content_data["published_at"], "%Y-%m-%d %H:%M")
                        kst_tz = pytz.timezone("Asia/Seoul")
                        kst_dt = kst_tz.localize(kst_time)
                        self.articles[i-1]["published_at"] = kst_dt.astimezone(pytz.UTC).isoformat()
                    except Exception as e:
                        console.print(f"âš ï¸ [{i}/{len(self.articles)}] ë°œí–‰ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")
                
                success_count += 1
                console.print(f"âœ… [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
            else:
                console.print(f"âš ï¸ [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨")

        console.print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(self.articles)}ê°œ ì„±ê³µ")

    async def save_to_supabase(self):
        """DB ì €ì¥"""
        if not self.articles:
            console.print("âŒ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")

        # ì–¸ë¡ ì‚¬ í™•ì¸
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
        else:
            media_id = media["id"]

        # ì¤‘ë³µ ì²´í¬
        urls = [art["url"] for art in self.articles]
        existing_urls = set()
        
        try:
            for url in urls:
                exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                if exists.data:
                    existing_urls.add(url)
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")

        success, failed, skipped = 0, 0, 0
        
        for i, art in enumerate(self.articles, 1):
            try:
                if art["url"] in existing_urls:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {art['title'][:30]}...")
                    skipped += 1
                    continue

                published_at_str = art.get("published_at")
                created_at_str = art.get("created_at", published_at_str)

                article_data = {
                    "media_id": media_id,
                    "title": art["title"],
                    "content": art["content"],
                    "url": art["url"],
                    "published_at": published_at_str,
                    "created_at": created_at_str,
                }

                if self.supabase_manager.insert_article(article_data):
                    success += 1
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {art['title'][:30]}...")
                else:
                    failed += 1
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {art['title'][:30]}...")
                    
            except Exception as e:
                failed += 1
                console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì˜¤ë¥˜: {str(e)[:50]}")

        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success}ê°œ")
        console.print(f"  âŒ ì‹¤íŒ¨: {failed}ê°œ") 
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skipped}ê°œ")
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(success / len(self.articles) * 100):.1f}%")

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self._browser:
                await self._browser.close()
                self._browser = None
            if self._playwright:
                await self._playwright.stop()
                self._playwright = None
            console.print("ğŸ§¹ Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            console.print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}")

    async def run(self, num_pages: int = 4):
        """ì‹¤í–‰"""
        try:
            console.print(f"ğŸš€ ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {num_pages}í˜ì´ì§€)")
            
            await self._collect_articles(num_pages)
            await self.collect_contents()
            await self.save_to_supabase()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            await self.cleanup()


async def main():
    collector = DongaPoliticsCollector()
    await collector.run(num_pages=10)  # 10í˜ì´ì§€ì—ì„œ ê°ê° 10ê°œì”© ì´ 100ê°œ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())