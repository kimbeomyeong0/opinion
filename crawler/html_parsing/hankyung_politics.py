#!/usr/bin/env python3
"""
í•œêµ­ê²½ì œ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (HTML íŒŒì‹± ê¸°ë°˜)
- ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ ìˆ˜ì§‘
- ë³¸ë¬¸ì€ httpxë¡œ ë³„ë„ ìˆ˜ì§‘
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import httpx
import pytz
from bs4 import BeautifulSoup
import re
import html
from rich.console import Console

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")


class HankyungPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.hankyung.com"
        self.media_name = "í•œêµ­ê²½ì œ"
        self.media_bias = "left"  # ì§„ë³´ ì„±í–¥
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # ë™ì‹œì„± ì œí•œ ì„¤ì •
        self.semaphore = asyncio.Semaphore(10)

    def _get_page_urls(self, num_pages: int = 8) -> List[str]:
        """í˜ì´ì§€ URL ëª©ë¡ ìƒì„± (page=1, 2, 3...)"""
        urls = []
        for page in range(1, num_pages + 1):
            url = f"{self.base_url}/all-news-politics?page={page}"
            urls.append(url)
        return urls

    async def _get_page_articles(self, page_url: str, page_num: int) -> List[Dict]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: HTML íŒŒì‹± ì¤‘...")

        async with self.semaphore:
            try:
                async with httpx.AsyncClient(
                    timeout=15.0,
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
                ) as client:
                    response = await client.get(page_url, headers=self.headers)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    articles = []
                    
                    # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ (.allnews-wrap .allnews-panel ul.allnews-list > li[data-aid])
                    list_items = soup.select('.allnews-wrap .allnews-panel ul.allnews-list > li[data-aid]')
                    
                    for li in list_items:
                        try:
                            # data-aidì—ì„œ article_id ì¶”ì¶œ
                            data_aid = li.get('data-aid', '')
                            article_id = data_aid
                            
                            # ì œëª©ê³¼ URL ì¶”ì¶œ (h2.news-tit a[href])
                            title_link = li.select_one('h2.news-tit a[href]')
                            if not title_link:
                                continue
                                
                            href = title_link.get('href')
                            if not href:
                                continue
                            
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if href.startswith('http'):
                                full_url = href
                            else:
                                full_url = urljoin(self.base_url, href)
                            
                            title = title_link.get_text(strip=True)
                            
                            # article_id ë³´ì • (/article/(\d+)ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ)
                            article_id_match = re.search(r'/article/(\d+)', href)
                            if article_id_match:
                                article_id = article_id_match.group(1)
                            
                            # ë°œí–‰ì‹œê° ì¶”ì¶œ (.txt-date)
                            published_date, published_at_kst, published_at_utc = self._extract_published_time(li)
                            
                            # ì¸ë„¤ì¼ ì¶”ì¶œ (.thumb img)
                            image_url, image_alt = self._extract_thumbnail(li)
                            
                            # í…ìŠ¤íŠ¸ ì •ë¦¬ (HTML ì—”í‹°í‹° ë””ì½”ë“œ í›„ ê³µë°± ì •ë¦¬)
                            title = self._clean_text(title)
                            
                            if title and len(title) > 10:
                                article = {
                                    'article_id': article_id,
                                    'title': title,
                                    'url': full_url,
                                    'content': '',
                                    'published_date': published_date,
                                    'published_at': published_at_utc,
                                    'image_url': image_url,
                                    'image_alt': image_alt
                                }
                                articles.append(article)
                                console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")
                        
                        except Exception as e:
                            console.print(f"âš ï¸ ê¸°ì‚¬ ì¹´ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            continue
                    
                    # ì¤‘ë³µ ì œê±° (article_id ê¸°ì¤€)
                    unique_articles = []
                    seen_ids = set()
                    for article in articles:
                        if article['article_id'] not in seen_ids:
                            unique_articles.append(article)
                            seen_ids.add(article['article_id'])
                    
                    console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {len(unique_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    return unique_articles
                    
            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return []

    def _extract_published_time(self, li) -> tuple:
        """ë°œí–‰ì‹œê° ì¶”ì¶œ (.txt-date)"""
        try:
            date_element = li.select_one('.txt-date')
            if not date_element:
                return "", "", ""
            
            date_text = date_element.get_text(strip=True)
            # YYYY.MM.DD HH:MM í˜•ì‹ íŒŒì‹±
            date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})\s+(\d{2}):(\d{2})', date_text)
            if not date_match:
                return "", "", ""
            
            year, month, day, hour, minute = date_match.groups()
            published_date = f"{year}-{month}-{day}"
            published_at_kst = f"{year}-{month}-{day}T{hour}:{minute}:00+09:00"
            
            # KSTë¥¼ UTCë¡œ ë³€í™˜
            try:
                kst_dt = datetime.strptime(f"{year}-{month}-{day} {hour}:{minute}", "%Y-%m-%d %H:%M")
                kst_dt = KST.localize(kst_dt)
                utc_dt = kst_dt.astimezone(pytz.UTC)
                published_at_utc = utc_dt.isoformat()
            except:
                published_at_utc = datetime.now(pytz.UTC).isoformat()
            
            return published_date, published_at_kst, published_at_utc
                
        except Exception as e:
            console.print(f"âš ï¸ ë°œí–‰ì‹œê° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return "", "", ""

    def _extract_thumbnail(self, li) -> tuple:
        """ì¸ë„¤ì¼ ì¶”ì¶œ (.thumb img)"""
        try:
            img_element = li.select_one('.thumb img')
            if not img_element:
                return None, ""
            
            src = img_element.get('src', '')
            alt = img_element.get('alt', '')
            
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            if src and not src.startswith('http'):
                src = urljoin(self.base_url, src)
            
            return src, alt
                
        except Exception as e:
            console.print(f"âš ï¸ ì¸ë„¤ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None, ""

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬ (HTML ì—”í‹°í‹° ë””ì½”ë“œ í›„ ê³µë°± ì •ë¦¬)"""
        try:
            # HTML ì—”í‹°í‹° ë””ì½”ë“œ
            text = html.unescape(text)
            # &nbsp; ì œê±°
            text = re.sub(r'&nbsp;', ' ', text)
            # ë‹¤ì¤‘ ê³µë°± ì¶•ì•½
            text = re.sub(r'\s+', ' ', text)
            # trim
            return text.strip()
        except:
            return text

    async def collect_articles_parallel(self, num_pages: int = 8):
        """ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        page_urls = self._get_page_urls(num_pages)
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        tasks = [self._get_page_articles(url, i + 1) for i, url in enumerate(page_urls)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_articles = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(result)}")
            else:
                self.articles.extend(result)
                total_articles += len(result)
                
        console.print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def collect_contents_parallel(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        if not self.articles:
            return
            
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì„œ)
        batch_size = 20
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(self.articles))
            batch_articles = self.articles[start_idx:end_idx]
            
            console.print(f"ğŸ“– ë°°ì¹˜ {batch_num + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            async with httpx.AsyncClient(
                timeout=15.0,
                follow_redirects=True,
                limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
            ) as client:
                tasks = [self._extract_content_httpx(client, article, i + start_idx + 1) for i, article in enumerate(batch_articles)]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ë”œë ˆì´
            if batch_num < total_batches - 1:
                await asyncio.sleep(0.5)

    async def _extract_content_httpx(self, client: httpx.AsyncClient, article: dict, index: int):
        """httpxë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
            
            response = await client.get(article["url"], headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_data = self._extract_content_text(soup)
            article["content"] = content_data.get("text", "")
            
            console.print(f"âœ… [{index}] ì™„ë£Œ: {len(article['content'])}ì")
            
        except Exception as e:
            console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
            article["content"] = ""

    def _extract_content_text(self, soup: BeautifulSoup) -> Dict[str, any]:
        """í•œêµ­ê²½ì œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # .article-body#articletxt[itemprop="articleBody"] ì°¾ê¸°
            content_container = soup.select_one('.article-body#articletxt[itemprop="articleBody"]')
            
            if not content_container:
                console.print("âš ï¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {"text": "", "byline": {}, "lead_image": None}
            
            # ì œì™¸í•  ìš”ì†Œë“¤ ì œê±° (figureëŠ” ì œì™¸í•˜ê³  ë‚˜ì¤‘ì— ë³„ë„ ì²˜ë¦¬)
            exclude_selectors = [
                'script', 'style', 'noscript', 'iframe',
                '.ad-area-wrap', '[id^=div-gpt-ad]', 'ins'
            ]
            
            for selector in exclude_selectors:
                elements = content_container.select(selector)
                for el in elements:
                    el.decompose()
            
            # figureëŠ” ì´ë¯¸ì§€ë§Œ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ëŠ” ë³´ì¡´
            figures = content_container.find_all('figure')
            for figure in figures:
                # figure ë‚´ë¶€ì˜ imgì™€ figcaptionë§Œ ì œê±°
                for img in figure.find_all('img'):
                    img.decompose()
                for caption in figure.find_all('figcaption'):
                    caption.decompose()
            
            # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
            for br in content_container.find_all('br'):
                br.replace_with('\n')
            
            # lead_image ì¶”ì¶œ (ì²« figure)
            lead_image = self._extract_lead_image(content_container)
            
            # paragraphs ìˆ˜ì§‘
            paragraphs = []
            
            # ë¨¼ì € <p> íƒœê·¸ì—ì„œ ìˆ˜ì§‘
            for element in content_container.find_all('p'):
                text = element.get_text(strip=True)
                if text and len(text) > 10:
                    # ê³µë°± ì •ë¦¬
                    text = self._clean_text(text)
                    if text:
                        paragraphs.append(text)
            
            # <p> íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì§ì ‘ í…ìŠ¤íŠ¸ ë…¸ë“œì—ì„œ ìˆ˜ì§‘
            if not paragraphs:
                # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                full_text = content_container.get_text()
                # ì—°ì† ì¤„ë°”ê¿ˆ(2ê°œ ì´ìƒ)ì„ ë‹¨ë½ ê²½ê³„ë¡œ ê°„ì£¼
                text_blocks = re.split(r'\n\s*\n', full_text)
                for block in text_blocks:
                    text = block.strip()
                    if text and len(text) > 10:
                        # ê³µë°± ì •ë¦¬
                        text = self._clean_text(text)
                        if text:
                            paragraphs.append(text)
            
            # ë§ˆì§€ë§‰ ë¬¸ë‹¨ì´ ê¸°ì ë˜ëŠ” ì´ë©”ì¼(@)ì„ í¬í•¨í•˜ë©´ bylineìœ¼ë¡œ ë¶„ë¦¬
            byline = {"author": "", "email": ""}
            if paragraphs:
                last_paragraph = paragraphs[-1]
                if ('ê¸°ì' in last_paragraph and len(last_paragraph) < 50) or '@' in last_paragraph:
                    # ê¸°ì ì •ë³´ ì¶”ì¶œ
                    if 'ê¸°ì' in last_paragraph:
                        author_text = last_paragraph.replace('ê¸°ì', '').strip()
                        byline["author"] = author_text
                    # ì´ë©”ì¼ ì¶”ì¶œ
                    email_match = re.search(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', last_paragraph)
                    if email_match:
                        byline["email"] = email_match.group(1)
                    
                    paragraphs.pop()
            
            # í…ìŠ¤íŠ¸ ê²°í•©
            combined_text = '\n\n'.join(paragraphs)
            
            # ê³µë°± ì •ë¦¬
            combined_text = re.sub(r'\n\s*\n', '\n\n', combined_text).strip()
            
            return {
                "text": combined_text,
                "byline": byline,
                "lead_image": lead_image
            }
            
        except Exception as e:
            console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {"text": "", "byline": {}, "lead_image": None}

    def _extract_lead_image(self, container) -> Optional[Dict[str, str]]:
        """ì²« figureì—ì„œ lead_image ì¶”ì¶œ"""
        try:
            first_figure = container.find('figure')
            if not first_figure:
                return None
            
            lead_image = {"src": "", "alt": "", "caption": ""}
            
            # img íƒœê·¸ì—ì„œ src, alt ì¶”ì¶œ
            img = first_figure.find('img')
            if img:
                lead_image["src"] = img.get('src', '')
                lead_image["alt"] = img.get('alt', '')
                
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                if lead_image["src"] and not lead_image["src"].startswith('http'):
                    lead_image["src"] = urljoin(self.base_url, lead_image["src"])
            
            # caption ì¶”ì¶œ (figcaption)
            caption = first_figure.find('figcaption')
            if caption:
                lead_image["caption"] = caption.get_text(strip=True)
            
            return lead_image if lead_image["src"] else None
                
        except Exception as e:
            console.print(f"âš ï¸ lead_image ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None

    async def save_articles_batch(self):
        """DB ë°°ì¹˜ ì €ì¥"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ë°°ì¹˜ ì €ì¥ ì¤‘...")

        try:
            # ì–¸ë¡ ì‚¬ í™•ì¸
            media = self.supabase_manager.get_media_outlet(self.media_name)
            if not media:
                media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
            else:
                media_id = media["id"]

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            # ì¤‘ë³µ ì œê±° ë° ë°°ì¹˜ ì¤€ë¹„
            new_articles = []
            skip_count = 0
            short_content_count = 0
            
            for article in self.articles:
                if article["url"] in existing_urls:
                    skip_count += 1
                    continue
                
                # ë³¸ë¬¸ ê¸¸ì´ ì²´í¬ (20ì ë¯¸ë§Œ ì œì™¸)
                content = article.get('content', '')
                if len(content.strip()) < 20:
                    short_content_count += 1
                    continue
                    
                # ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
                parsed_article = self._parse_article_data_simple(article, media_id)
                if parsed_article:
                    new_articles.append(parsed_article)

            # ë°°ì¹˜ ì €ì¥
            if new_articles:
                success_count = self._batch_insert_articles(new_articles)
                console.print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
            else:
                console.print("âš ï¸ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {len(new_articles)}, ìŠ¤í‚µ {skip_count}, ì§§ì€ë³¸ë¬¸ ì œì™¸ {short_content_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

    def _parse_article_data_simple(self, article: dict, media_id: str) -> Optional[dict]:
        """ê¸°ì‚¬ ë°ì´í„° ê°„ë‹¨ íŒŒì‹± (ë°°ì¹˜ ì €ì¥ìš©)"""
        try:
            return {
                'title': article['title'],
                'url': article['url'],
                'content': article.get('content', ''),
                'published_at': article.get('published_at', datetime.now(pytz.UTC).isoformat()),
                'created_at': datetime.now(pytz.UTC).isoformat(),
                'media_id': media_id
            }
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def _batch_insert_articles(self, articles: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ê¸°ì‚¬ ì‚½ì…"""
        try:
            # Supabaseì˜ upsert ê¸°ëŠ¥ ì‚¬ìš©
            result = self.supabase_manager.client.table("articles").upsert(articles).execute()
            return len(result.data) if result.data else 0
        except Exception as e:
            console.print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
            success_count = 0
            for article in articles:
                try:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                except:
                    continue
            return success_count

    async def run(self, num_pages: int = 8):
        """ì‹¤í–‰"""
        try:
            console.print(f"ğŸš€ í•œêµ­ê²½ì œ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {num_pages}í˜ì´ì§€)")
            
            # 1. ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_articles_parallel(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_contents_parallel()
            
            # 3. ê¸°ì‚¬ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
            await self.save_articles_batch()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def main():
    collector = HankyungPoliticsCollector()
    await collector.run(num_pages=4)  # 4í˜ì´ì§€ì—ì„œ ê°ê° 40ê°œì”© ì´ 160ê°œ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())
