#!/usr/bin/env python3
"""
ì˜¤ë§ˆì´ë‰´ìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
ê°œì„ ì‚¬í•­:
- ë™ì‹œì„± ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 
- ë°°ì¹˜ DB ì €ì¥ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
- ë”œë ˆì´ ìµœì í™”
- ì—°ê²° í’€ ìµœì í™”
"""

import asyncio
import sys
import os
from datetime import datetime
import pytz
import httpx
from bs4 import BeautifulSoup
from rich.console import Console
import time
import re
from typing import List, Dict, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ utils ë¶ˆëŸ¬ì˜¤ê¸°
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.supabase_manager import SupabaseManager

console = Console()

class OhmyNewsPoliticsCollector:
    def __init__(self):
        self.media_name = "ì˜¤ë§ˆì´ë‰´ìŠ¤"
        self.base_url = "https://www.ohmynews.com"
        self.list_url = "https://www.ohmynews.com/NWS_Web/Articlepage/Total_Article.aspx?PAGE_CD=C0400&pageno={}"
        self.supabase_manager = SupabaseManager()
        self.articles = []
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ìµœì í™”)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # ë™ì‹œì„± ì œí•œ ì„¤ì •
        self.semaphore = asyncio.Semaphore(10)  # ìµœëŒ€ 10ê°œ ë™ì‹œ ìš”ì²­
        self.batch_size = 20  # DB ë°°ì¹˜ ì €ì¥ í¬ê¸°

    async def run(self, num_pages=8):
        console.print(f"ğŸš€ {self.media_name} ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœì í™” ë²„ì „)")

        # 1ë‹¨ê³„: ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
        await self.collect_articles_parallel(num_pages)
        
        # 2ë‹¨ê³„: ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
        await self.collect_contents_parallel()
        
        # 3ë‹¨ê³„: ë°°ì¹˜ ì €ì¥
        await self.save_articles_batch()

        console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")

    async def collect_articles_parallel(self, num_pages):
        """ëª©ë¡ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“„ {num_pages} í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        tasks = [self._collect_page_articles(page_num) for page_num in range(1, num_pages + 1)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_articles = 0
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(result)}")
            else:
                total_articles += result
                
        console.print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def _collect_page_articles(self, page_num: int) -> int:
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        url = self.list_url.format(page_num)
        console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: {url}")

        async with self.semaphore:  # ë™ì‹œì„± ì œí•œ
            try:
                soup = await self._fetch_soup(url)
                if not soup:
                    console.print(f"âš ï¸ í˜ì´ì§€ {page_num} ë¡œë“œ ì‹¤íŒ¨")
                    return 0

                # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœëŒ€ 20ê°œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
                news_list = soup.select(".news_list")
                if not news_list:
                    console.print(f"âš ï¸ í˜ì´ì§€ {page_num}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return 0

                page_articles = 0
                for news_item in news_list[:20]:  # ìµœëŒ€ 20ê°œ
                    link = news_item.select_one("dt a")
                    if not link:
                        continue

                    title = link.get_text(strip=True)
                    href = link.get("href")

                    if href.startswith("/"):
                        href = f"{self.base_url}{href}"

                    article = {
                        "title": title,
                        "url": href,
                        "published_at": "",
                        "content": "",
                    }
                    self.articles.append(article)
                    page_articles += 1
                    console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")

                console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {page_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                return page_articles

            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return 0

    async def collect_contents_parallel(self):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ + ë°œí–‰ì‹œê°„ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“– ìƒì„¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ ({len(self.articles)}ê°œ) - ë³‘ë ¬ ì²˜ë¦¬")

        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì„œ)
        batch_size = 20  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(self.articles))
            batch_articles = self.articles[start_idx:end_idx]
            
            console.print(f"ğŸ“– ë°°ì¹˜ {batch_num + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [self._collect_single_article(i + start_idx, article) for i, article in enumerate(batch_articles)]
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if batch_num < total_batches - 1:
                await asyncio.sleep(0.5)

    async def _collect_single_article(self, index: int, article: Dict):
        """ë‹¨ì¼ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘"""
        console.print(f"ğŸ“– [{index + 1}/{len(self.articles)}] {article['title'][:40]}...")
        
        async with self.semaphore:  # ë™ì‹œì„± ì œí•œ
            try:
                data = await self._get_article_content(article["url"])
                article["published_at"] = data.get("published_at", "")
                article["content"] = data.get("content", "")
                
            except Exception as e:
                console.print(f"âŒ [{index + 1}] ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    async def _fetch_soup(self, url: str, max_retries=2) -> Optional[BeautifulSoup]:
        """URLì—ì„œ BeautifulSoup ê°ì²´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í—¬í¼ ë©”ì„œë“œ (ìµœì í™”)"""
        for attempt in range(max_retries):
            try:
                async with httpx.AsyncClient(
                    timeout=15.0,  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)  # ì—°ê²° í’€ ì¦ê°€
                ) as client:
                    response = await client.get(url, headers=self.headers)
                    response.raise_for_status()
                    return BeautifulSoup(response.text, "html.parser")
                    
            except (httpx.HTTPStatusError, httpx.TimeoutException, httpx.ConnectError) as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(0.5)  # ì¬ì‹œë„ ë”œë ˆì´ ë‹¨ì¶•
                else:
                    console.print(f"âŒ {url} ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    return None
            except Exception as e:
                console.print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
                return None

    async def _get_article_content(self, url: str):
        """ë³¸ë¬¸ê³¼ ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        soup = await self._fetch_soup(url)
        if not soup:
            return {"published_at": "", "content": ""}

        result = {"published_at": "", "content": ""}

        try:
            # 1. ë°œí–‰ ì‹œê°„ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            result["published_at"] = self._extract_publish_date(soup)

            # 2. ë³¸ë¬¸ ì¶”ì¶œ
            result["content"] = self._extract_content(soup)
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # if not result["content"]:
            #     console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url}")
            #     console.print(f"HTML êµ¬ì¡° í™•ì¸ í•„ìš”")

        except Exception as e:
            console.print(f"âš ï¸ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        return result

    def _extract_publish_date(self, soup: BeautifulSoup) -> str:
        """ë°œí–‰ ë‚ ì§œ ì¶”ì¶œ"""
        date_selectors = [
            "span.date",
            ".article_date",
            ".date_time",
            ".publish_time"
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                time_str = date_elem.get_text(strip=True)
                # "ìµœì¢… ì—…ë°ì´íŠ¸", "ê¸°ì‚¬ì…ë ¥" ë“±ì˜ í…ìŠ¤íŠ¸ ì œê±°
                time_str = re.sub(r'(ìµœì¢…\s*ì—…ë°ì´íŠ¸|ê¸°ì‚¬ì…ë ¥|ìˆ˜ì •)', '', time_str).strip()
                
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹± ì‹œë„
                date_formats = [
                    "%y.%m.%d %H:%M",
                    "%Y.%m.%d %H:%M",
                    "%Y-%m-%d %H:%M",
                    "%m/%d %H:%M",
                ]
                
                for fmt in date_formats:
                    try:
                        dt = datetime.strptime(time_str, fmt)
                        # ì—°ë„ê°€ ì—†ëŠ” ê²½ìš° í˜„ì¬ ì—°ë„ ì‚¬ìš©
                        if dt.year == 1900:
                            dt = dt.replace(year=datetime.now().year)
                            
                        kst = pytz.timezone("Asia/Seoul")
                        dt = kst.localize(dt)
                        return dt.astimezone(pytz.UTC).isoformat()
                    except ValueError:
                        continue
                        
                console.print(f"âš ï¸ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {time_str}")
                break
                
        return ""

    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ì¶”ì¶œ"""
        # ì˜¤ë§ˆì´ë‰´ìŠ¤ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        content_div = soup.select_one('div.at_contents[itemprop="articleBody"]')
        
        if not content_div:
            return ""
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in content_div.find_all(['figure', 'script', 'style', 'iframe', 'button', 'img', 'video', 'audio']):
            tag.decompose()
        
        # ê´‘ê³ ì„± div ì œê±°
        for tag in content_div.find_all('div', class_=lambda x: x and ('ad' in x.lower() or 'advertisement' in x.lower())):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        text = content_div.get_text(separator="\n", strip=True)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
        
        return text


    async def save_articles_batch(self):
        """DB ë°°ì¹˜ ì €ì¥ (ìµœì í™”)"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ë°°ì¹˜ ì €ì¥ ì¤‘...")

        try:
            media_outlet = self.supabase_manager.get_media_outlet(self.media_name)
            if media_outlet:
                media_id = media_outlet["id"]
            else:
                media_id = self.supabase_manager.create_media_outlet(self.media_name)

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            # ì¤‘ë³µ ì œê±° ë° ë°°ì¹˜ ì¤€ë¹„
            new_articles = []
            skip_count = 0
            
            for article in self.articles:
                if article["url"] in existing_urls:
                    skip_count += 1
                    continue
                    
                article_data = {
                    "title": article["title"],
                    "url": article["url"],
                    "content": article["content"],
                    "published_at": article["published_at"] or datetime.now(pytz.UTC).isoformat(),
                    "created_at": datetime.now(pytz.UTC).isoformat(),
                    "media_id": media_id,
                }
                new_articles.append(article_data)

            # ë°°ì¹˜ ì €ì¥
            if new_articles:
                success_count = self._batch_insert_articles(new_articles)
                console.print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
            else:
                console.print("âš ï¸ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {len(new_articles)}, ìŠ¤í‚µ {skip_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

    def _batch_insert_articles(self, articles: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ê¸°ì‚¬ ì‚½ì…"""
        try:
            # Supabaseì˜ upsert ê¸°ëŠ¥ ì‚¬ìš©
            result = self.supabase_manager.client.table("articles").upsert(articles).execute()
            return len(result.data) if result.data else 0
        except Exception as e:
            console.print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
            success_count = 0
            for article in articles:
                try:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                except:
                    continue
            return success_count

async def main():
    collector = OhmyNewsPoliticsCollector()
    await collector.run(num_pages=8)  # 8í˜ì´ì§€ì—ì„œ ê°ê° 20ê°œì”© ì´ 160ê°œ ìˆ˜ì§‘ (150ê°œ ëª©í‘œ)

if __name__ == "__main__":
    asyncio.run(main())
