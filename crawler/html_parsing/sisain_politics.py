#!/usr/bin/env python3
"""
ì‹œì‚¬IN ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
ë¬´í•œìŠ¤í¬ë¡¤ ë°©ì‹ì˜ APIë¥¼ í†µí•œ ê¸°ì‚¬ ìˆ˜ì§‘
"""

import asyncio
import httpx
import re
import html
import sys
import os
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from rich.console import Console

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils.supabase_manager import SupabaseManager

console = Console()


class SisainPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.sisain.co.kr"
        self.politics_url = "https://www.sisain.co.kr/news/articleList.html?sc_section_code=S1N6&view_type=sm"
        self.api_url = "https://www.sisain.co.kr/news/articleList.html"
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        self.supabase = SupabaseManager()
        self.media_outlet = None
        
    def initialize(self):
        """ë¯¸ë””ì–´ ì•„ì›ƒë › ì •ë³´ ì´ˆê¸°í™”"""
        try:
            # ì‹œì‚¬IN ë¯¸ë””ì–´ ì•„ì›ƒë › ID ì§ì ‘ ì„¤ì •
            self.media_outlet = {
                "id": "193bfb31-bd5b-49a1-ad19-e068070e5794",
                "name": "sisain"
            }
            
            console.print("âœ… ì‹œì‚¬IN ë¯¸ë””ì–´ ì•„ì›ƒë › ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            console.print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _get_api_params(self, page: int) -> Dict[str, str]:
        """API íŒŒë¼ë¯¸í„° ìƒì„±"""
        return {
            "sc_section_code": "S1N6",
            "view_type": "sm",
            "total": "5208",
            "list_per_page": "20",
            "page_per_page": "10",
            "page": str(page),
            "box_idxno": "0"
        }
    
    async def _get_page_articles(self, client: httpx.AsyncClient, page: int) -> List[Dict[str, Any]]:
        """íŠ¹ì • í˜ì´ì§€ì˜ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        try:
            params = self._get_api_params(page)
            
            response = await client.get(
                self.api_url,
                params=params,
                headers=self.headers,
                timeout=30.0
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            articles = []
            
            # ë””ë²„ê¹…: ì‘ë‹µ ë‚´ìš© í™•ì¸
            console.print(f"ğŸ” í˜ì´ì§€ {page} ì‘ë‹µ ê¸¸ì´: {len(response.text)}")
            
            # ê¸°ì‚¬ ëª©ë¡ íŒŒì‹±
            article_items = soup.select("ul.type li.items")
            console.print(f"ğŸ” í˜ì´ì§€ {page} ê¸°ì‚¬ ì•„ì´í…œ ìˆ˜: {len(article_items)}")
            
            for item in article_items:
                try:
                    # ì œëª©ê³¼ ë§í¬
                    title_link = item.select_one("div.view-cont h2.titles a")
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    relative_url = title_link.get("href", "")
                    if not relative_url:
                        continue
                    
                    article_url = urljoin(self.base_url, relative_url)
                    
                    # ìš”ì•½
                    lead = item.select_one("p.lead.line-x2 a")
                    description = lead.get_text(strip=True) if lead else ""
                    
                    # ë‚ ì§œ
                    date_elem = item.select_one("em.replace-date")
                    published_date = ""
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                        # YYYY.MM.DD HH:MM í˜•ì‹ì„ íŒŒì‹±
                        date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})\s+(\d{2}):(\d{2})', date_text)
                        if date_match:
                            year, month, day, hour, minute = date_match.groups()
                            published_date = f"{year}-{month}-{day}"
                    
                    # ì¸ë„¤ì¼
                    thumb_img = item.select_one("a.thumb img")
                    image_url = ""
                    image_alt = ""
                    if thumb_img:
                        image_url = thumb_img.get("src", "")
                        image_alt = thumb_img.get("alt", "")
                        if image_url and not image_url.startswith("http"):
                            image_url = urljoin(self.base_url, image_url)
                    
                    # article_id ì¶”ì¶œ (URLì—ì„œ)
                    article_id = ""
                    url_match = re.search(r'/(\d+)/?$', article_url)
                    if url_match:
                        article_id = url_match.group(1)
                    
                    article = {
                        "source": "sisain",
                        "article_id": article_id,
                        "url": article_url,
                        "title": title,
                        "description": description,
                        "published_date": published_date,
                        "image_url": image_url if image_url else None,
                        "image_alt": image_alt,
                        "byline": None,
                        "content": "",
                        "published_at": None,
                        "lead_image": None
                    }
                    
                    articles.append(article)
                    
                except Exception as e:
                    console.print(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    continue
            
            return articles
            
        except Exception as e:
            console.print(f"âŒ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _collect_page_articles_parallel(self, client: httpx.AsyncClient, num_pages: int) -> List[Dict[str, Any]]:
        """ë³‘ë ¬ë¡œ ì—¬ëŸ¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ìˆ˜ì§‘"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        tasks = []
        for page in range(1, num_pages + 1):
            task = self._get_page_articles(client, page)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_articles = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i+1} ì‹¤íŒ¨: {str(result)}")
            else:
                all_articles.extend(result)
                console.print(f"ğŸ“° ë°œê²¬: {len(result)}ê°œ ê¸°ì‚¬...")
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article["url"] not in seen_urls:
                seen_urls.add(article["url"])
                unique_articles.append(article)
        
        console.print(f"ğŸ“Š ì´ {len(unique_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        return unique_articles
    
    def _extract_content_text(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ì‹œì‚¬IN ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            content_container = soup.select_one('article#article-view-content-div.article-veiw-body[itemprop="articleBody"]')
            if not content_container:
                content_container = soup.select_one('article.article-veiw-body[itemprop="articleBody"]')
            
            if not content_container:
                console.print("âš ï¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {"paragraphs": [], "text": "", "headings": [], "images": [], "links": [], "appendix": None}
            
            # ê´‘ê³ /ì¥ì‹ ìš”ì†Œ ì œê±°
            exclude_selectors = [
                'script', 'style', 'noscript', 'iframe', 
                '.ad-template', 'ins.adsbygoogle', '[id^="AD"]', 
                '.IMGFLOATING', '[style*="display:none"]'
            ]
            
            for selector in exclude_selectors:
                elements = content_container.select(selector)
                for el in elements:
                    el.decompose()
            
            # ë˜í¼ div í’€ê¸°
            wrapper_divs = content_container.select('div[style*="text-align:center"]')
            for wrapper in wrapper_divs:
                # ìì‹ ìš”ì†Œë“¤ì„ ë¶€ëª¨ë¡œ ì´ë™
                parent = wrapper.parent
                if parent:
                    for child in list(wrapper.children):
                        parent.insert(wrapper.index(child), child)
                    wrapper.decompose()
            
            # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
            for br in content_container.find_all('br'):
                br.replace_with('\n')
            
            # ê³µë°± ì •ë¦¬ í•¨ìˆ˜
            def clean_text(text: str) -> str:
                text = text.replace('&nbsp;', ' ')
                text = re.sub(r'\s+', ' ', text)
                return text.strip()
            
            # ì œëª© ìˆ˜ì§‘
            headings = []
            for i in range(1, 7):
                for h in content_container.find_all(f'h{i}'):
                    heading_text = h.get_text(strip=True)
                    if heading_text:
                        headings.append(f"h{i} {heading_text}")
            
            # ì´ë¯¸ì§€ ìˆ˜ì§‘
            images = []
            lead_image = None
            figures = content_container.find_all('figure.photo-layout')
            
            for i, figure in enumerate(figures):
                img = figure.find('img')
                if img:
                    src = img.get('src', '')
                    alt = img.get('alt', '')
                    if src and not src.startswith('http'):
                        src = urljoin(self.base_url, src)
                    
                    figcaption = figure.find('figcaption')
                    caption = figcaption.get_text(strip=True) if figcaption else ""
                    
                    idxno = figure.get('data-idxno')
                    
                    image_data = {
                        "src": src,
                        "alt": alt,
                        "caption": caption,
                        "idxno": idxno
                    }
                    
                    images.append(image_data)
                    
                    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ lead_imageë¡œ ì„¤ì •
                    if i == 0:
                        lead_image = image_data
            
            # ë§í¬ ìˆ˜ì§‘
            links = []
            seen_hrefs = set()
            
            # ë‹¨ë½ ìˆ˜ì§‘
            paragraphs = []
            
            for p in content_container.find_all('p'):
                # ì•µì»¤ íƒœê·¸ ì²˜ë¦¬
                for a in p.find_all('a'):
                    href = a.get('href', '')
                    text = a.get_text(strip=True)
                    
                    if href and href not in seen_hrefs:
                        seen_hrefs.add(href)
                        if not href.startswith('http'):
                            href = urljoin(self.base_url, href)
                        links.append({"text": text, "href": href})
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ê³  hrefëŠ” ì œê±°
                    a.replace_with(text)
                
                text = p.get_text(strip=True)
                if text and len(text) > 10:  # ì˜ë¯¸ ìˆëŠ” ë‹¨ë½ë§Œ
                    text = clean_text(text)
                    if text:
                        paragraphs.append(text)
            
            # appendix ì²˜ë¦¬ ("â–  ì´ë ‡ê²Œ ì¡°ì‚¬í–ˆë‹¤" ë¸”ë¡)
            appendix = None
            for p in content_container.find_all('p'):
                span = p.find('span', style=lambda x: x and 'color:#2980b9' in x)
                if span and "â–  ì´ë ‡ê²Œ ì¡°ì‚¬í–ˆë‹¤" in span.get_text():
                    # <br> ê¸°ì¤€ìœ¼ë¡œ ì¤„ë°”ê¿ˆ ìœ ì§€
                    methodology_text = span.get_text(separator='\n')
                    appendix = {"methodology": methodology_text}
                    break
            
            # í…ìŠ¤íŠ¸ ê²°í•©
            combined_text = '\n\n'.join(paragraphs)
            
            return {
                "paragraphs": paragraphs,
                "text": combined_text,
                "headings": headings if headings else None,
                "images": images if images else None,
                "lead_image": lead_image,
                "links": links if links else None,
                "appendix": appendix
            }
            
        except Exception as e:
            console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {"paragraphs": [], "text": "", "headings": None, "images": None, "lead_image": None, "links": None, "appendix": None}
    
    def _should_skip_article(self, article: Dict[str, Any]) -> bool:
        """ê¸°ì‚¬ í•„í„°ë§ ì¡°ê±´ í™•ì¸"""
        title = article.get("title", "")
        content = article.get("content", "")
        
        # ì¡°ê±´ 1: ë³¸ë¬¸ì´ "â–  ë°©ì†¡"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
        if content.startswith("â–  ë°©ì†¡"):
            return True
            
        # ì¡°ê±´ 2: ë³¸ë¬¸ì´ "ã€ˆì‹œì‚¬INã€‰ì€"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°  
        if content.startswith("ã€ˆì‹œì‚¬INã€‰ì€"):
            return True
            
        # ì¡°ê±´ 3: íƒ€ì´í‹€ì´ "[ê¹€ì€ì§€ì˜ ë‰´ìŠ¤IN]"ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°
        if title.endswith("[ê¹€ì€ì§€ì˜ ë‰´ìŠ¤IN]"):
            return True
            
        return False
    
    async def _extract_content_httpx(self, client: httpx.AsyncClient, article: Dict[str, Any], index: int):
        """httpxë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
            
            response = await client.get(article["url"], headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_data = self._extract_content_text(soup)
            article["content"] = content_data.get("text", "")
            
            # ë°”ì´ë¼ì¸ì€ ë³„ë„ë¡œ ì¶”ì¶œí•˜ì§€ ì•ŠìŒ (ë³¸ë¬¸ì— í¬í•¨)
            article["byline"] = None
            
            # lead_image ì„¤ì •
            lead_image = content_data.get("lead_image")
            if lead_image:
                article["lead_image"] = lead_image
            
            # í•„í„°ë§ ì¡°ê±´ í™•ì¸
            if self._should_skip_article(article):
                console.print(f"â­ï¸ [{index}] ìŠ¤í‚µ: í•„í„°ë§ ì¡°ê±´ì— í•´ë‹¹")
                article["content"] = ""  # ìŠ¤í‚µëœ ê¸°ì‚¬ë¡œ í‘œì‹œ
                return
            
            console.print(f"âœ… [{index}] ì™„ë£Œ: {len(article['content'])}ì")
            
        except Exception as e:
            console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
            article["content"] = ""
            article["byline"] = None
            article["lead_image"] = None
    
    async def _extract_all_contents(self, articles: List[Dict[str, Any]]) -> None:
        """ëª¨ë“  ê¸°ì‚¬ì˜ ë³¸ë¬¸ ì¶”ì¶œ"""
        console.print(f"ğŸ“– {len(articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        batch_size = 20
        total_batches = (len(articles) + batch_size - 1) // batch_size
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(articles))
                batch_articles = articles[start_idx:end_idx]
                
                console.print(f"ğŸ“– ë°°ì¹˜ {batch_idx + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
                
                tasks = []
                for i, article in enumerate(batch_articles):
                    task = self._extract_content_httpx(client, article, start_idx + i + 1)
                    tasks.append(task)
                
                await asyncio.gather(*tasks, return_exceptions=True)
                
                # ë°°ì¹˜ ê°„ ì ì‹œ ëŒ€ê¸°
                if batch_idx < total_batches - 1:
                    await asyncio.sleep(1)
    
    async def run(self, num_pages: int = 8):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            if not self.initialize():
                return
            
            console.print(f"ğŸš€ ì‹œì‚¬IN ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {num_pages}í˜ì´ì§€)")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                # ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
                articles = await self._collect_page_articles_parallel(client, num_pages)
                
                if not articles:
                    console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                await self._extract_all_contents(articles)
                
                # Supabaseì— ì €ì¥
                console.print("ğŸ’¾ Supabaseì— ê¸°ì‚¬ ì €ì¥ ì¤‘...")
                
                saved_count = 0
                skipped_count = 0
                
                for article in articles:
                    try:
                        # í•„í„°ë§ëœ ê¸°ì‚¬ëŠ” ê±´ë„ˆë›°ê¸°
                        if not article.get("content") or article["content"] == "":
                            skipped_count += 1
                            continue
                        
                        # published_at ì„¤ì • (KST ê¸°ì¤€)
                        if article["published_date"]:
                            try:
                                date_obj = datetime.strptime(article["published_date"], "%Y-%m-%d")
                                kst_time = date_obj.replace(tzinfo=timezone(timedelta(hours=9)))
                                article["published_at"] = kst_time.isoformat()
                            except ValueError:
                                article["published_at"] = None
                        else:
                            article["published_at"] = None
                        
                        # Supabaseì— ì €ì¥
                        article_data = {
                            "media_id": self.media_outlet["id"],
                            "url": article["url"],
                            "title": article["title"],
                            "content": article["content"],
                            "published_at": article["published_at"],
                            "created_at": datetime.now(timezone.utc).isoformat()
                        }
                        
                        result = self.supabase.insert_article(article_data)
                        
                        if result:
                            saved_count += 1
                        else:
                            skipped_count += 1
                            
                    except Exception as e:
                        console.print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {str(e)[:50]}...")
                        skipped_count += 1
                
                console.print(f"ğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {saved_count}, ìŠ¤í‚µ {skipped_count}")
                console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
                
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def main():
    collector = SisainPoliticsCollector()
    await collector.run(num_pages=15)  # 15í˜ì´ì§€ì—ì„œ ê°ê° 20ê°œì”© ì´ 300ê°œ ìˆ˜ì§‘ (í•„í„°ë§ ê³ ë ¤)

if __name__ == "__main__":
    asyncio.run(main())
