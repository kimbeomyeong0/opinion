#!/usr/bin/env python3
"""
ë‚´ì¼ì‹ ë¬¸ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (HTML íŒŒì‹± ê¸°ë°˜)
- ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ ìˆ˜ì§‘
- ë³¸ë¬¸ì€ httpxë¡œ ë³„ë„ ìˆ˜ì§‘
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import httpx
import pytz
from bs4 import BeautifulSoup
import re
from rich.console import Console

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")


class NaeilPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.naeil.com"
        self.media_name = "ë‚´ì¼ì‹ ë¬¸"
        self.media_bias = "left"  # ì§„ë³´ ì„±í–¥
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # ë™ì‹œì„± ì œí•œ ì„¤ì •
        self.semaphore = asyncio.Semaphore(10)

    def _get_page_urls(self, num_pages: int = 8) -> List[str]:
        """í˜ì´ì§€ URL ëª©ë¡ ìƒì„± (page=1, 2, 3...)"""
        urls = []
        for page in range(1, num_pages + 1):
            url = f"{self.base_url}/politics?page={page}"
            urls.append(url)
        return urls

    async def _get_page_articles(self, page_url: str, page_num: int) -> List[Dict]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: HTML íŒŒì‹± ì¤‘...")

        async with self.semaphore:
            try:
                async with httpx.AsyncClient(
                    timeout=15.0,
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
                ) as client:
                    response = await client.get(page_url, headers=self.headers)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    articles = []
                    
                    # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ (.sub-news-list-wrap ul.story-list li.card.card-box)
                    story_list = soup.select('.sub-news-list-wrap ul.story-list li.card.card-box')
                    
                    for card in story_list:
                        try:
                            # ì œëª©ê³¼ URL ì¶”ì¶œ (.card-text .headline a)
                            headline_link = card.select_one('.card-text .headline a')
                            if not headline_link:
                                continue
                                
                            href = headline_link.get('href')
                            if not href:
                                continue
                            
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if href.startswith('http'):
                                full_url = href
                            else:
                                full_url = urljoin(self.base_url, href)
                            
                            title = headline_link.get_text(strip=True)
                            
                            # ìš”ì•½ ì¶”ì¶œ (.card-text .description a)
                            description = ""
                            desc_element = card.select_one('.card-text .description a')
                            if desc_element:
                                description = desc_element.get_text(strip=True)
                            
                            # ë‚ ì§œ ì¶”ì¶œ (.card-body .meta .yearì™€ .card-body .meta .date)
                            published_at = self._extract_date(card)
                            
                            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ (.card-image img)
                            image_url, image_alt = self._extract_image_info(card)
                            
                            if title and len(title) > 10:
                                article = {
                                    'title': title,
                                    'url': full_url,
                                    'content': '',
                                    'published_at': published_at,
                                    'description': description,
                                    'image_url': image_url,
                                    'image_alt': image_alt
                                }
                                articles.append(article)
                                console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")
                        
                        except Exception as e:
                            console.print(f"âš ï¸ ê¸°ì‚¬ ì¹´ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            continue
                    
                    console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    return articles
                    
            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return []

    def _extract_date(self, card) -> str:
        """ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ì„ì‹œ - ë³¸ë¬¸ì—ì„œ ì •í™•í•œ ì‹œê°„ ì¶”ì¶œ)"""
        try:
            year_element = card.select_one('.card-body .meta .year')
            date_element = card.select_one('.card-body .meta .date')
            
            if year_element and date_element:
                year = year_element.get_text(strip=True)
                date_str = date_element.get_text(strip=True)
                
                # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: 2025, 09.15 -> 2025-09-15)
                if year and date_str and '.' in date_str:
                    month, day = date_str.split('.')
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return datetime.now(pytz.UTC).isoformat()
                
        except Exception as e:
            console.print(f"âš ï¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return datetime.now(pytz.UTC).isoformat()

    def _extract_image_info(self, card) -> tuple:
        """ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ (.card-image img)"""
        try:
            img_element = card.select_one('.card-image img')
            if not img_element:
                return None, None
            
            # data-srcê°€ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ src ì‚¬ìš©
            image_url = img_element.get('data-src') or img_element.get('src')
            image_alt = img_element.get('alt', '')
            
            # https://static.naeil.com/img/1X1.pngì€ null ì²˜ë¦¬
            if image_url and '1X1.png' in image_url:
                return None, None
            
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            if image_url and not image_url.startswith('http'):
                image_url = urljoin(self.base_url, image_url)
            
            return image_url, image_alt
                
        except Exception as e:
            console.print(f"âš ï¸ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None, None

    async def collect_articles_parallel(self, num_pages: int = 8):
        """ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        page_urls = self._get_page_urls(num_pages)
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        tasks = [self._get_page_articles(url, i + 1) for i, url in enumerate(page_urls)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_articles = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(result)}")
            else:
                self.articles.extend(result)
                total_articles += len(result)
                
        console.print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def collect_contents_parallel(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        if not self.articles:
            return
            
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì„œ)
        batch_size = 20
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(self.articles))
            batch_articles = self.articles[start_idx:end_idx]
            
            console.print(f"ğŸ“– ë°°ì¹˜ {batch_num + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            async with httpx.AsyncClient(
                timeout=15.0,
                follow_redirects=True,
                limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
            ) as client:
                tasks = [self._extract_content_httpx(client, article, i + start_idx + 1) for i, article in enumerate(batch_articles)]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ë”œë ˆì´
            if batch_num < total_batches - 1:
                await asyncio.sleep(0.5)

    async def _extract_content_httpx(self, client: httpx.AsyncClient, article: dict, index: int):
        """httpxë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
            
            response = await client.get(article["url"], headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë°œí–‰Â·ìˆ˜ì • ì‹œê° ì¶”ì¶œ
            date_data = self._extract_published_dates(soup)
            if date_data.get("published_at_utc"):
                article["published_at"] = date_data["published_at_utc"]
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_data = self._extract_content_text(soup)
            article["content"] = content_data.get("text", "")
            article["byline"] = content_data.get("byline", "")
            
            console.print(f"âœ… [{index}] ì™„ë£Œ: {len(article['content'])}ì")
            
        except Exception as e:
            console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
            article["content"] = ""
            article["byline"] = ""

    def _extract_content_text(self, soup: BeautifulSoup) -> Dict[str, str]:
        """ë‚´ì¼ì‹ ë¬¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            # 1ì°¨: .article-view.font-size03 ì°¾ê¸°
            content_container = soup.select_one('.article-view.font-size03')
            
            # 2ì°¨ í´ë°±: .article-view
            if not content_container:
                content_container = soup.select_one('.article-view')
            
            if not content_container:
                console.print("âš ï¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {"text": "", "byline": ""}
            
            # ì œì™¸í•  ìš”ì†Œë“¤ ì œê±°
            exclude_selectors = [
                'script', 'style', 'noscript', 'figure', 'figcaption', 
                'iframe', 'aside', '[class^=ad-]', '[data-svcad]'
            ]
            
            for selector in exclude_selectors:
                elements = content_container.select(selector)
                for el in elements:
                    el.decompose()
            
            # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
            for br in content_container.find_all('br'):
                br.replace_with('\n')
            
            # subtitles ìˆ˜ì§‘ (.article-subtitle.type01 > p)
            subtitles = []
            subtitle_elements = content_container.select('.article-subtitle.type01 > p')
            for element in subtitle_elements:
                text = element.get_text(strip=True)
                if text:
                    subtitles.append(text)
            
            # .article-subtitle ì œê±°
            subtitle_containers = content_container.select('.article-subtitle')
            for container in subtitle_containers:
                container.decompose()
            
            # paragraphs ìˆ˜ì§‘ (ì»¨í…Œì´ë„ˆ ì§ê³„ <p>)
            paragraphs = []
            direct_p_elements = content_container.find_all('p', recursive=False)
            for element in direct_p_elements:
                text = element.get_text(strip=True)
                if text and len(text) > 10:  # 10ì ì´ìƒì¸ ë¬¸ë‹¨ë§Œ
                    paragraphs.append(text)
            
            # &nbsp;ì™€ ì¤‘ë³µ ê³µë°±/ê°œí–‰ ì •ë¦¬
            for i, paragraph in enumerate(paragraphs):
                paragraphs[i] = re.sub(r'&nbsp;', ' ', paragraph)
                paragraphs[i] = re.sub(r'\s+', ' ', paragraphs[i]).strip()
            
            # ë§ˆì§€ë§‰ ë‹¨ë½ì´ â€¦ ê¸°ì ë˜ëŠ” ì´ë©”ì¼(@)ì„ í¬í•¨í•˜ë©´ bylineìœ¼ë¡œ ì´ë™
            byline = ""
            if paragraphs:
                last_paragraph = paragraphs[-1]
                if ('â€¦' in last_paragraph and 'ê¸°ì' in last_paragraph) or '@' in last_paragraph:
                    byline = last_paragraph
                    paragraphs.pop()
            
            # í…ìŠ¤íŠ¸ ê²°í•©
            if subtitles and paragraphs:
                combined_text = '\n\n'.join(subtitles + [''] + paragraphs)
            elif subtitles:
                combined_text = '\n\n'.join(subtitles)
            elif paragraphs:
                combined_text = '\n\n'.join(paragraphs)
            else:
                combined_text = ""
            
            # ê³µë°± ì •ê·œí™”
            combined_text = re.sub(r'\n\s*\n', '\n\n', combined_text).strip()
            
            return {
                "text": combined_text,
                "byline": byline
            }
            
        except Exception as e:
            console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {"text": "", "byline": ""}

    def _extract_published_dates(self, soup: BeautifulSoup) -> Dict[str, str]:
        """ë°œí–‰Â·ìˆ˜ì • ì‹œê° ì¶”ì¶œ"""
        try:
            result = {
                "published_at_kst": "",
                "published_at_utc": "",
                "updated_at_kst": "",
                "updated_at_utc": "",
                "raw_dates": []
            }
            
            # 1ì°¨: header.article-header .group .datetime .dateì—ì„œ ì°¾ê¸°
            datetime_elements = soup.select('header.article-header .group .datetime .date')
            
            for element in datetime_elements:
                text = element.get_text(strip=True)
                if not text:
                    continue
                
                result["raw_dates"].append(text)
                
                # YYYY-MM-DD HH:MM(:SS)? íŒ¨í„´ ì¶”ì¶œ
                date_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}(?::\d{2})?)', text)
                if not date_match:
                    continue
                
                date_str = date_match.group(1)
                
                # ì´ˆ ë‹¨ìœ„ê°€ ì—†ìœ¼ë©´ :00 ë³´ì™„
                if len(date_str.split(':')) == 2:
                    date_str += ':00'
                
                # ê¼¬ë¦¬í‘œ ì œê±°í•˜ê³  êµ¬ë¶„
                if any(keyword in text for keyword in ['ê²Œì¬', 'ì…ë ¥', 'ë“±ë¡']):
                    if not result["published_at_kst"]:
                        result["published_at_kst"] = f"{date_str}+09:00"
                        result["published_at_utc"] = self._convert_kst_to_utc(date_str)
                elif any(keyword in text for keyword in ['ìˆ˜ì •', 'ìµœì¢…ìˆ˜ì •']):
                    if not result["updated_at_kst"]:
                        result["updated_at_kst"] = f"{date_str}+09:00"
                        result["updated_at_utc"] = self._convert_kst_to_utc(date_str)
            
            # 2ì°¨ í´ë°±: ë©”íƒ€ íƒœê·¸ë“¤
            if not result["published_at_kst"]:
                fallback_dates = self._extract_meta_dates(soup)
                if fallback_dates.get("published_at_utc"):
                    result["published_at_kst"] = fallback_dates["published_at_kst"]
                    result["published_at_utc"] = fallback_dates["published_at_utc"]
            
            return result
            
        except Exception as e:
            console.print(f"âš ï¸ ë°œí–‰Â·ìˆ˜ì • ì‹œê° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {
                "published_at_kst": "",
                "published_at_utc": "",
                "updated_at_kst": "",
                "updated_at_utc": "",
                "raw_dates": []
            }

    def _extract_meta_dates(self, soup: BeautifulSoup) -> Dict[str, str]:
        """ë©”íƒ€ íƒœê·¸ì—ì„œ ë°œí–‰ ì‹œê° ì¶”ì¶œ (í´ë°±)"""
        try:
            # meta[property="article:published_time"]
            meta_published = soup.find('meta', property='article:published_time')
            if meta_published:
                content = meta_published.get('content', '')
                if content:
                    return self._parse_iso_date(content)
            
            # meta[name="pubdate"|"date"|"ptime"]
            for name in ['pubdate', 'date', 'ptime']:
                meta_element = soup.find('meta', attrs={'name': name})
                if meta_element:
                    content = meta_element.get('content', '')
                    if content:
                        return self._parse_iso_date(content)
            
            # time[datetime]
            time_element = soup.find('time', attrs={'datetime': True})
            if time_element:
                datetime_attr = time_element.get('datetime', '')
                if datetime_attr:
                    return self._parse_iso_date(datetime_attr)
            
            # JSON-LD
            json_scripts = soup.find_all('script', type='application/ld+json')
            for script in json_scripts:
                try:
                    import json
                    data = json.loads(script.string)
                    if isinstance(data, dict):
                        if 'datePublished' in data:
                            return self._parse_iso_date(data['datePublished'])
                        elif 'dateModified' in data:
                            return self._parse_iso_date(data['dateModified'])
                except:
                    continue
            
            return {"published_at_kst": "", "published_at_utc": ""}
            
        except Exception as e:
            console.print(f"âš ï¸ ë©”íƒ€ íƒœê·¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {"published_at_kst": "", "published_at_utc": ""}

    def _parse_iso_date(self, date_str: str) -> Dict[str, str]:
        """ISO ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            # ISO í˜•ì‹ íŒŒì‹±
            if '+' in date_str or date_str.endswith('Z'):
                dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                utc_dt = dt.astimezone(pytz.UTC)
                kst_dt = dt.astimezone(KST)
                
                return {
                    "published_at_kst": kst_dt.isoformat(),
                    "published_at_utc": utc_dt.isoformat()
                }
            else:
                # ì‹œê°„ëŒ€ ì •ë³´ê°€ ì—†ìœ¼ë©´ KSTë¡œ ê°€ì •
                dt = datetime.fromisoformat(date_str)
                kst_dt = KST.localize(dt)
                utc_dt = kst_dt.astimezone(pytz.UTC)
                
                return {
                    "published_at_kst": kst_dt.isoformat(),
                    "published_at_utc": utc_dt.isoformat()
                }
                
        except Exception as e:
            console.print(f"âš ï¸ ISO ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {str(e)}")
            return {"published_at_kst": "", "published_at_utc": ""}

    def _convert_kst_to_utc(self, date_str: str) -> str:
        """KST ì‹œê°„ì„ UTCë¡œ ë³€í™˜"""
        try:
            dt = datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S")
            kst_dt = KST.localize(dt)
            utc_dt = kst_dt.astimezone(pytz.UTC)
            return utc_dt.isoformat()
        except Exception as e:
            console.print(f"âš ï¸ KST to UTC ë³€í™˜ ì‹¤íŒ¨: {date_str} - {str(e)}")
            return datetime.now(pytz.UTC).isoformat()

    async def save_articles_batch(self):
        """DB ë°°ì¹˜ ì €ì¥"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ë°°ì¹˜ ì €ì¥ ì¤‘...")

        try:
            # ì–¸ë¡ ì‚¬ í™•ì¸
            media = self.supabase_manager.get_media_outlet(self.media_name)
            if not media:
                media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
            else:
                media_id = media["id"]

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            # ì¤‘ë³µ ì œê±° ë° ë°°ì¹˜ ì¤€ë¹„
            new_articles = []
            skip_count = 0
            
            for article in self.articles:
                if article["url"] in existing_urls:
                    skip_count += 1
                    continue
                    
                # ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
                parsed_article = self._parse_article_data_simple(article, media_id)
                if parsed_article:
                    new_articles.append(parsed_article)

            # ë°°ì¹˜ ì €ì¥
            if new_articles:
                success_count = self._batch_insert_articles(new_articles)
                console.print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
            else:
                console.print("âš ï¸ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {len(new_articles)}, ìŠ¤í‚µ {skip_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

    def _parse_article_data_simple(self, article: dict, media_id: str) -> Optional[dict]:
        """ê¸°ì‚¬ ë°ì´í„° ê°„ë‹¨ íŒŒì‹± (ë°°ì¹˜ ì €ì¥ìš©)"""
        try:
            return {
                'title': article['title'],
                'url': article['url'],
                'content': article.get('content', ''),
                'published_at': article.get('published_at', datetime.now(pytz.UTC).isoformat()),
                'created_at': datetime.now(pytz.UTC).isoformat(),
                'media_id': media_id
            }
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def _batch_insert_articles(self, articles: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ê¸°ì‚¬ ì‚½ì…"""
        try:
            # Supabaseì˜ upsert ê¸°ëŠ¥ ì‚¬ìš©
            result = self.supabase_manager.client.table("articles").upsert(articles).execute()
            return len(result.data) if result.data else 0
        except Exception as e:
            console.print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
            success_count = 0
            for article in articles:
                try:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                except:
                    continue
            return success_count

    async def run(self, num_pages: int = 8):
        """ì‹¤í–‰"""
        try:
            console.print(f"ğŸš€ ë‚´ì¼ì‹ ë¬¸ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {num_pages}í˜ì´ì§€)")
            
            # 1. ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_articles_parallel(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_contents_parallel()
            
            # 3. ê¸°ì‚¬ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
            await self.save_articles_batch()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def main():
    collector = NaeilPoliticsCollector()
    await collector.run(num_pages=8)  # 8í˜ì´ì§€ì—ì„œ ê°ê° 20ê°œì”© ì´ 160ê°œ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())
