#!/usr/bin/env python3
"""
ë‰´ì‹œìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
ê°œì„ ì‚¬í•­:
- ë™ì‹œì„± ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 
- ë°°ì¹˜ DB ì €ì¥ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
- ì—°ê²° í’€ ìµœì í™”
"""
import asyncio
import sys
import os
import httpx
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from datetime import datetime
import pytz
from rich.console import Console
import re
from typing import List, Dict, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.supabase_manager import SupabaseManager

console = Console()

BASE_URL = "https://www.newsis.com"
LIST_URL = "https://www.newsis.com/pol/list/"


class NewsisPoliticsCollector:
    def __init__(self):
        self.articles = []
        self.semaphore = asyncio.Semaphore(5)  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
        self.supabase_manager = SupabaseManager()

    async def run(self, num_pages=8):
        console.print("ğŸš€ ë‰´ì‹œìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
        await self.collect_articles(num_pages)
        await self.collect_contents_parallel()  # ë³‘ë ¬ ì²˜ë¦¬!
        await self.save_articles()  # DB ì €ì¥
        console.print("ğŸ‰ ì™„ë£Œ")

    async def collect_articles(self, num_pages=8):
        for page in range(1, num_pages + 1):
            url = f"{LIST_URL}?cid=10300&scid=10301&page={page}"
            console.print(f"ğŸ“¡ ëª©ë¡ ìš”ì²­: {url}")

            async with httpx.AsyncClient() as client:
                r = await client.get(url)
                soup = BeautifulSoup(r.text, "html.parser")

                for el in soup.select(".txtCont")[:20]:  # ì•ì—ì„œ 20ê°œë§Œ
                    a = el.select_one(".tit a")
                    if not a:
                        continue

                    title = a.get_text(strip=True)
                    href = a["href"]
                    if href.startswith("/"):
                        href = BASE_URL + href

                    self.articles.append(
                        {"title": title, "url": href, "content": "", "published_at": ""}
                    )
                    console.print(f"ğŸ“° {title[:50]}...")

    async def collect_contents_parallel(self):
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë³¸ë¬¸ ìˆ˜ì§‘"""
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ë³‘ë ¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘...")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            
            # ë³‘ë ¬ ì‘ì—… ì‹¤í–‰
            tasks = []
            for i, article in enumerate(self.articles):
                task = self._extract_single_content(browser, article, i + 1)
                tasks.append(task)
            
            # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            await asyncio.gather(*tasks, return_exceptions=True)
            
            await browser.close()

    async def _extract_single_content(self, browser, article, index):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ)"""
        async with self.semaphore:  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            page = None
            try:
                page = await browser.new_page()
                console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
                
                # í˜ì´ì§€ ë¡œë“œ (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
                await page.goto(article["url"], wait_until="domcontentloaded", timeout=20000)

                # ë°œí–‰ ì‹œê°„ ì¶”ì¶œ - ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
                try:
                    # article:published_time ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
                    published_time = await page.get_attribute('meta[property="article:published_time"]', 'content')
                    if published_time:
                        # ISO 8601 í˜•ì‹ íŒŒì‹± (ì˜ˆ: 2025-09-05T13:47:51+09:00)
                        dt = datetime.fromisoformat(published_time.replace('Z', '+00:00'))
                        article["published_at"] = dt.astimezone(pytz.UTC).isoformat()
                        console.print(f"ğŸ“… [{index}] ë°œí–‰ì‹œê°„: {published_time} -> {article['published_at']}")
                    else:
                        # ëŒ€ì•ˆ: ë“±ë¡ ì‹œê°„ì—ì„œ ì¶”ì¶œ
                        time_text = await page.inner_text("span:has-text('ë“±ë¡')", timeout=5000)
                        time_str = time_text.replace("ë“±ë¡", "").strip()
                        dt = datetime.strptime(time_str, "%Y.%m.%d %H:%M:%S")
                        kst = pytz.timezone("Asia/Seoul")
                        article["published_at"] = kst.localize(dt).astimezone(pytz.UTC).isoformat()
                        console.print(f"ğŸ“… [{index}] ë“±ë¡ì‹œê°„: {time_str} -> {article['published_at']}")
                except Exception as e:
                    console.print(f"âš ï¸ [{index}] ë°œí–‰ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:50]}...")
                    # ë°œí–‰ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                    article["published_at"] = "2025-01-01T00:00:00Z"

                # ë³¸ë¬¸ ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)
                content = await page.evaluate(
                    """
                    () => {
                        const article = document.querySelector("article");
                        if (!article) return "";

                        // ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
                        const elementsToRemove = [
                            'div.summury',           // ìš”ì•½ ë¶€ë¶„
                            'div#textBody',          // textBody div ì „ì²´
                            'iframe',                // ê´‘ê³  iframe
                            'script',                // ìŠ¤í¬ë¦½íŠ¸
                            'div#view_ad',          // ê´‘ê³ 
                            'div.thumCont img',     // ì´ë¯¸ì§€
                            'p.photojournal'        // ì‚¬ì§„ ì„¤ëª…
                        ];
                        
                        elementsToRemove.forEach(selector => {
                            article.querySelectorAll(selector).forEach(el => el.remove());
                        });

                        // article ë‚´ìš©ì„ ê°€ì ¸ì˜¨ í›„ HTML íƒœê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        let content = article.innerHTML;
                        
                        // <br> íƒœê·¸ë¥¼ ê°œí–‰ë¬¸ìë¡œ ë³€í™˜
                        content = content.replace(/<br\s*\/?>/gi, '\\n');
                        
                        // ë‹¤ë¥¸ HTML íƒœê·¸ë“¤ ì œê±°
                        content = content.replace(/<[^>]*>/g, '');
                        
                        // HTML ì—”í‹°í‹° ë””ì½”ë”©
                        const tempDiv = document.createElement('div');
                        tempDiv.innerHTML = content;
                        content = tempDiv.textContent || tempDiv.innerText || '';
                        
                        // ì •ë¦¬ ì‘ì—…
                        content = content
                            .replace(/\\n\\s*\\n/g, '\\n')  // ì—°ì†ëœ ê°œí–‰ë¬¸ì ì œê±°
                            .replace(/^\\s+|\\s+$/g, '')    // ì•ë’¤ ê³µë°± ì œê±°
                            .replace(/\\t+/g, ' ')          // íƒ­ì„ ê³µë°±ìœ¼ë¡œ
                            .replace(/\\s+/g, ' ')          // ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                            .replace(/\\n /g, '\\n')        // ê°œí–‰ í›„ ê³µë°± ì œê±°
                            .trim();
                        
                        return content;
                    }
                """
                )
                
                # ì¶”ê°€ ì •ë¦¬ ì‘ì—… (Pythonì—ì„œ)
                if content:
                    # ê¸°ìëª…, ì´ë©”ì¼ ë“± ì •ë¦¬
                    content = re.sub(r'[ê°€-í£]+\s*ê¸°ì\s*=?\s*', '', content)
                    content = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', content)
                    content = re.sub(r'\[ë‰´ì‹œìŠ¤\]', '', content)
                    content = re.sub(r'â—ê³µê°ì–¸ë¡ \s*ë‰´ì‹œìŠ¤.*', '', content)
                    content = re.sub(r'\*ì¬íŒë§¤.*', '', content)
                    content = content.strip()
                
                article["content"] = content
                console.print(f"âœ… [{index}] ì™„ë£Œ: {len(content)}ì")
                
            except Exception as e:
                console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
                article["content"] = ""
                article["published_at"] = datetime.now(pytz.UTC).isoformat()
                
            finally:
                if page:
                    await page.close()

    async def save_articles(self):
        """ìˆ˜ì§‘í•œ ê¸°ì‚¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ë¯¸ë””ì–´ ì•„ì›ƒë › ID ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        media_outlet = self.supabase_manager.get_media_outlet("ë‰´ì‹œìŠ¤")
        if media_outlet:
            media_id = media_outlet['id']
        else:
            media_id = self.supabase_manager.create_media_outlet("ë‰´ì‹œìŠ¤")
        
        # ê¸°ì¡´ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬ìš©)
        existing_urls = set()
        try:
            result = self.supabase_manager.client.table('articles').select('url').eq('media_id', media_id).execute()
            existing_urls = {article['url'] for article in result.data}
        except Exception as e:
            console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        
        success_count = 0
        skip_count = 0
        
        for i, article in enumerate(self.articles, 1):
            try:
                # ì¤‘ë³µ ì²´í¬
                if article['url'] in existing_urls:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")
                    skip_count += 1
                    continue
                
                # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
                article_data = {
                    'title': article['title'],
                    'url': article['url'],
                    'content': article.get('content', ''),
                    'published_at': article.get('published_at', datetime.now(pytz.UTC).isoformat()),
                    'created_at': datetime.now(pytz.UTC).isoformat(),
                    'media_id': media_id
                }
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                success = self.supabase_manager.insert_article(article_data)
                
                if success:
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                    success_count += 1
                else:
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}...")
                
            except Exception as e:
                console.print(f"âŒ [{i}/{len(self.articles)}] ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skip_count}ê°œ")
        total_processed = success_count + skip_count
        success_rate = (success_count / total_processed) * 100 if total_processed > 0 else 0
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")


# ë” ë¹ ë¥¸ ë²„ì „: httpxë§Œ ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
class NewsisFastCollector:
    """httpxë§Œ ì‚¬ìš©í•˜ëŠ” ì´ˆê³ ì† ë²„ì „ (ì„±ëŠ¥ ìµœì í™”)"""
    
    def __init__(self):
        self.articles = []
        self.supabase_manager = SupabaseManager()
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ìµœì í™”)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # ë™ì‹œì„± ì œí•œ ì„¤ì •
        self.semaphore = asyncio.Semaphore(10)  # ìµœëŒ€ 10ê°œ ë™ì‹œ ìš”ì²­
        self.batch_size = 20  # DB ë°°ì¹˜ ì €ì¥ í¬ê¸°

    async def run(self, num_pages=8):
        console.print("ğŸš€ ë‰´ì‹œìŠ¤ ì´ˆê³ ì† í¬ë¡¤ë§ ì‹œì‘ (ìµœì í™” ë²„ì „)")
        
        # 1ë‹¨ê³„: ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
        await self.collect_articles_parallel(num_pages)
        
        # 2ë‹¨ê³„: ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
        await self.collect_contents_httpx_only()
        
        # 3ë‹¨ê³„: ë°°ì¹˜ ì €ì¥
        await self.save_articles_batch()
        
        console.print("ğŸ‰ ì™„ë£Œ")

    async def collect_articles_parallel(self, num_pages=8):
        """ëª©ë¡ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        tasks = [self._collect_page_articles(page_num) for page_num in range(1, num_pages + 1)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_articles = 0
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(result)}")
            else:
                total_articles += result
                
        console.print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def _collect_page_articles(self, page_num: int) -> int:
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        url = f"{LIST_URL}?cid=10300&scid=10301&page={page_num}"
        console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: {url}")

        async with self.semaphore:  # ë™ì‹œì„± ì œí•œ
            try:
                async with httpx.AsyncClient(
                    timeout=15.0,
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
                ) as client:
                    response = await client.get(url, headers=self.headers)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")

                    articles = []
                    for el in soup.select(".txtCont")[:20]:  # ê° í˜ì´ì§€ 20ê°œ
                        a = el.select_one(".tit a")
                        if not a:
                            continue

                        title = a.get_text(strip=True)
                        href = a["href"]
                        if href.startswith("/"):
                            href = BASE_URL + href

                        article = {
                            "title": title,
                            "url": href,
                            "content": "",
                            "published_at": ""
                        }
                        articles.append(article)
                        console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")

                    self.articles.extend(articles)
                    console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    return len(articles)

            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return 0

    async def collect_contents_httpx_only(self):
        """httpxë§Œìœ¼ë¡œ ë³‘ë ¬ ë³¸ë¬¸ ìˆ˜ì§‘ - ë°°ì¹˜ ì²˜ë¦¬!"""
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ì´ˆê³ ì† ë³‘ë ¬ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)...")
        
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì„œ)
        batch_size = 20  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(self.articles))
            batch_articles = self.articles[start_idx:end_idx]
            
            console.print(f"ğŸ“– ë°°ì¹˜ {batch_num + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            async with httpx.AsyncClient(
                timeout=15.0,
                follow_redirects=True,
                limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
            ) as client:
                tasks = [self._extract_with_httpx(client, article, i + start_idx + 1) for i, article in enumerate(batch_articles)]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if batch_num < total_batches - 1:
                await asyncio.sleep(0.5)

    def _clean_content(self, content):
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜"""
        if not content:
            return ""
        
        # ê¸°ìëª…, ì´ë©”ì¼ ë“± ì œê±°
        content = re.sub(r'[ê°€-í£]+\s*ê¸°ì\s*=?\s*', '', content)
        content = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', content)
        content = re.sub(r'\[ë‰´ì‹œìŠ¤\]', '', content)
        content = re.sub(r'â—ê³µê°ì–¸ë¡ \s*ë‰´ì‹œìŠ¤.*', '', content)
        content = re.sub(r'\*ì¬íŒë§¤.*', '', content)
        content = re.sub(r'photo@newsis\.com.*', '', content)
        
        # ì—°ì†ëœ ê³µë°±ê³¼ ê°œí–‰ ì •ë¦¬
        content = re.sub(r'\n\s*\n', '\n', content)
        content = re.sub(r'\s+', ' ', content)
        content = content.strip()
        
        return content

    async def _extract_with_httpx(self, client, article, index):
        """httpxë¡œ HTML íŒŒì‹± - ê°œì„ ëœ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
            
            response = await client.get(article["url"])
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë°œí–‰ì‹œê°„ ì¶”ì¶œ - ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
            published_time = None
            
            # 1. article:published_time ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
            meta_elem = soup.select_one('meta[property="article:published_time"]')
            if meta_elem:
                published_time = meta_elem.get('content', '')
                console.print(f"ğŸ“… [{index}] ë©”íƒ€ íƒœê·¸ì—ì„œ ë°œí–‰ì‹œê°„ ë°œê²¬: {published_time}")
            
            # 2. ëŒ€ì•ˆ: ë“±ë¡ ì‹œê°„ì—ì„œ ì¶”ì¶œ
            if not published_time:
                for span in soup.find_all('span'):
                    if span.get_text() and 'ë“±ë¡' in span.get_text():
                        time_str = span.get_text().replace("ë“±ë¡", "").strip()
                        published_time = time_str
                        console.print(f"ğŸ“… [{index}] ë“±ë¡ ì‹œê°„ì—ì„œ ë°œê²¬: {time_str}")
                        break
            
            if published_time:
                try:
                    if published_time.startswith('2025'):  # ISO 8601 í˜•ì‹
                        # ISO 8601 í˜•ì‹ íŒŒì‹± (ì˜ˆ: 2025-09-05T13:47:51+09:00)
                        dt = datetime.fromisoformat(published_time.replace('Z', '+00:00'))
                        article["published_at"] = dt.astimezone(pytz.UTC).isoformat()
                    else:
                        # ì¼ë°˜ í˜•ì‹ íŒŒì‹± (ì˜ˆ: 2025.09.05 13:47:51)
                        dt = datetime.strptime(published_time, "%Y.%m.%d %H:%M:%S")
                        kst = pytz.timezone("Asia/Seoul")
                        article["published_at"] = kst.localize(dt).astimezone(pytz.UTC).isoformat()
                    console.print(f"ğŸ“… [{index}] ë°œí–‰ì‹œê°„: {published_time} -> {article['published_at']}")
                except Exception as e:
                    console.print(f"âš ï¸ [{index}] ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    article["published_at"] = "2025-01-01T00:00:00Z"
            else:
                console.print(f"âš ï¸ [{index}] ì‹œê°„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                article["published_at"] = "2025-01-01T00:00:00Z"
            
            # ë³¸ë¬¸ ì¶”ì¶œ - ê°œì„ ëœ ë¡œì§
            article_elem = soup.select_one("article")
            if article_elem:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
                for selector in [
                    'div.summury',      # ìš”ì•½
                    'div#textBody',     # textBody div ì „ì²´
                    'iframe',           # ê´‘ê³ 
                    'script',           # ìŠ¤í¬ë¦½íŠ¸
                    'div#view_ad',      # ê´‘ê³ 
                    'img',              # ì´ë¯¸ì§€
                    'p.photojournal'    # ì‚¬ì§„ ì„¤ëª…
                ]:
                    for elem in article_elem.select(selector):
                        elem.decompose()
                
                # articleì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (br íƒœê·¸ ê³ ë ¤)
                # br íƒœê·¸ë¥¼ ê°œí–‰ë¬¸ìë¡œ ë³€í™˜
                for br in article_elem.find_all('br'):
                    br.replace_with('\n')
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                content = article_elem.get_text(separator=' ', strip=True)
                
                # ì •ë¦¬ ì‘ì—…
                content = self._clean_content(content)
                article["content"] = content
                
            else:
                article["content"] = ""
            
            console.print(f"âœ… [{index}] ì™„ë£Œ: {len(article.get('content', ''))}ì")
            
        except Exception as e:
            console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
            article["content"] = ""
            article["published_at"] = datetime.now(pytz.UTC).isoformat()

    async def save_articles_batch(self):
        """DB ë°°ì¹˜ ì €ì¥ (ìµœì í™”)"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ë°°ì¹˜ ì €ì¥ ì¤‘...")

        try:
            media_outlet = self.supabase_manager.get_media_outlet("ë‰´ì‹œìŠ¤")
            if media_outlet:
                media_id = media_outlet["id"]
            else:
                media_id = self.supabase_manager.create_media_outlet("ë‰´ì‹œìŠ¤")

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            # ì¤‘ë³µ ì œê±° ë° ë°°ì¹˜ ì¤€ë¹„
            new_articles = []
            skip_count = 0
            
            for article in self.articles:
                if article["url"] in existing_urls:
                    skip_count += 1
                    continue
                    
                article_data = {
                    "title": article["title"],
                    "url": article["url"],
                    "content": article.get("content", ""),
                    "published_at": article.get("published_at", datetime.now(pytz.UTC).isoformat()),
                    "created_at": datetime.now(pytz.UTC).isoformat(),
                    "media_id": media_id,
                }
                new_articles.append(article_data)

            # ë°°ì¹˜ ì €ì¥
            if new_articles:
                success_count = self._batch_insert_articles(new_articles)
                console.print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
            else:
                console.print("âš ï¸ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {len(new_articles)}, ìŠ¤í‚µ {skip_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

    def _batch_insert_articles(self, articles: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ê¸°ì‚¬ ì‚½ì…"""
        try:
            # Supabaseì˜ upsert ê¸°ëŠ¥ ì‚¬ìš©
            result = self.supabase_manager.client.table("articles").upsert(articles).execute()
            return len(result.data) if result.data else 0
        except Exception as e:
            console.print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
            success_count = 0
            for article in articles:
                try:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                except:
                    continue
            return success_count


async def main():
    console.print("ğŸš€ ë‰´ì‹œìŠ¤ ì´ˆê³ ì† í¬ë¡¤ë§ ì‹œì‘ (httpxë§Œ ì‚¬ìš©)")
    collector = NewsisFastCollector()
    await collector.run(num_pages=8)  # 8í˜ì´ì§€ì—ì„œ ê°ê° 20ê°œì”© ì´ 160ê°œ ìˆ˜ì§‘ (150ê°œ ëª©í‘œ)
    
    # ê²°ê³¼ ì¶œë ¥
    console.print(f"\nğŸ“‹ ìˆ˜ì§‘ëœ ê¸°ì‚¬ {len(collector.articles)}ê°œ:")
    for i, art in enumerate(collector.articles, 1):
        console.print(f"\n=== ê¸°ì‚¬ {i} ===")
        console.print(f"ì œëª©: {art['title']}")
        console.print(f"URL: {art['url']}")
        console.print(f"ë°œí–‰ì‹œê°„: {art.get('published_at', 'N/A')}")
        console.print(f"ë³¸ë¬¸ ê¸¸ì´: {len(art.get('content', ''))}ì")
        if art.get('content'):
            preview = art['content'][:200].replace('\n', ' ')
            console.print(f"ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {preview}...")


if __name__ == "__main__":
    asyncio.run(main())