#!/usr/bin/env python3
"""
ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
ê°œì„ ì‚¬í•­:
- ë™ì‹œì„± ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 
- ë°°ì¹˜ DB ì €ì¥ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
- httpx ê¸°ë°˜ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ì†ë„ í–¥ìƒ
- ì—°ê²° í’€ ìµœì í™”
"""

import asyncio
import httpx
from datetime import datetime
from urllib.parse import urljoin
from rich.console import Console
import pytz
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Optional

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.supabase_manager import SupabaseManager

console = Console()

class JoongangPoliticsCollector:
    """ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ê¸° (ì„±ëŠ¥ ìµœì í™”)"""
    
    def __init__(self):
        self.media_name = "ì¤‘ì•™ì¼ë³´"
        self.base_url = "https://www.joongang.co.kr"
        self.articles = []
        self.supabase_manager = SupabaseManager()
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ìµœì í™”)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # ë™ì‹œì„± ì œí•œ ì„¤ì •
        self.semaphore = asyncio.Semaphore(10)  # ìµœëŒ€ 10ê°œ ë™ì‹œ ìš”ì²­
        self.batch_size = 20  # DB ë°°ì¹˜ ì €ì¥ í¬ê¸°
        
    async def _get_page_articles(self, page_num: int) -> list:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        try:
            url = f"{self.base_url}/politics?page={page_num}"
            console.print(f"ğŸ“¡ í˜ì´ì§€ ìˆ˜ì§‘: {url}")
            
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë¬´ì‹œí•´ì•¼ í•˜ëŠ” ì˜ì—­ ì œê±°
                showcase = soup.find('section', class_='showcase_general')
                if showcase:
                    showcase.decompose()
                    console.print("ğŸ—‘ï¸ showcase_general ì˜ì—­ ì œê±°")
                
                rank_list = soup.find('ul', class_='card_right_list rank_list')
                if rank_list:
                    rank_list.decompose()
                    console.print("ğŸ—‘ï¸ rank_list ì˜ì—­ ì œê±°")
                
                # ìˆ˜ì§‘ ëŒ€ìƒ: <ul id="story_list"> ì•ˆì˜ <li class="card">
                story_list = soup.find('ul', id='story_list')
                if not story_list:
                    console.print("âŒ story_listë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    return []
                
                cards = story_list.find_all('li', class_='card')
                console.print(f"ğŸ” story_listì—ì„œ {len(cards)}ê°œ ì¹´ë“œ ë°œê²¬")
                
                articles = []
                max_articles_per_page = 24  # ê° í˜ì´ì§€ì—ì„œ 24ê°œ ìˆ˜ì§‘
                collected_count = 0
                
                for i, card in enumerate(cards):
                    if collected_count >= max_articles_per_page:
                        break
                        
                    try:
                        # ì œëª©ê³¼ URL ì¶”ì¶œ
                        headline = card.find('h2', class_='headline')
                        if not headline:
                            continue
                            
                        link = headline.find('a')
                        if not link:
                            continue
                        
                        title = link.get_text(strip=True)
                        article_url = link.get('href', '')
                        
                        if title and article_url:
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if article_url.startswith('/'):
                                full_url = urljoin(self.base_url, article_url)
                            else:
                                full_url = article_url
                            
                            articles.append({
                                'title': title,
                                'url': full_url
                            })
                            collected_count += 1
                            console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬ [{collected_count}]: {title[:50]}...")
                    
                    except Exception as e:
                        console.print(f"âš ï¸ ì¹´ë“œ [{i}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                console.print(f"ğŸ“Š í˜ì´ì§€ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                return articles
                
        except Exception as e:
            console.print(f"âŒ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_content(self, url: str) -> dict:
        """ê¸°ì‚¬ ë³¸ë¬¸ ë° ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ë° ë°œí–‰ì‹œê°„ ì¶”ì¶œ
                content_data = await page.evaluate("""
                    () => {
                        // ë°œí–‰ì‹œê°„ ì¶”ì¶œ - <time itemprop="datePublished">ì˜ datetime ì†ì„± ì‚¬ìš©
                        let published_at = '';
                        
                        // 1. time[itemprop="datePublished"] datetime ì†ì„± ì‹œë„
                        const timeElement = document.querySelector('time[itemprop="datePublished"]');
                        if (timeElement) {
                            published_at = timeElement.getAttribute('datetime');
                        }
                        
                        // 2. ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì‹œê°„ ì„ íƒìë“¤ ì‹œë„
                        if (!published_at) {
                            const timeSelectors = [
                                'time[datetime]',
                                'button.btn_datetime span',
                                '.article_info .date',
                                '.article_info .time',
                                '.date_info',
                                '.article_date',
                                '.publish_date'
                            ];
                            
                            for (const selector of timeSelectors) {
                                const element = document.querySelector(selector);
                                if (element) {
                                    // datetime ì†ì„±ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                                    const datetime = element.getAttribute('datetime');
                                    if (datetime) {
                                        published_at = datetime;
                                        break;
                                    }
                                    
                                    // ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ í˜•ì‹ ì°¾ê¸°
                                    const text = element.textContent || element.innerText || '';
                                    const trimmed = text.trim();
                                    if (trimmed.match(/\\d{4}-\\d{2}-\\d{2}/) || 
                                        trimmed.match(/\\d{4}\\.\\d{2}\\.\\d{2}/) ||
                                        trimmed.match(/\\d{2}:\\d{2}/)) {
                                        published_at = trimmed;
                                        break;
                                    }
                                }
                            }
                        }
                        
                        // ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
                        const articleBody = document.getElementById('article_body');
                        if (!articleBody) return { content: '', published_at: published_at };
                        
                        // ê´‘ê³  ì˜ì—­ ì œê±°
                        const adElements = articleBody.querySelectorAll('#ad_art_content_mid, .ad, .advertisement');
                        adElements.forEach(el => el.remove());
                        
                        // <p> íƒœê·¸ë“¤ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        const paragraphs = articleBody.querySelectorAll('p');
                        const contentLines = [];
                        
                        paragraphs.forEach(p => {
                            const text = p.textContent || p.innerText || '';
                            const trimmedText = text.trim();
                            
                            // ê¸°ìëª…/ì¶œì²˜ ë¶€ë¶„ ì œê±°
                            if (trimmedText && 
                                !trimmedText.includes('ê¸°ì') && 
                                !trimmedText.includes('@') &&
                                !trimmedText.includes('[ì¶œì²˜:') &&
                                !trimmedText.includes('ì¶œì²˜:') &&
                                !trimmedText.includes('ì •ì¬í™') &&
                                !trimmedText.includes('hongj@joongang.co.kr') &&
                                trimmedText.length > 10) {
                                contentLines.push(trimmedText);
                            }
                        });
                        
                        // ê° ë¬¸ë‹¨ì„ ê°œí–‰ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê²°í•©
                        const content = contentLines.join('\\n\\n');
                        
                        return {
                            content: content,
                            published_at: published_at
                        };
                    }
                """)
                
                await browser.close()
                return content_data
                
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return {"content": "", "published_at": ""}
    
    async def _parse_article_data(self, article: dict, content_data: dict) -> dict:
        """ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ë° ì •ë¦¬"""
        try:
            # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬ (ê¸°ì‚¬ ì‹¤ì œ ë°œí–‰ì‹œê°„)
            published_at_str = content_data.get('published_at', '')
            
            if published_at_str and published_at_str.strip():
                try:
                    clean_time = published_at_str.strip()
                    
                    # "ì—…ë°ì´íŠ¸ ì •ë³´ ë”ë³´ê¸°" ê°™ì€ í…ìŠ¤íŠ¸ ì œê±°
                    if 'ì—…ë°ì´íŠ¸' in clean_time or 'ë”ë³´ê¸°' in clean_time:
                        clean_time = ''
                    
                    if clean_time:
                        if 'T' in clean_time and '+' in clean_time:
                            # ISO í˜•ì‹ with timezone (ì˜ˆ: "2025-09-05T01:17:00+09:00")
                            published_at = datetime.fromisoformat(clean_time)
                            # UTCë¡œ ë³€í™˜
                            published_at = published_at.astimezone(pytz.UTC)
                        elif 'T' in clean_time:
                            # ISO í˜•ì‹ without timezone (UTCë¡œ ê°€ì •)
                            published_at = datetime.fromisoformat(clean_time.replace('Z', '+00:00'))
                        elif '-' in clean_time and ':' in clean_time:
                            # "YYYY-MM-DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                            published_at = datetime.strptime(clean_time, "%Y-%m-%d %H:%M")
                            kst = pytz.timezone("Asia/Seoul")
                            published_at = kst.localize(published_at).astimezone(pytz.UTC)
                        elif '.' in clean_time and ':' in clean_time:
                            # "YYYY.MM.DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                            published_at = datetime.strptime(clean_time, "%Y.%m.%d %H:%M")
                            kst = pytz.timezone("Asia/Seoul")
                            published_at = kst.localize(published_at).astimezone(pytz.UTC)
                        else:
                            # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                            console.print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‹œê°„ í˜•ì‹: {clean_time}")
                            published_at = datetime.now(pytz.UTC)
                    else:
                        published_at = datetime.now(pytz.UTC)
                        
                except Exception as e:
                    console.print(f"âš ï¸ ë°œí–‰ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {published_at_str} - {e}")
                    published_at = datetime.now(pytz.UTC)
            else:
                published_at = datetime.now(pytz.UTC)
            
            # ìƒì„± ì‹œê°„ (í¬ë¡¤ë§ ì‹œì ì˜ í˜„ì¬ ì‹œê°)
            created_at = datetime.now(pytz.UTC)
            
            return {
                'title': article['title'],
                'url': article['url'],
                'content': content_data.get('content', ''),
                'published_at': published_at.isoformat(),
                'created_at': created_at.isoformat(),
                'media_outlet': self.media_name
            }
            
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    async def collect_articles_parallel(self, num_pages: int = 7):
        """ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        tasks = [self._collect_page_articles_parallel(page_num) for page_num in range(1, num_pages + 1)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_articles = 0
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(result)}")
            else:
                total_articles += result
                
        console.print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def _collect_page_articles_parallel(self, page_num: int) -> int:
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        url = f"{self.base_url}/politics?page={page_num}"
        console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: {url}")

        async with self.semaphore:  # ë™ì‹œì„± ì œí•œ
            try:
                async with httpx.AsyncClient(
                    timeout=15.0,
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
                ) as client:
                    response = await client.get(url, headers=self.headers)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ë¬´ì‹œí•´ì•¼ í•˜ëŠ” ì˜ì—­ ì œê±°
                    showcase = soup.find('section', class_='showcase_general')
                    if showcase:
                        showcase.decompose()
                    
                    rank_list = soup.find('ul', class_='card_right_list rank_list')
                    if rank_list:
                        rank_list.decompose()
                    
                    # ìˆ˜ì§‘ ëŒ€ìƒ: <ul id="story_list"> ì•ˆì˜ <li class="card">
                    story_list = soup.find('ul', id='story_list')
                    if not story_list:
                        console.print(f"âŒ í˜ì´ì§€ {page_num}: story_listë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        return 0
                    
                    cards = story_list.find_all('li', class_='card')
                    console.print(f"ğŸ” í˜ì´ì§€ {page_num}: {len(cards)}ê°œ ì¹´ë“œ ë°œê²¬")
                    
                    articles = []
                    max_articles_per_page = 24  # ê° í˜ì´ì§€ì—ì„œ 24ê°œ ìˆ˜ì§‘
                    collected_count = 0
                    
                    for i, card in enumerate(cards):
                        if collected_count >= max_articles_per_page:
                            break
                            
                        try:
                            # ì œëª©ê³¼ URL ì¶”ì¶œ
                            headline = card.find('h2', class_='headline')
                            if not headline:
                                continue
                                
                            link = headline.find('a')
                            if not link:
                                continue
                            
                            title = link.get_text(strip=True)
                            article_url = link.get('href', '')
                            
                            if title and article_url:
                                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                                if article_url.startswith('/'):
                                    full_url = urljoin(self.base_url, article_url)
                                else:
                                    full_url = article_url
                                
                                article = {
                                    'title': title,
                                    'url': full_url,
                                    'content': '',
                                    'published_at': ''
                                }
                                articles.append(article)
                                collected_count += 1
                                console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")
                        
                        except Exception as e:
                            console.print(f"âš ï¸ ì¹´ë“œ [{i}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            continue
                    
                    self.articles.extend(articles)
                    console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    return len(articles)

            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return 0
    
    async def collect_contents_parallel(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì„œ)
        batch_size = 20  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(self.articles))
            batch_articles = self.articles[start_idx:end_idx]
            
            console.print(f"ğŸ“– ë°°ì¹˜ {batch_num + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            async with httpx.AsyncClient(
                timeout=15.0,
                follow_redirects=True,
                limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
            ) as client:
                tasks = [self._extract_content_httpx(client, article, i + start_idx + 1) for i, article in enumerate(batch_articles)]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if batch_num < total_batches - 1:
                await asyncio.sleep(0.5)

    async def _extract_content_httpx(self, client: httpx.AsyncClient, article: dict, index: int):
        """httpxë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ë° ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
            
            response = await client.get(article["url"], headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë°œí–‰ì‹œê°„ ì¶”ì¶œ
            published_at = self._extract_published_at(soup)
            article["published_at"] = published_at
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content_text(soup)
            article["content"] = content
            
            console.print(f"âœ… [{index}] ì™„ë£Œ: {len(content)}ì")
            
        except Exception as e:
            console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
            article["content"] = ""
            article["published_at"] = datetime.now(pytz.UTC).isoformat()

    def _extract_published_at(self, soup: BeautifulSoup) -> str:
        """ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            # 1. time[itemprop="datePublished"] datetime ì†ì„± ì‹œë„
            time_element = soup.select_one('time[itemprop="datePublished"]')
            if time_element:
                published_at = time_element.get('datetime', '')
                if published_at:
                    return self._parse_datetime(published_at)
            
            # 2. ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì‹œê°„ ì„ íƒìë“¤ ì‹œë„
            time_selectors = [
                'time[datetime]',
                'button.btn_datetime span',
                '.article_info .date',
                '.article_info .time',
                '.date_info',
                '.article_date',
                '.publish_date'
            ]
            
            for selector in time_selectors:
                element = soup.select_one(selector)
                if element:
                    # datetime ì†ì„±ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                    datetime_attr = element.get('datetime')
                    if datetime_attr:
                        return self._parse_datetime(datetime_attr)
                    
                    # ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ í˜•ì‹ ì°¾ê¸°
                    text = element.get_text(strip=True)
                    if text and (re.match(r'\d{4}-\d{2}-\d{2}', text) or 
                               re.match(r'\d{4}\.\d{2}\.\d{2}', text) or
                               re.match(r'\d{2}:\d{2}', text)):
                        return self._parse_datetime(text)
            
            return datetime.now(pytz.UTC).isoformat()
            
        except Exception as e:
            console.print(f"âš ï¸ ë°œí–‰ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return datetime.now(pytz.UTC).isoformat()

    def _extract_content_text(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            article_body = soup.find('div', id='article_body')
            if not article_body:
                return ""
            
            # ê´‘ê³  ì˜ì—­ ì œê±°
            ad_elements = article_body.select('#ad_art_content_mid, .ad, .advertisement')
            for el in ad_elements:
                el.decompose()
            
            # <p> íƒœê·¸ë“¤ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            paragraphs = article_body.select('p')
            content_lines = []
            
            for p in paragraphs:
                text = p.get_text(strip=True)
                
                # ê¸°ìëª…/ì¶œì²˜ ë¶€ë¶„ ì œê±°
                if (text and 
                    not text.find('ê¸°ì') >= 0 and 
                    not text.find('@') >= 0 and
                    not text.find('[ì¶œì²˜:') >= 0 and
                    not text.find('ì¶œì²˜:') >= 0 and
                    not text.find('ì •ì¬í™') >= 0 and
                    not text.find('hongj@joongang.co.kr') >= 0 and
                    len(text) > 10):
                    content_lines.append(text)
            
            # ê° ë¬¸ë‹¨ì„ ê°œí–‰ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê²°í•©
            return '\n\n'.join(content_lines)
            
        except Exception as e:
            console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return ""

    def _parse_datetime(self, datetime_str: str) -> str:
        """ë‚ ì§œì‹œê°„ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            clean_time = datetime_str.strip()
            
            # "ì—…ë°ì´íŠ¸ ì •ë³´ ë”ë³´ê¸°" ê°™ì€ í…ìŠ¤íŠ¸ ì œê±°
            if 'ì—…ë°ì´íŠ¸' in clean_time or 'ë”ë³´ê¸°' in clean_time:
                return datetime.now(pytz.UTC).isoformat()
            
            if 'T' in clean_time and '+' in clean_time:
                # ISO í˜•ì‹ with timezone (ì˜ˆ: "2025-09-05T01:17:00+09:00")
                published_at = datetime.fromisoformat(clean_time)
                return published_at.astimezone(pytz.UTC).isoformat()
            elif 'T' in clean_time:
                # ISO í˜•ì‹ without timezone (UTCë¡œ ê°€ì •)
                published_at = datetime.fromisoformat(clean_time.replace('Z', '+00:00'))
                return published_at.isoformat()
            elif '-' in clean_time and ':' in clean_time:
                # "YYYY-MM-DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                published_at = datetime.strptime(clean_time, "%Y-%m-%d %H:%M")
                kst = pytz.timezone("Asia/Seoul")
                return kst.localize(published_at).astimezone(pytz.UTC).isoformat()
            elif '.' in clean_time and ':' in clean_time:
                # "YYYY.MM.DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                published_at = datetime.strptime(clean_time, "%Y.%m.%d %H:%M")
                kst = pytz.timezone("Asia/Seoul")
                return kst.localize(published_at).astimezone(pytz.UTC).isoformat()
            else:
                return datetime.now(pytz.UTC).isoformat()
                
        except Exception as e:
            console.print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {clean_time} - {str(e)}")
            return datetime.now(pytz.UTC).isoformat()
    
    async def save_articles_batch(self):
        """DB ë°°ì¹˜ ì €ì¥ (ìµœì í™”)"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ë°°ì¹˜ ì €ì¥ ì¤‘...")

        try:
            # ì–¸ë¡ ì‚¬ í™•ì¸
            media = self.supabase_manager.get_media_outlet(self.media_name)
            if not media:
                media_id = self.supabase_manager.create_media_outlet(
                    name=self.media_name,
                    bias="center-right",
                    website=self.base_url
                )
            else:
                media_id = media["id"]

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            # ì¤‘ë³µ ì œê±° ë° ë°°ì¹˜ ì¤€ë¹„
            new_articles = []
            skip_count = 0
            
            for article in self.articles:
                if article["url"] in existing_urls:
                    skip_count += 1
                    continue
                    
                # ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
                parsed_article = self._parse_article_data_simple(article, media_id)
                if parsed_article:
                    new_articles.append(parsed_article)

            # ë°°ì¹˜ ì €ì¥
            if new_articles:
                success_count = self._batch_insert_articles(new_articles)
                console.print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
            else:
                console.print("âš ï¸ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {len(new_articles)}, ìŠ¤í‚µ {skip_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

    def _parse_article_data_simple(self, article: dict, media_id: str) -> Optional[dict]:
        """ê¸°ì‚¬ ë°ì´í„° ê°„ë‹¨ íŒŒì‹± (ë°°ì¹˜ ì €ì¥ìš©)"""
        try:
            return {
                'title': article['title'],
                'url': article['url'],
                'content': article.get('content', ''),
                'published_at': article.get('published_at', datetime.now(pytz.UTC).isoformat()),
                'created_at': datetime.now(pytz.UTC).isoformat(),
                'media_id': media_id
            }
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def _batch_insert_articles(self, articles: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ê¸°ì‚¬ ì‚½ì…"""
        try:
            # Supabaseì˜ upsert ê¸°ëŠ¥ ì‚¬ìš©
            result = self.supabase_manager.client.table("articles").upsert(articles).execute()
            return len(result.data) if result.data else 0
        except Exception as e:
            console.print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
            success_count = 0
            for article in articles:
                try:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                except:
                    continue
            return success_count
    
    async def run(self, num_pages: int = 7):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰ (ìµœì í™” ë²„ì „)"""
        try:
            console.print("ğŸš€ ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœì í™” ë²„ì „)")
            
            # 1. ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_articles_parallel(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_contents_parallel()
            
            # 3. ê¸°ì‚¬ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
            await self.save_articles_batch()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

async def main():
    collector = JoongangPoliticsCollector()
    await collector.run(num_pages=7)  # 7í˜ì´ì§€ì—ì„œ ê°ê° 24ê°œì”© ì´ 168ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (150ê°œ ëª©í‘œ)

if __name__ == "__main__":
    asyncio.run(main())
