#!/usr/bin/env python3
"""
ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
"""

import asyncio
import httpx
from datetime import datetime
from urllib.parse import urljoin
from rich.console import Console
from playwright.async_api import async_playwright
import pytz
from bs4 import BeautifulSoup

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.supabase_manager import SupabaseManager

console = Console()

class JoongangPoliticsCollector:
    """ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.media_name = "ì¤‘ì•™ì¼ë³´"
        self.base_url = "https://www.joongang.co.kr"
        self.articles = []
        self.supabase_manager = SupabaseManager()
        
    async def _get_page_articles(self, page_num: int) -> list:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        try:
            url = f"{self.base_url}/politics?page={page_num}"
            console.print(f"ğŸ“¡ í˜ì´ì§€ ìˆ˜ì§‘: {url}")
            
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë¬´ì‹œí•´ì•¼ í•˜ëŠ” ì˜ì—­ ì œê±°
                showcase = soup.find('section', class_='showcase_general')
                if showcase:
                    showcase.decompose()
                    console.print("ğŸ—‘ï¸ showcase_general ì˜ì—­ ì œê±°")
                
                rank_list = soup.find('ul', class_='card_right_list rank_list')
                if rank_list:
                    rank_list.decompose()
                    console.print("ğŸ—‘ï¸ rank_list ì˜ì—­ ì œê±°")
                
                # ìˆ˜ì§‘ ëŒ€ìƒ: <ul id="story_list"> ì•ˆì˜ <li class="card">
                story_list = soup.find('ul', id='story_list')
                if not story_list:
                    console.print("âŒ story_listë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    return []
                
                cards = story_list.find_all('li', class_='card')
                console.print(f"ğŸ” story_listì—ì„œ {len(cards)}ê°œ ì¹´ë“œ ë°œê²¬")
                
                articles = []
                max_articles_per_page = 24  # ê° í˜ì´ì§€ì—ì„œ 24ê°œ ìˆ˜ì§‘
                collected_count = 0
                
                for i, card in enumerate(cards):
                    if collected_count >= max_articles_per_page:
                        break
                        
                    try:
                        # ì œëª©ê³¼ URL ì¶”ì¶œ
                        headline = card.find('h2', class_='headline')
                        if not headline:
                            continue
                            
                        link = headline.find('a')
                        if not link:
                            continue
                        
                        title = link.get_text(strip=True)
                        article_url = link.get('href', '')
                        
                        if title and article_url:
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if article_url.startswith('/'):
                                full_url = urljoin(self.base_url, article_url)
                            else:
                                full_url = article_url
                            
                            articles.append({
                                'title': title,
                                'url': full_url
                            })
                            collected_count += 1
                            console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬ [{collected_count}]: {title[:50]}...")
                    
                    except Exception as e:
                        console.print(f"âš ï¸ ì¹´ë“œ [{i}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                console.print(f"ğŸ“Š í˜ì´ì§€ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                return articles
                
        except Exception as e:
            console.print(f"âŒ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_content(self, url: str) -> dict:
        """ê¸°ì‚¬ ë³¸ë¬¸ ë° ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ë° ë°œí–‰ì‹œê°„ ì¶”ì¶œ
                content_data = await page.evaluate("""
                    () => {
                        // ë°œí–‰ì‹œê°„ ì¶”ì¶œ - <time itemprop="datePublished">ì˜ datetime ì†ì„± ì‚¬ìš©
                        let published_at = '';
                        
                        // 1. time[itemprop="datePublished"] datetime ì†ì„± ì‹œë„
                        const timeElement = document.querySelector('time[itemprop="datePublished"]');
                        if (timeElement) {
                            published_at = timeElement.getAttribute('datetime');
                        }
                        
                        // 2. ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì‹œê°„ ì„ íƒìë“¤ ì‹œë„
                        if (!published_at) {
                            const timeSelectors = [
                                'time[datetime]',
                                'button.btn_datetime span',
                                '.article_info .date',
                                '.article_info .time',
                                '.date_info',
                                '.article_date',
                                '.publish_date'
                            ];
                            
                            for (const selector of timeSelectors) {
                                const element = document.querySelector(selector);
                                if (element) {
                                    // datetime ì†ì„±ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                                    const datetime = element.getAttribute('datetime');
                                    if (datetime) {
                                        published_at = datetime;
                                        break;
                                    }
                                    
                                    // ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ í˜•ì‹ ì°¾ê¸°
                                    const text = element.textContent || element.innerText || '';
                                    const trimmed = text.trim();
                                    if (trimmed.match(/\\d{4}-\\d{2}-\\d{2}/) || 
                                        trimmed.match(/\\d{4}\\.\\d{2}\\.\\d{2}/) ||
                                        trimmed.match(/\\d{2}:\\d{2}/)) {
                                        published_at = trimmed;
                                        break;
                                    }
                                }
                            }
                        }
                        
                        // ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
                        const articleBody = document.getElementById('article_body');
                        if (!articleBody) return { content: '', published_at: published_at };
                        
                        // ê´‘ê³  ì˜ì—­ ì œê±°
                        const adElements = articleBody.querySelectorAll('#ad_art_content_mid, .ad, .advertisement');
                        adElements.forEach(el => el.remove());
                        
                        // <p> íƒœê·¸ë“¤ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        const paragraphs = articleBody.querySelectorAll('p');
                        const contentLines = [];
                        
                        paragraphs.forEach(p => {
                            const text = p.textContent || p.innerText || '';
                            const trimmedText = text.trim();
                            
                            // ê¸°ìëª…/ì¶œì²˜ ë¶€ë¶„ ì œê±°
                            if (trimmedText && 
                                !trimmedText.includes('ê¸°ì') && 
                                !trimmedText.includes('@') &&
                                !trimmedText.includes('[ì¶œì²˜:') &&
                                !trimmedText.includes('ì¶œì²˜:') &&
                                !trimmedText.includes('ì •ì¬í™') &&
                                !trimmedText.includes('hongj@joongang.co.kr') &&
                                trimmedText.length > 10) {
                                contentLines.push(trimmedText);
                            }
                        });
                        
                        // ê° ë¬¸ë‹¨ì„ ê°œí–‰ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê²°í•©
                        const content = contentLines.join('\\n\\n');
                        
                        return {
                            content: content,
                            published_at: published_at
                        };
                    }
                """)
                
                await browser.close()
                return content_data
                
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return {"content": "", "published_at": ""}
    
    async def _parse_article_data(self, article: dict, content_data: dict) -> dict:
        """ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ë° ì •ë¦¬"""
        try:
            # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬ (ê¸°ì‚¬ ì‹¤ì œ ë°œí–‰ì‹œê°„)
            published_at_str = content_data.get('published_at', '')
            
            if published_at_str and published_at_str.strip():
                try:
                    clean_time = published_at_str.strip()
                    
                    # "ì—…ë°ì´íŠ¸ ì •ë³´ ë”ë³´ê¸°" ê°™ì€ í…ìŠ¤íŠ¸ ì œê±°
                    if 'ì—…ë°ì´íŠ¸' in clean_time or 'ë”ë³´ê¸°' in clean_time:
                        clean_time = ''
                    
                    if clean_time:
                        if 'T' in clean_time and '+' in clean_time:
                            # ISO í˜•ì‹ with timezone (ì˜ˆ: "2025-09-05T01:17:00+09:00")
                            published_at = datetime.fromisoformat(clean_time)
                            # UTCë¡œ ë³€í™˜
                            published_at = published_at.astimezone(pytz.UTC)
                        elif 'T' in clean_time:
                            # ISO í˜•ì‹ without timezone (UTCë¡œ ê°€ì •)
                            published_at = datetime.fromisoformat(clean_time.replace('Z', '+00:00'))
                        elif '-' in clean_time and ':' in clean_time:
                            # "YYYY-MM-DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                            published_at = datetime.strptime(clean_time, "%Y-%m-%d %H:%M")
                            kst = pytz.timezone("Asia/Seoul")
                            published_at = kst.localize(published_at).astimezone(pytz.UTC)
                        elif '.' in clean_time and ':' in clean_time:
                            # "YYYY.MM.DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                            published_at = datetime.strptime(clean_time, "%Y.%m.%d %H:%M")
                            kst = pytz.timezone("Asia/Seoul")
                            published_at = kst.localize(published_at).astimezone(pytz.UTC)
                        else:
                            # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                            console.print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‹œê°„ í˜•ì‹: {clean_time}")
                            published_at = datetime.now(pytz.UTC)
                    else:
                        published_at = datetime.now(pytz.UTC)
                        
                except Exception as e:
                    console.print(f"âš ï¸ ë°œí–‰ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {published_at_str} - {e}")
                    published_at = datetime.now(pytz.UTC)
            else:
                published_at = datetime.now(pytz.UTC)
            
            # ìƒì„± ì‹œê°„ (í¬ë¡¤ë§ ì‹œì ì˜ í˜„ì¬ ì‹œê°)
            created_at = datetime.now(pytz.UTC)
            
            return {
                'title': article['title'],
                'url': article['url'],
                'content': content_data.get('content', ''),
                'published_at': published_at.isoformat(),
                'created_at': created_at.isoformat(),
                'media_outlet': self.media_name
            }
            
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    async def collect_articles(self, num_pages: int = 5):
        """ê¸°ì‚¬ ìˆ˜ì§‘"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
        
        for page in range(1, num_pages + 1):
            console.print(f"ğŸ“„ í˜ì´ì§€ {page}/{num_pages} ì²˜ë¦¬ ì¤‘...")
            articles = await self._get_page_articles(page)
            self.articles.extend(articles)
        
        console.print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: {len(self.articles)}ê°œ ì„±ê³µ")
    
    async def collect_contents(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘"""
        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬")
        
        for i, article in enumerate(self.articles, 1):
            console.print(f"ğŸ“– [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘: {article['title'][:50]}...")
            
            content_data = await self._extract_content(article['url'])
            
            # ê¸°ì‚¬ ë°ì´í„°ì— ë³¸ë¬¸ê³¼ ë°œí–‰ì‹œê°„ ì¶”ê°€
            article['content'] = content_data.get('content', '')
            article['published_at'] = content_data.get('published_at', '')
            
            console.print(f"âœ… [{i}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
    
    async def save_articles(self):
        """ê¸°ì‚¬ ì €ì¥"""
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ì–¸ë¡ ì‚¬ í™•ì¸
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(
                name=self.media_name,
                bias="center-right",
                website=self.base_url
            )
        else:
            media_id = media["id"]
        
        # ì¤‘ë³µ ì²´í¬
        urls = [art["url"] for art in self.articles]
        existing_urls = set()
        
        try:
            for url in urls:
                exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                if exists.data:
                    existing_urls.add(url)
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        success_count = 0
        skip_count = 0
        
        for i, article in enumerate(self.articles, 1):
            try:
                # ì¤‘ë³µ ì²´í¬
                if article["url"] in existing_urls:
                    skip_count += 1
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")
                    continue
                
                # ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
                parsed_article = await self._parse_article_data(article, article)
                
                if not parsed_article:
                    continue
                
                # media_id ì¶”ê°€
                if media_id:
                    parsed_article['media_id'] = media_id
                
                # media_outlet í•„ë“œ ì œê±° (ìŠ¤í‚¤ë§ˆì— ì—†ìŒ)
                if 'media_outlet' in parsed_article:
                    del parsed_article['media_outlet']
                
                # Supabaseì— ì €ì¥
                result = self.supabase_manager.insert_article(parsed_article)
                
                if result:
                    success_count += 1
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {parsed_article['title'][:50]}...")
                else:
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨")
                    
            except Exception as e:
                console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        console.print(f"  âŒ ì‹¤íŒ¨: {len(self.articles) - success_count - skip_count}ê°œ")
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skip_count}ê°œ")
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_count/len(self.articles)*100:.1f}%")
    
    async def run(self, num_pages: int = 5):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            console.print("ğŸš€ ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
            
            # 1. ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
            await self.collect_articles(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
            await self.collect_contents()
            
            # 3. ê¸°ì‚¬ ì €ì¥
            await self.save_articles()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

async def main():
    collector = JoongangPoliticsCollector()
    await collector.run(num_pages=5)  # 5í˜ì´ì§€ì—ì„œ ê°ê° 24ê°œì”© ì´ 120ê°œ ê¸°ì‚¬ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())
