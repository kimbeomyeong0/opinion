#!/usr/bin/env python3
"""
ì—°í•©ë‰´ìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
 - ê¸°ì‚¬ ëª©ë¡: https://www.yna.co.kr/politics/all/{page}
 - ë³¸ë¬¸: .story-news.article ë‚´ë¶€ <p>
 - ë°œí–‰ì‹œê°„: p.txt-time01
ê°œì„ ì‚¬í•­:
- ë™ì‹œì„± ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 
- ë°°ì¹˜ DB ì €ì¥ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
- ì—°ê²° í’€ ìµœì í™”
"""

import asyncio
import re
import sys
import os
from datetime import datetime
import pytz
import httpx
from bs4 import BeautifulSoup
from rich.console import Console
from typing import List, Dict, Optional

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ utils ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.supabase_manager import SupabaseManager

console = Console()


class YonhapPoliticsCollector:
    def __init__(self):
        self.media_name = "ì—°í•©ë‰´ìŠ¤"
        self.base_url = "https://www.yna.co.kr"
        self.list_url = "https://www.yna.co.kr/politics/all"
        self.supabase_manager = SupabaseManager()
        self.articles = []
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ìµœì í™”)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # ë™ì‹œì„± ì œí•œ ì„¤ì •
        self.semaphore = asyncio.Semaphore(10)  # ìµœëŒ€ 10ê°œ ë™ì‹œ ìš”ì²­
        self.batch_size = 20  # DB ë°°ì¹˜ ì €ì¥ í¬ê¸°

    async def run(self, num_pages=10):
        console.print(f"ğŸš€ {self.media_name} ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœì í™” ë²„ì „)")

        # 1ë‹¨ê³„: ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
        await self.collect_articles_parallel(num_pages)
        
        # 2ë‹¨ê³„: ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
        await self.collect_contents_parallel()
        
        # 3ë‹¨ê³„: ë°°ì¹˜ ì €ì¥
        await self.save_articles_batch()

        console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")

    async def collect_articles_parallel(self, num_pages):
        """ëª©ë¡ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        tasks = [self._collect_page_articles(page_num) for page_num in range(1, num_pages + 1)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_articles = 0
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(result)}")
            else:
                total_articles += result
                
        console.print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def _collect_page_articles(self, page_num: int) -> int:
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        url = f"{self.list_url}/{page_num}"
        console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: {url}")

        async with self.semaphore:  # ë™ì‹œì„± ì œí•œ
            try:
                soup = await self._fetch_soup(url)
                if not soup:
                    console.print(f"âš ï¸ í˜ì´ì§€ {page_num} ë¡œë“œ ì‹¤íŒ¨")
                    return 0

                articles = []
                for item in soup.select("div.item-box01")[:15]:  # ê° í˜ì´ì§€ 15ê°œ
                    title_tag = item.select_one("a.tit-news span.title01")
                    link_tag = item.select_one("a.tit-news")

                    if not title_tag or not link_tag:
                        continue

                    title = title_tag.get_text(strip=True)
                    href = link_tag.get("href")
                    article_url = href if href.startswith("http") else self.base_url + href

                    article = {
                        "title": title,
                        "url": article_url,
                        "content": "",
                        "published_at": ""
                    }
                    articles.append(article)
                    console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")

                self.articles.extend(articles)
                console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                return len(articles)

            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return 0

    async def _fetch_soup(self, url: str, max_retries=2) -> Optional[BeautifulSoup]:
        """URLì—ì„œ BeautifulSoup ê°ì²´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í—¬í¼ ë©”ì„œë“œ (ìµœì í™”)"""
        for attempt in range(max_retries):
            try:
                async with httpx.AsyncClient(
                    timeout=15.0,  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)  # ì—°ê²° í’€ ì¦ê°€
                ) as client:
                    response = await client.get(url, headers=self.headers)
                    response.raise_for_status()
                    return BeautifulSoup(response.text, "html.parser")
                    
            except (httpx.HTTPStatusError, httpx.TimeoutException, httpx.ConnectError) as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(0.5)  # ì¬ì‹œë„ ë”œë ˆì´ ë‹¨ì¶•
                else:
                    console.print(f"âŒ {url} ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    return None
            except Exception as e:
                console.print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
                return None

    async def collect_contents_parallel(self):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ + ë°œí–‰ì‹œê°„ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“– ìƒì„¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ ({len(self.articles)}ê°œ) - ë³‘ë ¬ ì²˜ë¦¬")

        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì„œ)
        batch_size = 20  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(self.articles))
            batch_articles = self.articles[start_idx:end_idx]
            
            console.print(f"ğŸ“– ë°°ì¹˜ {batch_num + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [self._collect_single_article(i + start_idx, article) for i, article in enumerate(batch_articles)]
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if batch_num < total_batches - 1:
                await asyncio.sleep(0.5)

    async def _collect_single_article(self, index: int, article: Dict):
        """ë‹¨ì¼ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘"""
        console.print(f"ğŸ“– [{index + 1}/{len(self.articles)}] {article['title'][:40]}...")
        
        async with self.semaphore:  # ë™ì‹œì„± ì œí•œ
            try:
                soup = await self._fetch_soup(article["url"])
                if soup:
                    article["content"] = self.extract_content(soup)
                    article["published_at"] = self.extract_published_at(soup)
                    console.print(f"âœ… [{index + 1}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ: {article['title'][:40]}...")
                else:
                    console.print(f"âŒ [{index + 1}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {article['title'][:40]}...")
                
            except Exception as e:
                console.print(f"âŒ [{index + 1}] ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    def extract_content(self, soup: BeautifulSoup) -> str:
        """ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        article = soup.select_one(".story-news.article")
        if not article:
            return ""

        # ê´‘ê³ /ì‚¬ì§„/aside/ì €ì‘ê¶Œ ì œê±°
        for tag in article.select("aside, figure, div.comp-box, p.txt-copyright"):
            tag.decompose()

        paragraphs = []
        for p in article.find_all("p"):
            text = p.get_text(" ", strip=True)
            if not text:
                continue
            # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
            if "ì €ì‘ê¶Œì" in text or "ë¬´ë‹¨ ì „ì¬" in text:
                continue
            if "ì œë³´ëŠ” ì¹´ì¹´ì˜¤í†¡" in text:
                continue
            if re.match(r"^\[.*\]$", text):
                continue
            if re.search(r"[a-zA-Z0-9._%+-]+@yna\.co\.kr", text):
                continue
            paragraphs.append(text)

        return "\n\n".join(paragraphs).strip()

    def extract_published_at(self, soup: BeautifulSoup) -> str:
        """ì†¡ê³  ì‹œê° UTC ë³€í™˜"""
        tag = soup.select_one("p.txt-time01")
        if not tag:
            return datetime.now(pytz.UTC).isoformat()

        text = tag.get_text(" ", strip=True)
        m = re.search(r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}", text)
        if not m:
            return datetime.now(pytz.UTC).isoformat()

        kst = pytz.timezone("Asia/Seoul")
        dt = datetime.strptime(m.group(), "%Y-%m-%d %H:%M")
        dt = kst.localize(dt)
        return dt.astimezone(pytz.UTC).isoformat()

    async def save_articles_batch(self):
        """DB ë°°ì¹˜ ì €ì¥ (ìµœì í™”)"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ë°°ì¹˜ ì €ì¥ ì¤‘...")

        try:
            media_outlet = self.supabase_manager.get_media_outlet(self.media_name)
            if media_outlet:
                media_id = media_outlet["id"]
            else:
                media_id = self.supabase_manager.create_media_outlet(self.media_name)

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            # ì¤‘ë³µ ì œê±° ë° ë°°ì¹˜ ì¤€ë¹„
            new_articles = []
            skip_count = 0
            
            for article in self.articles:
                if article["url"] in existing_urls:
                    skip_count += 1
                    continue
                    
                article_data = {
                    "title": article["title"],
                    "url": article["url"],
                    "content": article["content"],
                    "published_at": article["published_at"] or datetime.now(pytz.UTC).isoformat(),
                    "created_at": datetime.now(pytz.UTC).isoformat(),
                    "media_id": media_id,
                }
                new_articles.append(article_data)

            # ë°°ì¹˜ ì €ì¥
            if new_articles:
                success_count = self._batch_insert_articles(new_articles)
                console.print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
            else:
                console.print("âš ï¸ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {len(new_articles)}, ìŠ¤í‚µ {skip_count}, ì§§ì€ë³¸ë¬¸ ì œì™¸ {short_content_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

    def _batch_insert_articles(self, articles: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ê¸°ì‚¬ ì‚½ì…"""
        try:
            # Supabaseì˜ upsert ê¸°ëŠ¥ ì‚¬ìš©
            result = self.supabase_manager.client.table("articles").upsert(articles).execute()
            return len(result.data) if result.data else 0
        except Exception as e:
            console.print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
            success_count = 0
            for article in articles:
                try:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                except:
                    continue
            return success_count


async def main():
    collector = YonhapPoliticsCollector()
    await collector.run(num_pages=10)


if __name__ == "__main__":
    asyncio.run(main())
