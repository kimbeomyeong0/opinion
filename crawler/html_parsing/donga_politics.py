#!/usr/bin/env python3
"""
ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
ê°œì„ ì‚¬í•­:
- ë™ì‹œì„± ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 
- ë°°ì¹˜ DB ì €ì¥ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
- httpx ê¸°ë°˜ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ì†ë„ í–¥ìƒ
- ì—°ê²° í’€ ìµœì í™”
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin
import httpx
import pytz
from rich.console import Console
from bs4 import BeautifulSoup
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")


class DongaPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.donga.com"
        self.politics_url = "https://www.donga.com/news/Politics"
        self.media_name = "ë™ì•„ì¼ë³´"
        self.media_bias = "center"
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ìµœì í™”)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # ë™ì‹œì„± ì œí•œ ì„¤ì •
        self.semaphore = asyncio.Semaphore(10)  # ìµœëŒ€ 10ê°œ ë™ì‹œ ìš”ì²­
        self.batch_size = 20  # DB ë°°ì¹˜ ì €ì¥ í¬ê¸°

    def _get_page_urls(self, num_pages: int = 15) -> List[str]:
        """í˜ì´ì§€ URL ëª©ë¡ ìƒì„± (p=1, 11, 21, 31...)"""
        urls = []
        for i in range(num_pages):
            page_num = i * 10 + 1  # 1, 11, 21, 31...
            url = f"{self.politics_url}?p={page_num}&prod=news&ymd=&m="
            urls.append(url)
        return urls

    async def _get_page_articles(self, page_url: str) -> List[Dict]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        console.print(f"ğŸ“¡ í˜ì´ì§€ ìˆ˜ì§‘: {page_url}")
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.get(page_url)
                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                articles = []
                
                # ë™ì•„ì¼ë³´ ì •ì¹˜ ì„¹ì…˜ì˜ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                # ë””ë²„ê¹…ì„ ìœ„í•´ HTML êµ¬ì¡° í™•ì¸
                console.print("ğŸ” HTML êµ¬ì¡° ë¶„ì„ ì¤‘...")
                
                # divide_area ì°¾ê¸°
                divide_area = soup.find('div', class_='divide_area')
                console.print(f"divide_area ë°œê²¬: {divide_area is not None}")
                
                if divide_area:
                    # ì˜¬ë°”ë¥¸ êµ¬ì¡°: divide_area > section.sub_news_sec > ul.row_list > li > article.news_card
                    sub_news_sec = divide_area.find('section', class_='sub_news_sec')
                    console.print(f"sub_news_sec (section) ë°œê²¬: {sub_news_sec is not None}")
                    
                    if sub_news_sec:
                        row_list = sub_news_sec.find('ul', class_='row_list')
                        console.print(f"row_list ë°œê²¬: {row_list is not None}")
                        
                        if row_list:
                            li_items = row_list.find_all('li')
                            console.print(f"ğŸ” row_listì—ì„œ {len(li_items)}ê°œ li ìš”ì†Œ ë°œê²¬")
                            
                            # ê° í˜ì´ì§€ì—ì„œ ìµœëŒ€ 10ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
                            collected_count = 0
                            max_articles_per_page = 10
                            
                            for i, li in enumerate(li_items):
                                if collected_count >= max_articles_per_page:
                                    break
                                    
                                # li ì•ˆì—ì„œ article.news_card ì°¾ê¸°
                                news_card = li.find('article', class_='news_card')
                                console.print(f"li[{i}]ì—ì„œ news_card ë°œê²¬: {news_card is not None}")
                                
                                if news_card:
                                    # news_card ì•ˆì˜ êµ¬ì¡° í™•ì¸
                                    news_head = news_card.find('header', class_='news_head')
                                    news_body = news_card.find('div', class_='news_body')
                                    console.print(f"  news_head: {news_head is not None}, news_body: {news_body is not None}")
                                    
                                    # news_card ì•ˆì—ì„œ ë§í¬ ì°¾ê¸° (news_bodyì˜ .tit aì—ì„œ)
                                    link = None
                                    
                                    # news_bodyì˜ .tit aì—ì„œ ë§í¬ ì°¾ê¸°
                                    if news_body:
                                        tit_link = news_body.find('h4', class_='tit')
                                        if tit_link:
                                            link = tit_link.find('a', href=True)
                                            console.print(f"  .tit aì—ì„œ ë§í¬ ë°œê²¬: {link is not None}")
                                    
                                    # ëŒ€ì•ˆ: news_headì—ì„œ ë§í¬ ì°¾ê¸°
                                    if not link and news_head:
                                        link = news_head.find('a', href=True)
                                        console.print(f"  news_headì—ì„œ ë§í¬ ë°œê²¬: {link is not None}")
                                    
                                    # ëŒ€ì•ˆ: news_cardì—ì„œ ì§ì ‘ ë§í¬ ì°¾ê¸°
                                    if not link:
                                        link = news_card.find('a', href=True)
                                        console.print(f"  news_cardì—ì„œ ì§ì ‘ ë§í¬ ë°œê²¬: {link is not None}")
                                    
                                    if link:
                                        href = link.get('href')
                                        category = link.get('data-ep_button_category')
                                        console.print(f"  ë§í¬ href: {href}")
                                        console.print(f"  ì¹´í…Œê³ ë¦¬: {category}")
                                        
                                        # ì •ì¹˜ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§ (data ì†ì„±ìœ¼ë¡œ)
                                        is_politics = False
                                        
                                        if href and '/news/' in href and '/article/' in href:
                                            # data ì†ì„±ìœ¼ë¡œ ì •ì¹˜ ì¹´í…Œê³ ë¦¬ í™•ì¸
                                            if category == 'ì •ì¹˜':
                                                is_politics = True
                                                console.print(f"  data ì†ì„±ìœ¼ë¡œ ì •ì¹˜ ê¸°ì‚¬ í™•ì¸: {category}")
                                        
                                        if is_politics:
                                            console.print(f"  ì •ì¹˜ ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ ë°œê²¬!")
                                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                                            if href.startswith('/'):
                                                full_url = urljoin(self.base_url, href)
                                            else:
                                                full_url = href
                                            
                                            # ì œëª© ì¶”ì¶œ (data-ep_button_name ì†ì„± ìš°ì„  ì‚¬ìš©)
                                            title = link.get('data-ep_button_name', '').strip()
                                            console.print(f"  ì œëª© ì¶”ì¶œ ì‹œë„ 1 - data-ep_button_name: '{title}'")
                                            
                                            # ëŒ€ì•ˆ: data-ep_contentdata_content_title ì†ì„±
                                            if not title:
                                                title = link.get('data-ep_contentdata_content_title', '').strip()
                                                console.print(f"  ì œëª© ì¶”ì¶œ ì‹œë„ 2 - data-ep_contentdata_content_title: '{title}'")
                                            
                                            # ëŒ€ì•ˆ: a íƒœê·¸ì˜ ì§ì ‘ í…ìŠ¤íŠ¸ ë…¸ë“œ
                                            if not title:
                                                title_text = link.find(text=True, recursive=False)
                                                if title_text:
                                                    title = title_text.strip()
                                                console.print(f"  a íƒœê·¸ ì§ì ‘ í…ìŠ¤íŠ¸: '{title}'")
                                            
                                            # ëŒ€ì•ˆ: img íƒœê·¸ì˜ alt ì†ì„±
                                            if not title:
                                                img_tag = link.find('img')
                                                if img_tag:
                                                    title = img_tag.get('alt', '').strip()
                                                console.print(f"  img alt ì†ì„±: '{title}'")
                                            
                                            console.print(f"  ìµœì¢… ì œëª©: '{title[:50]}...'")
                                            
                                            if title and len(title) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                                                articles.append({
                                                    'title': title,
                                                    'url': full_url
                                                })
                                                collected_count += 1
                                                console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬ [{collected_count}]: {title[:50]}...")
                                            else:
                                                if i == 0:
                                                    console.print(f"  ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ: '{title}'")
                                else:
                                    console.print(f"  li[{i}]ì—ì„œ news_cardë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    console.print("âš ï¸ divide_areaë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    # ì „ì²´ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
                    links = soup.find_all('a', href=True)
                    news_links = [link for link in links if link.get('href') and '/news/article/' in link.get('href')]
                    console.print(f"ì „ì²´ í˜ì´ì§€ì—ì„œ {len(news_links)}ê°œ ë‰´ìŠ¤ ë§í¬ ë°œê²¬")
                    
                    for link in news_links:
                        href = link.get('href')
                        if href.startswith('/'):
                            full_url = urljoin(self.base_url, href)
                        else:
                            full_url = href
                        
                        title = link.get_text(strip=True)
                        if title and len(title) > 10:
                            articles.append({
                                'title': title,
                                'url': full_url
                            })
                            console.print(f"ğŸ“° ê¸°ì‚¬ ë°œê²¬: {title[:50]}...")
                            break
                
                console.print(f"ğŸ“Š í˜ì´ì§€ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                return articles
                
            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                return []

    async def collect_articles_parallel(self, num_pages: int = 15):
        """ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        page_urls = self._get_page_urls(num_pages)
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        tasks = [self._collect_page_articles_parallel(page_url, i + 1) for i, page_url in enumerate(page_urls)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_articles = 0
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(result)}")
            else:
                total_articles += result
                
        console.print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def _collect_page_articles_parallel(self, page_url: str, page_num: int) -> int:
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: {page_url}")

        async with self.semaphore:  # ë™ì‹œì„± ì œí•œ
            try:
                async with httpx.AsyncClient(
                    timeout=15.0,
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
                ) as client:
                    response = await client.get(page_url, headers=self.headers)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    articles = []
                    
                    # ë™ì•„ì¼ë³´ ì •ì¹˜ ì„¹ì…˜ì˜ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                    divide_area = soup.find('div', class_='divide_area')
                    
                    if divide_area:
                        sub_news_sec = divide_area.find('section', class_='sub_news_sec')
                        if sub_news_sec:
                            row_list = sub_news_sec.find('ul', class_='row_list')
                            if row_list:
                                li_items = row_list.find_all('li')
                                
                                # ê° í˜ì´ì§€ì—ì„œ ìµœëŒ€ 10ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
                                collected_count = 0
                                max_articles_per_page = 10
                                
                                for i, li in enumerate(li_items):
                                    if collected_count >= max_articles_per_page:
                                        break
                                        
                                    news_card = li.find('article', class_='news_card')
                                    
                                    if news_card:
                                        # ë§í¬ ì°¾ê¸°
                                        link = None
                                        
                                        # news_bodyì˜ .tit aì—ì„œ ë§í¬ ì°¾ê¸°
                                        news_body = news_card.find('div', class_='news_body')
                                        if news_body:
                                            tit_link = news_body.find('h4', class_='tit')
                                            if tit_link:
                                                link = tit_link.find('a', href=True)
                                        
                                        # ëŒ€ì•ˆ: news_headì—ì„œ ë§í¬ ì°¾ê¸°
                                        if not link:
                                            news_head = news_card.find('header', class_='news_head')
                                            if news_head:
                                                link = news_head.find('a', href=True)
                                        
                                        # ëŒ€ì•ˆ: news_cardì—ì„œ ì§ì ‘ ë§í¬ ì°¾ê¸°
                                        if not link:
                                            link = news_card.find('a', href=True)
                                        
                                        if link:
                                            href = link.get('href')
                                            category = link.get('data-ep_button_category')
                                            
                                            # ì •ì¹˜ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
                                            is_politics = False
                                            if href and '/news/' in href and '/article/' in href:
                                                if category == 'ì •ì¹˜':
                                                    is_politics = True
                                            
                                            if is_politics:
                                                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                                                if href.startswith('/'):
                                                    full_url = urljoin(self.base_url, href)
                                                else:
                                                    full_url = href
                                                
                                                # ì œëª© ì¶”ì¶œ
                                                title = link.get('data-ep_button_name', '').strip()
                                                if not title:
                                                    title = link.get('data-ep_contentdata_content_title', '').strip()
                                                if not title:
                                                    title_text = link.find(text=True, recursive=False)
                                                    if title_text:
                                                        title = title_text.strip()
                                                if not title:
                                                    img_tag = link.find('img')
                                                    if img_tag:
                                                        title = img_tag.get('alt', '').strip()
                                                
                                                if title and len(title) > 10:
                                                    article = {
                                                        'title': title,
                                                        'url': full_url,
                                                        'content': '',
                                                        'published_at': ''
                                                    }
                                                    articles.append(article)
                                                    collected_count += 1
                                                    console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")
                    else:
                        # ì „ì²´ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
                        links = soup.find_all('a', href=True)
                        news_links = [link for link in links if link.get('href') and '/news/article/' in link.get('href')]
                        
                        for link in news_links:
                            href = link.get('href')
                            if href.startswith('/'):
                                full_url = urljoin(self.base_url, href)
                            else:
                                full_url = href
                            
                            title = link.get_text(strip=True)
                            if title and len(title) > 10:
                                article = {
                                    'title': title,
                                    'url': full_url,
                                    'content': '',
                                    'published_at': ''
                                }
                                articles.append(article)
                                console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")
                                break
                    
                    self.articles.extend(articles)
                    console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    return len(articles)

            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return 0

    def _parse_article_data(self, article_data: Dict) -> Optional[Dict]:
        """ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±"""
        try:
            title = article_data.get("title")
            url = article_data.get("url")
            content_data = article_data.get("content_data", {})
            
            if not title or not url:
                return None

            # ë°œí–‰ ì‹œê°„ íŒŒì‹± ë° UTC ë³€í™˜
            published_at = None
            if content_data.get("published_at"):
                try:
                    # KST ì‹œê°„ íŒŒì‹±
                    kst_time = datetime.strptime(content_data["published_at"], "%Y-%m-%d %H:%M")
                    # KST íƒ€ì„ì¡´ ì ìš©
                    kst_tz = pytz.timezone("Asia/Seoul")
                    kst_dt = kst_tz.localize(kst_time)
                    # UTCë¡œ ë³€í™˜
                    published_at = kst_dt.astimezone(pytz.UTC).isoformat()
                except Exception as e:
                    console.print(f"âš ï¸ ë°œí–‰ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    published_at = None

            return {
                "title": title.strip(),
                "url": url,
                "content": content_data.get("content", ""),
                "published_at": published_at,
                "created_at": datetime.now(KST).isoformat(),
                "author": "",  # ë‚˜ì¤‘ì— ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ
                "section": "ì •ì¹˜",
                "tags": [],
                "description": "",
            }
        except Exception as e:
            console.print(f"âŒ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    async def _extract_content(self, url: str) -> Dict[str, str]:
        """Playwrightë¡œ ë³¸ë¬¸ ì „ë¬¸ ì¶”ì¶œ"""
        page = None
        try:
            if not self._browser:
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox', 
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--memory-pressure-off'
                    ]
                )

            page = await self._browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 720})
            await page.goto(url, wait_until="domcontentloaded", timeout=10000)

            # ë™ì•„ì¼ë³´ ë³¸ë¬¸ ë° ë°œí–‰ ì‹œê°„ ì¶”ì¶œ
            result = {"content": "", "published_at": ""}
            
            try:
                result = await page.evaluate('''() => {
                    // <section class="news_view"> ì°¾ê¸°
                    const newsView = document.querySelector('section.news_view');
                    if (!newsView) {
                        return {content: '', published_at: ''};
                    }
                    
                    // ë°œí–‰ ì‹œê°„ ì¶”ì¶œ (<span aria-hidden="true">2025-09-04 15:33</span>)
                    let publishedAt = '';
                    const timeSpan = document.querySelector('span[aria-hidden="true"]');
                    if (timeSpan) {
                        const timeText = timeSpan.textContent.trim();
                        // YYYY-MM-DD HH:MM í˜•ì‹ì¸ì§€ í™•ì¸
                        if (/^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}$/.test(timeText)) {
                            publishedAt = timeText;
                        }
                    }
                    
                    // ì œì™¸í•  ìš”ì†Œë“¤ ì œê±°
                    const excludeSelectors = [
                        'h2.sub_tit',  // ê¸°ì‚¬ ì œëª©
                        'figure',      // ì´ë¯¸ì§€ ê´€ë ¨
                        'img',         // ì´ë¯¸ì§€
                        'figcaption',  // ì´ë¯¸ì§€ ìº¡ì…˜
                        '.view_m_adK', // ê´‘ê³  ì˜ì—­
                        '.view_ad06',  // ê´‘ê³  ì˜ì—­
                        '.view_m_adA', // ê´‘ê³  ì˜ì—­
                        '.view_m_adB', // ê´‘ê³  ì˜ì—­
                        '.a1'          // ê´‘ê³  ì˜ì—­
                    ];
                    
                    // ì œì™¸í•  ìš”ì†Œë“¤ ì œê±°
                    excludeSelectors.forEach(selector => {
                        const elements = newsView.querySelectorAll(selector);
                        elements.forEach(el => el.remove());
                    });
                    
                    // í…ìŠ¤íŠ¸ ë…¸ë“œë“¤ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜ì§‘
                    const textNodes = [];
                    const walker = document.createTreeWalker(
                        newsView,
                        NodeFilter.SHOW_TEXT,
                        {
                            acceptNode: function(node) {
                                const text = node.textContent.trim();
                                if (text.length === 0) {
                                    return NodeFilter.FILTER_REJECT;
                                }
                                return NodeFilter.FILTER_ACCEPT;
                            }
                        }
                    );
                    
                    let node;
                    while (node = walker.nextNode()) {
                        textNodes.push(node.textContent.trim());
                    }
                    
                    // <br> íƒœê·¸ë¥¼ ë¬¸ë‹¨ êµ¬ë¶„ìë¡œ ì²˜ë¦¬í•˜ê³  í…ìŠ¤íŠ¸ ì—°ê²°
                    let content = textNodes.join(' ').replace(/\\s+/g, ' ').trim();
                    
                    // <br> íƒœê·¸ë¥¼ ë¬¸ë‹¨ êµ¬ë¶„ìë¡œ ë³€í™˜
                    content = content.replace(/<br\\s*\\/?>/gi, '\\n\\n');
                    
                    // ì—°ì†ëœ ê³µë°± ì •ë¦¬
                    content = content.replace(/\\s+/g, ' ').trim();
                    
                    return {content: content, published_at: publishedAt};
                }''')
                
                if result.get("content") and len(result["content"].strip()) > 50:
                    return result
                    
            except Exception as e:
                console.print(f"âš ï¸ JavaScript ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:50]}")
            
            return result
            
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url[:50]}...): {str(e)[:50]}")
            return {"content": "", "published_at": ""}
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass

    async def collect_contents_parallel(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì„œ)
        batch_size = 20  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(self.articles))
            batch_articles = self.articles[start_idx:end_idx]
            
            console.print(f"ğŸ“– ë°°ì¹˜ {batch_num + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            async with httpx.AsyncClient(
                timeout=15.0,
                follow_redirects=True,
                limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
            ) as client:
                tasks = [self._extract_content_httpx(client, article, i + start_idx + 1) for i, article in enumerate(batch_articles)]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if batch_num < total_batches - 1:
                await asyncio.sleep(0.5)

    async def _extract_content_httpx(self, client: httpx.AsyncClient, article: dict, index: int):
        """httpxë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ë° ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
            
            response = await client.get(article["url"], headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë°œí–‰ì‹œê°„ ì¶”ì¶œ
            published_at = self._extract_published_at(soup)
            article["published_at"] = published_at
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content_text(soup)
            article["content"] = content
            
            console.print(f"âœ… [{index}] ì™„ë£Œ: {len(content)}ì")
            
        except Exception as e:
            console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
            article["content"] = ""
            article["published_at"] = datetime.now(pytz.UTC).isoformat()

    def _extract_published_at(self, soup: BeautifulSoup) -> str:
        """ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            # <span aria-hidden="true">2025-09-04 15:33</span> í˜•ì‹ ì°¾ê¸°
            time_span = soup.select_one('span[aria-hidden="true"]')
            if time_span:
                time_text = time_span.get_text(strip=True)
                if re.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}$', time_text):
                    return self._parse_datetime(time_text)
            
            # ë‹¤ë¥¸ ì‹œê°„ ì„ íƒìë“¤ ì‹œë„
            time_selectors = [
                'time[datetime]',
                '.date',
                '.time',
                '.publish_date',
                '.article_date'
            ]
            
            for selector in time_selectors:
                element = soup.select_one(selector)
                if element:
                    datetime_attr = element.get('datetime')
                    if datetime_attr:
                        return self._parse_datetime(datetime_attr)
                    
                    text = element.get_text(strip=True)
                    if text and re.match(r'\d{4}-\d{2}-\d{2}', text):
                        return self._parse_datetime(text)
            
            return datetime.now(pytz.UTC).isoformat()
            
        except Exception as e:
            console.print(f"âš ï¸ ë°œí–‰ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return datetime.now(pytz.UTC).isoformat()

    def _extract_content_text(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            # <section class="news_view"> ì°¾ê¸°
            news_view = soup.find('section', class_='news_view')
            if not news_view:
                # ëŒ€ì•ˆ: .view_body ì°¾ê¸°
                news_view = soup.find('div', class_='view_body')
                if not news_view:
                    return ""
            
            # ì œì™¸í•  ìš”ì†Œë“¤ ì œê±°
            exclude_selectors = [
                'h2.sub_tit',  # ê¸°ì‚¬ ì œëª©
                'figure',      # ì´ë¯¸ì§€ ê´€ë ¨
                'img',         # ì´ë¯¸ì§€
                'figcaption',  # ì´ë¯¸ì§€ ìº¡ì…˜
                '.view_m_adK', # ê´‘ê³  ì˜ì—­
                '.view_ad06',  # ê´‘ê³  ì˜ì—­
                '.view_m_adA', # ê´‘ê³  ì˜ì—­
                '.view_m_adB', # ê´‘ê³  ì˜ì—­
                '.a1',         # ê´‘ê³  ì˜ì—­
                '.view_series', # ê´€ë ¨ ê¸°ì‚¬
                '.view_trend',  # íŠ¸ë Œë“œ ë‰´ìŠ¤
                'script',      # ìŠ¤í¬ë¦½íŠ¸
                'style'        # ìŠ¤íƒ€ì¼
            ]
            
            for selector in exclude_selectors:
                elements = news_view.select(selector)
                for el in elements:
                    el.decompose()
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ë” ì •í™•í•œ ë°©ë²•
            content_parts = []
            
            # 1. <p> íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            paragraphs = news_view.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 20:  # 20ì ì´ìƒì¸ ë¬¸ë‹¨ë§Œ
                    content_parts.append(text)
            
            # 2. <p> íƒœê·¸ê°€ ì—†ìœ¼ë©´ <div>ì—ì„œ ì¶”ì¶œ
            if not content_parts:
                divs = news_view.find_all('div')
                for div in divs:
                    text = div.get_text(strip=True)
                    if text and len(text) > 50:  # 50ì ì´ìƒì¸ divë§Œ
                        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì´ë¯¸ í¬í•¨ëœ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                        is_duplicate = any(text in existing for existing in content_parts)
                        if not is_duplicate:
                            content_parts.append(text)
            
            # 3. í…ìŠ¤íŠ¸ ì—°ê²° ë° ì •ë¦¬
            if content_parts:
                content = ' '.join(content_parts)
                # ì—°ì†ëœ ê³µë°± ì •ë¦¬
                content = re.sub(r'\s+', ' ', content).strip()
                return content
            else:
                # 4. ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                full_text = news_view.get_text(strip=True)
                if len(full_text) > 100:
                    return re.sub(r'\s+', ' ', full_text).strip()
                
            return ""
            
        except Exception as e:
            console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return ""

    def _parse_datetime(self, datetime_str: str) -> str:
        """ë‚ ì§œì‹œê°„ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            clean_time = datetime_str.strip()
            
            if re.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}$', clean_time):
                # "YYYY-MM-DD HH:MM" í˜•ì‹ì¸ ê²½ìš° (KST ê¸°ì¤€)
                kst_time = datetime.strptime(clean_time, "%Y-%m-%d %H:%M")
                kst_tz = pytz.timezone("Asia/Seoul")
                kst_dt = kst_tz.localize(kst_time)
                return kst_dt.astimezone(pytz.UTC).isoformat()
            elif 'T' in clean_time:
                # ISO í˜•ì‹
                if '+' in clean_time:
                    published_at = datetime.fromisoformat(clean_time)
                    return published_at.astimezone(pytz.UTC).isoformat()
                else:
                    published_at = datetime.fromisoformat(clean_time.replace('Z', '+00:00'))
                    return published_at.isoformat()
            else:
                return datetime.now(pytz.UTC).isoformat()
                
        except Exception as e:
            console.print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {clean_time} - {str(e)}")
            return datetime.now(pytz.UTC).isoformat()

    async def save_articles_batch(self):
        """DB ë°°ì¹˜ ì €ì¥ (ìµœì í™”)"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ë°°ì¹˜ ì €ì¥ ì¤‘...")

        try:
            # ì–¸ë¡ ì‚¬ í™•ì¸
            media = self.supabase_manager.get_media_outlet(self.media_name)
            if not media:
                media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
            else:
                media_id = media["id"]

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            # ì¤‘ë³µ ì œê±° ë° ë°°ì¹˜ ì¤€ë¹„
            new_articles = []
            skip_count = 0
            short_content_count = 0
            
            for article in self.articles:
                if article["url"] in existing_urls:
                    skip_count += 1
                    continue
                
                # ë³¸ë¬¸ ê¸¸ì´ ì²´í¬ (20ì ë¯¸ë§Œ ì œì™¸)
                content = article.get('content', '')
                if len(content.strip()) < 20:
                    short_content_count += 1
                    continue
                    
                # ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
                parsed_article = self._parse_article_data_simple(article, media_id)
                if parsed_article:
                    new_articles.append(parsed_article)

            # ë°°ì¹˜ ì €ì¥
            if new_articles:
                success_count = self._batch_insert_articles(new_articles)
                console.print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
            else:
                console.print("âš ï¸ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {len(new_articles)}, ìŠ¤í‚µ {skip_count}, ì§§ì€ë³¸ë¬¸ ì œì™¸ {short_content_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

    def _parse_article_data_simple(self, article: dict, media_id: str) -> Optional[dict]:
        """ê¸°ì‚¬ ë°ì´í„° ê°„ë‹¨ íŒŒì‹± (ë°°ì¹˜ ì €ì¥ìš©)"""
        try:
            return {
                'title': article['title'],
                'url': article['url'],
                'content': article.get('content', ''),
                'published_at': article.get('published_at', datetime.now(pytz.UTC).isoformat()),
                'created_at': datetime.now(pytz.UTC).isoformat(),
                'media_id': media_id
            }
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def _batch_insert_articles(self, articles: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ê¸°ì‚¬ ì‚½ì…"""
        try:
            # Supabaseì˜ upsert ê¸°ëŠ¥ ì‚¬ìš©
            result = self.supabase_manager.client.table("articles").upsert(articles).execute()
            return len(result.data) if result.data else 0
        except Exception as e:
            console.print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
            success_count = 0
            for article in articles:
                try:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                except:
                    continue
            return success_count

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self._browser:
                await self._browser.close()
                self._browser = None
            if self._playwright:
                await self._playwright.stop()
                self._playwright = None
            console.print("ğŸ§¹ Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            console.print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}")

    async def run(self, num_pages: int = 15):
        """ì‹¤í–‰ (ìµœì í™” ë²„ì „)"""
        try:
            console.print(f"ğŸš€ ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœì í™” ë²„ì „, ìµœëŒ€ {num_pages}í˜ì´ì§€)")
            
            # 1. ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_articles_parallel(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_contents_parallel()
            
            # 3. ê¸°ì‚¬ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
            await self.save_articles_batch()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def main():
    collector = DongaPoliticsCollector()
    await collector.run(num_pages=15)  # 15í˜ì´ì§€ì—ì„œ ê°ê° 10ê°œì”© ì´ 150ê°œ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())