#!/usr/bin/env python3
"""
í•œê²¨ë ˆ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬
- HTML íŒŒì‹±ìœ¼ë¡œ ê¸°ì‚¬ ëª©ë¡ ë° ë³¸ë¬¸ ìˆ˜ì§‘
- í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›
- BeautifulSoupì„ ì‚¬ìš©í•œ HTML íŒŒì‹±
"""

import sys
import os
import re
import time
import requests
from datetime import datetime
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import html

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from utils.supabase_manager import SupabaseManager

class HaniPoliticsCrawler:
    """í•œê²¨ë ˆ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.base_url = "https://www.hani.co.kr"
        self.politics_url = "https://www.hani.co.kr/arti/politics"
        
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise Exception("Supabase ì—°ê²° ì‹¤íŒ¨")
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def get_media_outlet_id(self) -> Optional[int]:
        """í•œê²¨ë ˆ ì–¸ë¡ ì‚¬ ID ì¡°íšŒ"""
        try:
            result = self.supabase_manager.client.table('media_outlets').select('id').eq('name', 'í•œê²¨ë ˆ').execute()
            if result.data:
                return result.data[0]['id']
            return None
        except Exception as e:
            print(f"âŒ ì–¸ë¡ ì‚¬ ID ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def fetch_articles_page(self, page: int) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • í˜ì´ì§€ì˜ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ (HTML íŒŒì‹±)
        
        Args:
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            
        Returns:
            List[Dict]: ê¸°ì‚¬ ëª©ë¡
        """
        try:
            # í˜ì´ì§€ URL êµ¬ì„±
            if page == 1:
                url = self.politics_url
            else:
                url = f"{self.politics_url}?page={page}"
            
            print(f"ğŸ“¡ í˜ì´ì§€ {page} ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ ì¤‘: {url}")
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
            articles = []
            
            # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            article_items = soup.select('.ArticleList_item___OGQO')
            
            for item in article_items:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_element = item.select_one('.BaseArticleCard_title__TVFqt')
                    title = title_element.get_text(strip=True) if title_element else ""
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_element = item.select_one('.BaseArticleCard_link__Q3YFK')
                    article_url = ""
                    if link_element and link_element.get('href'):
                        href = link_element.get('href')
                        if href.startswith('/'):
                            article_url = urljoin(self.base_url, href)
                        else:
                            article_url = href
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    date_element = item.select_one('.BaseArticleCard_date__4R8Ru')
                    published_at = ""
                    if date_element:
                        date_text = date_element.get_text(strip=True)
                        try:
                            # "2025-09-18 15:15" í˜•ì‹ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                            dt = datetime.strptime(date_text, '%Y-%m-%d %H:%M')
                            published_at = dt.isoformat() + '+09:00'
                        except ValueError:
                            published_at = ""
                    
                    if title and article_url:
                        article = {
                            'title': title,
                            'url': article_url,
                            'published_at': published_at,
                            'media_id': self.get_media_outlet_id()
                        }
                        articles.append(article)
                        print(f"ğŸ“° ë°œê²¬: {title}")
                        
                except Exception as e:
                    print(f"âš ï¸ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    continue
            
            print(f"âœ… í˜ì´ì§€ {page}: {len(articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ")
            return articles
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page} ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ë¦¬
        
        Args:
            text: ì •ë¦¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = html.unescape(text)
        
        # <br> íƒœê·¸ë¥¼ \nìœ¼ë¡œ ë³€í™˜
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    def extract_article_content(self, article_url: str) -> str:
        """
        ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (HTML íŒŒì‹±)
        
        Args:
            article_url: ê¸°ì‚¬ URL
            
        Returns:
            str: ì¶”ì¶œëœ ë³¸ë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {article_url}")
            
            response = self.session.get(article_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # div.article-text ì°¾ê¸°
            article_text_div = soup.find('div', class_='article-text')
            if not article_text_div:
                print(f"âš ï¸ article-text divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {article_url}")
                return ""
            
            # ì œê±°í•  ìš”ì†Œë“¤ (ë¶ˆí•„ìš”í•œ ê´‘ê³ , ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ë“±)
            unwanted_selectors = [
                '[class*="ArticleDetailAudioPlayer"]',
                '[class*="ArticleDetailContent_adWrap"]',
                '[class*="ArticleDetailContent_adFlex"]',
                '[class*="BaseAd_"]',
                'figure',
                'script',
                'style',
                'noscript',
                'iframe',
                'img'  # ì´ë¯¸ì§€ ì œê±°
            ]
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for selector in unwanted_selectors:
                for element in article_text_div.select(selector):
                    element.decompose()
            
            # p.text ìš”ì†Œë“¤ ì¶”ì¶œ
            paragraphs = []
            for p in article_text_div.find_all('p', class_='text'):
                text = self.clean_text(p.get_text())
                if text and text.strip():  # ê³µë°±ë§Œ ìˆëŠ” ë‹¨ë½ ì œê±°
                    # ê¸°ì ì •ë³´ ì œê±° (ì´ë©”ì¼ í¬í•¨ëœ ë¬¸ë‹¨)
                    if '@' in text and ('ê¸°ì' in text or 'reporter' in text.lower()):
                        continue
                    paragraphs.append(text.strip())
            
            # ë‹¨ë½ë“¤ì„ \n\në¡œ ì—°ê²°
            content = '\n\n'.join(paragraphs)
            
            print(f"âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ: {len(content)}ì")
            return content
            
        except Exception as e:
            print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {article_url} - {str(e)}")
            return ""
    
    def process_article(self, article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ê°œë³„ ê¸°ì‚¬ ì²˜ë¦¬
        
        Args:
            article: ê¸°ì‚¬ ë°ì´í„°
            
        Returns:
            Optional[Dict]: ì²˜ë¦¬ëœ ê¸°ì‚¬ ë°ì´í„°
        """
        try:
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self.extract_article_content(article['url'])
            if not content:
                print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ê±´ë„ˆëœ€: {article['title']}")
                return None
            
            # ìµœì¢… ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
            processed_article = {
                'title': article['title'],
                'content': content,
                'url': article['url'],
                'published_at': article['published_at'],
                'media_id': article['media_id'],
                'is_preprocessed': False  # ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ìƒíƒœë¡œ ì €ì¥
            }
            
            return processed_article
            
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {article['title']} - {str(e)}")
            return None
    
    def crawl_articles(self, max_pages: int = 5) -> List[Dict[str, Any]]:
        """
        ê¸°ì‚¬ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            List[Dict]: í¬ë¡¤ë§ëœ ê¸°ì‚¬ ëª©ë¡
        """
        try:
            print(f"ğŸš€ í•œê²¨ë ˆ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ë§ ì‹œì‘... (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
            
            all_articles = []
            
            for page in range(1, max_pages + 1):
                # í˜ì´ì§€ë³„ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ
                articles = self.fetch_articles_page(page)
                if not articles:
                    print(f"ğŸ“ í˜ì´ì§€ {page}ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ê° ê¸°ì‚¬ ì²˜ë¦¬
                for article in articles:
                    processed_article = self.process_article(article)
                    if processed_article:
                        all_articles.append(processed_article)
                
                # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                time.sleep(1)
            
            print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ ê¸°ì‚¬")
            return all_articles
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def save_articles(self, articles: List[Dict[str, Any]]) -> int:
        """
        ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        
        Args:
            articles: ì €ì¥í•  ê¸°ì‚¬ ëª©ë¡
            
        Returns:
            int: ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜
        """
        if not articles:
            print("ğŸ“ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        try:
            print(f"ğŸ’¾ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
            
            success_count = 0
            short_content_count = 0
            
            for article in articles:
                # ë³¸ë¬¸ ê¸¸ì´ ì²´í¬ (20ì ë¯¸ë§Œ ì œì™¸)
                content = article.get('content', '')
                if len(content.strip()) < 20:
                    short_content_count += 1
                    print(f"âš ï¸ ì§§ì€ ë³¸ë¬¸ ì œì™¸: {article.get('title', '')[:30]}...")
                    continue
                
                if self.supabase_manager.insert_article(article):
                    success_count += 1
            
            print(f"âœ… {success_count}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
            if short_content_count > 0:
                print(f"ğŸ“ ì§§ì€ë³¸ë¬¸ ì œì™¸: {short_content_count}ê°œ")
            return success_count
            
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return 0

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ“° í•œê²¨ë ˆ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬")
    print("=" * 60)
    
    try:
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = HaniPoliticsCrawler()
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        articles = crawler.crawl_articles(max_pages=10)
        
        if articles:
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            saved_count = crawler.save_articles(articles)
            print(f"\nğŸ‰ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!")
            print(f"ğŸ“Š í¬ë¡¤ë§: {len(articles)}ê°œ, ì €ì¥: {saved_count}ê°œ")
        else:
            print("\nâŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
