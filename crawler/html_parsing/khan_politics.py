#!/usr/bin/env python3
"""
ê²½í–¥ì‹ ë¬¸ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (HTML íŒŒì‹± ê¸°ë°˜)
- HTML íŒŒì‹±ìœ¼ë¡œ ì™„ì „ ì¬ì‘ì„±í•˜ì—¬ ì†ë„ ëŒ€í­ ê°œì„ 
- httpx + BeautifulSoup ì‚¬ìš©
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
"""

import asyncio
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import httpx
import pytz
from bs4 import BeautifulSoup
import re
from rich.console import Console

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")

class KhanPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.khan.co.kr"
        self.politics_url = "https://www.khan.co.kr/politics"
        self.media_name = "ê²½í–¥ì‹ ë¬¸"
        self.media_bias = "center-left"
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # ë™ì‹œì„± ì œí•œ ì„¤ì •
        self.semaphore = asyncio.Semaphore(10)
    def _get_page_urls(self, num_pages: int = 15) -> List[str]:
        """í˜ì´ì§€ URL ëª©ë¡ ìƒì„±"""
        urls = []
        for page in range(1, num_pages + 1):
            url = f"{self.politics_url}?page={page}"
            urls.append(url)
        return urls

    async def _get_page_articles(self, page_url: str, page_num: int) -> List[Dict]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
        console.print(f"ğŸ“¡ í˜ì´ì§€ {page_num}: HTML íŒŒì‹± ì¤‘...")

        async with self.semaphore:
            try:
                async with httpx.AsyncClient(
                    timeout=15.0,
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
                ) as client:
                    response = await client.get(page_url, headers=self.headers)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    articles = []
                    
                    # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ (ul#recentList li article)
                    recent_list = soup.find('ul', id='recentList')
                    if not recent_list:
                        console.print(f"âŒ í˜ì´ì§€ {page_num}: recentListë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        return []
                    
                    article_items = recent_list.find_all('li')
                    
                    for li in article_items:
                        article_element = li.find('article')
                        if not article_element:
                            continue
                            
                        try:
                            # ì œëª©ê³¼ URL ì¶”ì¶œ (div > a)
                            link = article_element.find('a', href=True)
                            if not link:
                                continue
                                
                            href = link.get('href')
                            if not href:
                                continue
                            
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if href.startswith('http'):
                                full_url = href
                            else:
                                full_url = urljoin(self.base_url, href)
                            
                            title = link.get_text(strip=True)
                            
                            # ìš”ì•½ ì¶”ì¶œ (p.desc)
                            desc_element = article_element.find('p', class_='desc')
                            description = desc_element.get_text(strip=True) if desc_element else ""
                            
                            # ë‚ ì§œ ì¶”ì¶œ (p.date)
                            date_element = article_element.find('p', class_='date')
                            date_text = date_element.get_text(strip=True) if date_element else ""
                            published_at = self._parse_relative_time(date_text)
                            
                            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ (ì„ íƒì )
                            img_element = article_element.find('img')
                            image_url = ""
                            image_alt = ""
                            if img_element:
                                image_url = img_element.get('src', '')
                                image_alt = img_element.get('alt', '')
                                if image_url and not image_url.startswith('http'):
                                    image_url = urljoin(self.base_url, image_url)
                            
                            if title and len(title) > 10:
                                article = {
                                    'title': title,
                                    'url': full_url,
                                    'content': '',
                                    'published_at': published_at,
                                    'description': description,
                                    'image_url': image_url,
                                    'image_alt': image_alt
                                }
                                articles.append(article)
                                console.print(f"ğŸ“° ë°œê²¬: {title[:50]}...")
                        
                        except Exception as e:
                            console.print(f"âš ï¸ ê¸°ì‚¬ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            continue
                    
                    console.print(f"ğŸ“„ í˜ì´ì§€ {page_num}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    return articles
                    
            except Exception as e:
                console.print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return []

    def _parse_relative_time(self, time_text: str) -> str:
        """ìƒëŒ€ ì‹œê°„ í…ìŠ¤íŠ¸ë¥¼ UTC ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if not time_text:
                return datetime.now(pytz.UTC).isoformat()
            
            now = datetime.now(KST)
            
            # "26ë¶„ ì „", "2ì‹œê°„ ì „", "1ì¼ ì „" ë“± ì²˜ë¦¬
            if "ë¶„ ì „" in time_text:
                minutes = int(re.search(r'(\d+)ë¶„', time_text).group(1))
                target_time = now - timedelta(minutes=minutes)
            elif "ì‹œê°„ ì „" in time_text:
                hours = int(re.search(r'(\d+)ì‹œê°„', time_text).group(1))
                target_time = now - timedelta(hours=hours)
            elif "ì¼ ì „" in time_text:
                days = int(re.search(r'(\d+)ì¼', time_text).group(1))
                target_time = now - timedelta(days=days)
            elif "ì£¼ ì „" in time_text:
                weeks = int(re.search(r'(\d+)ì£¼', time_text).group(1))
                target_time = now - timedelta(weeks=weeks)
            else:
                # ì ˆëŒ€ ì‹œê°„ í˜•ì‹ ì‹œë„ (YYYY.MM.DD HH:MM)
                if re.match(r'\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}', time_text):
                    target_time = datetime.strptime(time_text, "%Y.%m.%d %H:%M")
                    target_time = KST.localize(target_time)
                else:
                    target_time = now
            
            return target_time.astimezone(pytz.UTC).isoformat()
            
        except Exception as e:
            console.print(f"âš ï¸ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {time_text} - {str(e)}")
            return datetime.now(pytz.UTC).isoformat()

    async def collect_articles_parallel(self, num_pages: int = 15):
        """ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        console.print(f"ğŸ“„ {num_pages}ê°œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        page_urls = self._get_page_urls(num_pages)
        
        # ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬
        tasks = [self._get_page_articles(url, i + 1) for i, url in enumerate(page_urls)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_articles = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                console.print(f"âŒ í˜ì´ì§€ {i + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(result)}")
            else:
                self.articles.extend(result)
                total_articles += len(result)
                
        console.print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    async def collect_contents_parallel(self):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        if not self.articles:
            return
            
        console.print(f"ğŸ“– {len(self.articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
        
        # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì„œ)
        batch_size = 20
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(self.articles))
            batch_articles = self.articles[start_idx:end_idx]
            
            console.print(f"ğŸ“– ë°°ì¹˜ {batch_num + 1}/{total_batches}: {len(batch_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            async with httpx.AsyncClient(
                timeout=15.0,
                follow_redirects=True,
                limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
            ) as client:
                tasks = [self._extract_content_httpx(client, article, i + start_idx + 1) for i, article in enumerate(batch_articles)]
                await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ë”œë ˆì´
            if batch_num < total_batches - 1:
                await asyncio.sleep(0.5)

    async def _extract_content_httpx(self, client: httpx.AsyncClient, article: dict, index: int):
        """httpxë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ë° ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            console.print(f"ğŸ“– [{index}] ì‹œì‘: {article['title'][:40]}...")
            
            response = await client.get(article["url"], headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë°œí–‰ì‹œê°„ ì¶”ì¶œ (ë” ì •í™•í•œ ì‹œê°„ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸)
            published_at = self._extract_published_at(soup)
            if published_at:
                article["published_at"] = published_at
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content_text(soup)
            article["content"] = content
            
            console.print(f"âœ… [{index}] ì™„ë£Œ: {len(content)}ì")
                
        except Exception as e:
            console.print(f"âŒ [{index}] ì‹¤íŒ¨: {str(e)[:50]}...")
            article["content"] = ""
            if not article.get("published_at"):
                article["published_at"] = datetime.now(pytz.UTC).isoformat()

    def _extract_published_at(self, soup: BeautifulSoup) -> str:
        """ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            # 1. time íƒœê·¸ì—ì„œ ì¶”ì¶œ
            time_element = soup.find('time', datetime=True)
            if time_element:
                return self._parse_datetime(time_element.get('datetime', ''))
            
            # 2. meta íƒœê·¸ì—ì„œ ì¶”ì¶œ
            meta_date = soup.find('meta', property='article:published_time')
            if meta_date:
                return self._parse_datetime(meta_date.get('content', ''))
            
            # 3. ê¸°ì‚¬ ë‚ ì§œ ì˜ì—­ì—ì„œ ì¶”ì¶œ
            date_selectors = [
                            'a[title*="ê¸°ì‚¬ ì…ë ¥/ìˆ˜ì •ì¼"]',
                            '.article-date',
                            '.date',
                '.publish_date'
            ]
            
            for selector in date_selectors:
                element = soup.select_one(selector)
                if element:
                    # ì…ë ¥/ìˆ˜ì • ì‹œê°„ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œ
                    paragraphs = element.find_all('p')
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if 'ì…ë ¥' in text:
                            time_text = text.replace('ì…ë ¥', '').strip()
                            if time_text:
                                return self._parse_datetime(time_text)
            
            return datetime.now(pytz.UTC).isoformat()
            
        except Exception as e:
            console.print(f"âš ï¸ ë°œí–‰ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return datetime.now(pytz.UTC).isoformat()

    def _extract_content_text(self, soup: BeautifulSoup) -> str:
        """ê²½í–¥ì‹ ë¬¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (p.content_textë§Œ ì¶”ì¶œ)"""
        try:
            # #articleBody ì°¾ê¸°
            article_body = soup.find('div', id='articleBody')
            if not article_body:
                console.print("âš ï¸ #articleBodyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return ""
            
            # ì œì™¸í•  ìš”ì†Œë“¤ ì™„ì „ ì œê±°
            exclude_selectors = [
                'div.editor-subtitle',  # ë¶€ì œëª©
                'div.art_photo',  # ì‚¬ì§„ ì˜ì—­
                'p.caption',  # ì‚¬ì§„ ì„¤ëª…
                'div[class*="banner-article"]',  # ê´‘ê³  ë°°ë„ˆ (banner-article-left, banner-article-right ë“±)
                'div.srch-kw',  # ê´€ë ¨ í‚¤ì›Œë“œ
                'div[class*="banner"]', 'div[class*="ad"]', 'div[class*="advertisement"]',
                'script', 'style', 'noscript', 'iframe'
            ]
            
            for selector in exclude_selectors:
                elements = article_body.select(selector)
                for el in elements:
                    el.decompose()
            
            # p.content_text íƒœê·¸ë§Œ ì„ íƒì ìœ¼ë¡œ ì¶”ì¶œ
            paragraphs = []
            content_paragraphs = article_body.select('p.content_text')
            
            for p in content_paragraphs:
                text = p.get_text(strip=True)
                
                # HTML ì—”í‹°í‹° ì²˜ë¦¬ ë° ì •ê·œí™”
                text = re.sub(r'&nbsp;', ' ', text)  # &nbsp; ì œê±°
                text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°± ì •ê·œí™”
                text = text.strip()
                
                # í•„í„°ë§: ê¸°ìëª…, ì´ë©”ì¼, ì¶œì²˜ ì œê±°
                if (text and 
                    len(text) > 10 and  # 10ì ì´ìƒ
                    not re.search(r'[ê°€-í£]+\s*ê¸°ì', text) and  # ê¸°ìëª… ì œì™¸
                    not re.search(r'[ê°€-í£]+\s*íŠ¹íŒŒì›', text) and  # íŠ¹íŒŒì› ì œì™¸
                    not re.search(r'[ê°€-í£]+\s*í†µì‹ ì›', text) and  # í†µì‹ ì› ì œì™¸
                    '@' not in text and  # ì´ë©”ì¼ ì œì™¸
                    '[ì¶œì²˜:' not in text and  # ì¶œì²˜ ì œì™¸
                    '[ê²½í–¥ì‹ ë¬¸]' not in text):  # ì¶œì²˜ ì œì™¸
                    paragraphs.append(text)
            
            if not paragraphs:
                console.print("âš ï¸ ì¶”ì¶œí•  ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return ""
            
            # ë¬¸ë‹¨ë“¤ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°
            combined_text = '\n\n'.join(paragraphs)
            
            # ìµœì¢… ì •ê·œí™”
            combined_text = re.sub(r'\n\s*\n', '\n\n', combined_text)  # ì—°ì† ì¤„ë°”ê¿ˆ ì •ê·œí™”
            
            return combined_text.strip()
            
        except Exception as e:
            console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return ""

    def _parse_datetime(self, datetime_str: str) -> str:
        """ë‚ ì§œì‹œê°„ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            clean_time = datetime_str.strip()
            
            # ISO í˜•ì‹ (2025-09-18T14:07:01+09:00)
            if 'T' in clean_time:
                if '+' in clean_time:
                    published_at = datetime.fromisoformat(clean_time)
                    return published_at.astimezone(pytz.UTC).isoformat()
            else:
                    published_at = datetime.fromisoformat(clean_time.replace('Z', '+00:00'))
                    return published_at.isoformat()
            
            # ì¼ë°˜ í˜•ì‹ (2025.09.18 14:07)
            if re.match(r'^\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}$', clean_time):
                kst_time = datetime.strptime(clean_time, "%Y.%m.%d %H:%M")
                kst_tz = pytz.timezone("Asia/Seoul")
                kst_dt = kst_tz.localize(kst_time)
                return kst_dt.astimezone(pytz.UTC).isoformat()
            
            return datetime.now(pytz.UTC).isoformat()
            
        except Exception as e:
            console.print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {clean_time} - {str(e)}")
            return datetime.now(pytz.UTC).isoformat()

    async def save_articles_batch(self):
        """DB ë°°ì¹˜ ì €ì¥"""
        if not self.articles:
            console.print("âš ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ë°°ì¹˜ ì €ì¥ ì¤‘...")

        try:
            # ì–¸ë¡ ì‚¬ í™•ì¸
            media = self.supabase_manager.get_media_outlet(self.media_name)
            if not media:
                media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
            else:
                media_id = media["id"]

            # ê¸°ì¡´ URL ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì²´í¬)
            existing_urls = set()
            try:
                result = self.supabase_manager.client.table("articles").select("url").eq("media_id", media_id).execute()
                existing_urls = {article["url"] for article in result.data}
            except Exception as e:
                console.print(f"âš ï¸ ê¸°ì¡´ URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

            # ì¤‘ë³µ ì œê±° ë° ë°°ì¹˜ ì¤€ë¹„
            new_articles = []
            skip_count = 0
            short_content_count = 0
            
            for article in self.articles:
                if article["url"] in existing_urls:
                    skip_count += 1
                    continue
                
                # ë³¸ë¬¸ ê¸¸ì´ ì²´í¬ (20ì ë¯¸ë§Œ ì œì™¸)
                content = article.get('content', '')
                if len(content.strip()) < 20:
                    short_content_count += 1
                    continue
                    
                # ê¸°ì‚¬ ë°ì´í„° íŒŒì‹±
                parsed_article = self._parse_article_data_simple(article, media_id)
                if parsed_article:
                    new_articles.append(parsed_article)

            # ë°°ì¹˜ ì €ì¥
            if new_articles:
                success_count = self._batch_insert_articles(new_articles)
                console.print(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
            else:
                console.print("âš ï¸ ì €ì¥í•  ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼: ì„±ê³µ {len(new_articles)}, ìŠ¤í‚µ {skip_count}, ì§§ì€ë³¸ë¬¸ ì œì™¸ {short_content_count}")
            
        except Exception as e:
            console.print(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")

    def _parse_article_data_simple(self, article: dict, media_id: str) -> Optional[dict]:
        """ê¸°ì‚¬ ë°ì´í„° ê°„ë‹¨ íŒŒì‹± (ë°°ì¹˜ ì €ì¥ìš©)"""
        try:
            return {
                'title': article['title'],
                'url': article['url'],
                'content': article.get('content', ''),
                'published_at': article.get('published_at', datetime.now(pytz.UTC).isoformat()),
                'created_at': datetime.now(pytz.UTC).isoformat(),
                'media_id': media_id
            }
        except Exception as e:
            console.print(f"âŒ ê¸°ì‚¬ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def _batch_insert_articles(self, articles: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ê¸°ì‚¬ ì‚½ì…"""
        try:
            # Supabaseì˜ upsert ê¸°ëŠ¥ ì‚¬ìš©
            result = self.supabase_manager.client.table("articles").upsert(articles).execute()
            return len(result.data) if result.data else 0
        except Exception as e:
            console.print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
            success_count = 0
            for article in articles:
                try:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                except:
                    continue
            return success_count
        
    async def run(self, num_pages: int = 15):
        """ì‹¤í–‰"""
        try:
            console.print(f"ğŸš€ ê²½í–¥ì‹ ë¬¸ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {num_pages}í˜ì´ì§€)")
            
            # 1. ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_articles_parallel(num_pages)
            
            if not self.articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            await self.collect_contents_parallel()
            
            # 3. ê¸°ì‚¬ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
            await self.save_articles_batch()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

async def main():
    collector = KhanPoliticsCollector()
    await collector.run(num_pages=15)  # 15í˜ì´ì§€ì—ì„œ ê°ê° 10ê°œì”© ì´ 150ê°œ ê¸°ì‚¬ ìˆ˜ì§‘

if __name__ == "__main__":
    asyncio.run(main())