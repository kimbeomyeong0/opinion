#!/usr/bin/env python3
"""
ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ìµœì‹  50ê°œ + ë³¸ë¬¸ ì „ë¬¸ ìˆ˜ì§‘)
- APIë¡œ ìµœì‹  50ê°œ ê°€ì ¸ì˜´
- Playwrightë¡œ ë³¸ë¬¸ ì „ë¬¸ í¬ë¡¤ë§
- published_at â†’ í•œêµ­ì‹œê°„(KST) ë³€í™˜
"""

import asyncio
import json
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin
import httpx
import pytz
from bs4 import BeautifulSoup
from rich.console import Console
from rich.table import Table
from playwright.async_api import async_playwright

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager import SupabaseManager

console = Console()
KST = pytz.timezone("Asia/Seoul")


class ChosunPoliticsCollector:
    def __init__(self):
        self.base_url = "https://www.chosun.com"
        self.media_name = "ì¡°ì„ ì¼ë³´"
        self.media_bias = "right"
        self.supabase_manager = SupabaseManager()
        self.articles: List[Dict] = []
        self._playwright = None
        self._browser = None

    async def _collect_latest_articles(self):
        """API ê¸°ë°˜ ìµœì‹  100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì¤‘ë³µ ì œì™¸)"""
        console.print("ğŸ”Œ APIë¥¼ í†µí•œ ìµœì‹  ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘ ì‹œì‘...")
        api_base = "https://www.chosun.com/pf/api/v3/content/fetch/story-feed"
        target_count = 100
        offset = 0
        size = 50
        max_attempts = 5  # ìµœëŒ€ 5ë²ˆ API í˜¸ì¶œ

        async with httpx.AsyncClient(timeout=10.0) as client:
            for attempt in range(max_attempts):
                if len(self.articles) >= target_count:
                    break
                    
                try:
                    console.print(f"ğŸ“¡ API í˜¸ì¶œ {attempt + 1}/{max_attempts} (offset: {offset})")
                    
                    query_params = {
                        "query": json.dumps({
                            "excludeContentTypes": "gallery, video",
                            "includeContentTypes": "story",
                            "includeSections": "/politics",
                            "offset": offset,
                            "size": size
                        }),
                        "_website": "chosun"
                    }
                    
                    resp = await client.get(api_base, params=query_params)
                    resp.raise_for_status()
                    data = resp.json()

                    content_elements = data.get("content_elements", [])
                    console.print(f"ğŸ“Š API ì‘ë‹µ: {len(content_elements)}ê°œ ìš”ì†Œ ìˆ˜ì‹ ")
                    
                    if not content_elements:
                        console.print("âš ï¸ ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                        break
                    
                    parsed_count = 0
                    added_count = 0
                    
                    for element in content_elements:
                        if len(self.articles) >= target_count:
                            break
                            
                        article = self._parse_api_article(element)
                        if article:
                            parsed_count += 1
                            if self._add_article(article):
                                added_count += 1
                    
                    console.print(f"ğŸ“ˆ íŒŒì‹± ì„±ê³µ: {parsed_count}ê°œ, ìµœì¢… ì¶”ê°€: {added_count}ê°œ")
                    console.print(f"ğŸ“Š í˜„ì¬ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(self.articles)}ê°œ")
                    
                    # ë‹¤ìŒ offsetìœ¼ë¡œ ì´ë™
                    offset += size
                    
                    # API í˜¸ì¶œ ê°„ ì ì‹œ ëŒ€ê¸°
                    await asyncio.sleep(0.5)

                except Exception as e:
                    console.print(f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                    offset += size
                    continue

        console.print(f"ğŸ¯ ìˆ˜ì§‘ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ (ëª©í‘œ: {target_count}ê°œ)")

    def _parse_api_article(self, element: Dict) -> Optional[Dict]:
        """API ì‘ë‹µ íŒŒì‹±"""
        try:
            title = element.get("headlines", {}).get("basic")
            if not title:
                return None

            canonical_url = element.get("canonical_url")
            if not canonical_url:
                return None
            url = urljoin(self.base_url, canonical_url) if canonical_url.startswith("/") else canonical_url

            # ë‚ ì§œ â†’ í•œêµ­ì‹œê°„ ë³€í™˜ (ì¡°ì„ ì¼ë³´ API í˜•ì‹ ëŒ€ì‘)
            display_date = element.get("display_date")
            if display_date:
                try:
                    # ì¡°ì„ ì¼ë³´ API ë‚ ì§œ í˜•ì‹ ì •ê·œí™”
                    # ì˜ˆ: "2025-09-02T09:39:49.26Z" â†’ "2025-09-02T09:39:49.260000Z"
                    normalized_date = display_date
                    
                    # Zë¡œ ëë‚˜ëŠ” ê²½ìš° ì²˜ë¦¬
                    if normalized_date.endswith("Z"):
                        # ì†Œìˆ˜ì  ë¶€ë¶„ ì •ê·œí™”
                        if "." in normalized_date:
                            # Tì™€ Z ì‚¬ì´ì˜ ë¶€ë¶„ ì¶”ì¶œ
                            t_index = normalized_date.find("T")
                            z_index = normalized_date.find("Z")
                            time_part = normalized_date[t_index+1:z_index]
                            
                            if "." in time_part:
                                # ì†Œìˆ˜ì ì„ 6ìë¦¬ë¡œ ë§ì¶¤
                                seconds_part = time_part.split(".")[0]
                                decimal_part = time_part.split(".")[1]
                                decimal_part = decimal_part.ljust(6, "0")[:6]  # 6ìë¦¬ë¡œ ë§ì¶¤
                                normalized_date = normalized_date[:t_index+1] + seconds_part + "." + decimal_part + "Z"
                        
                        # Zë¥¼ +00:00ë¡œ ë³€í™˜
                        normalized_date = normalized_date.replace("Z", "+00:00")
                    
                    # ISO í˜•ì‹ íŒŒì‹±
                    dt_utc = datetime.fromisoformat(normalized_date)
                    
                    # í•œêµ­ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (naive datetimeìœ¼ë¡œ ì €ì¥)
                    published_at = dt_utc.astimezone(KST).replace(tzinfo=None)
                        
                except Exception as e:
                    console.print(f"âš ï¸ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {display_date} - {e}")
                    published_at = None
            else:
                published_at = None

            return {
                "title": title.strip(),
                "url": url,
                "content": "",  # ë³¸ë¬¸ì€ ë‚˜ì¤‘ì— Playwrightë¡œ ì±„ì›€
                "published_at": published_at,
                "created_at": published_at,  # ë°œí–‰ ì‹œê°„ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
            }
        except Exception:
            return None

    def _add_article(self, article: Dict) -> bool:
        """ì¤‘ë³µ ì œê±° í›„ ì¶”ê°€"""
        if not article.get("url"):
            return False
        if any(a["url"] == article["url"] for a in self.articles):
            return False
        self.articles.append(article)
        return True

    async def _extract_content(self, url: str) -> str:
        """Playwrightë¡œ ë³¸ë¬¸ ì „ë¬¸ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        page = None
        try:
            if not self._browser:
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox', 
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--memory-pressure-off'
                    ]
                )

            page = await self._browser.new_page()
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
            await page.set_viewport_size({"width": 1280, "height": 720})
            await page.goto(url, wait_until="domcontentloaded", timeout=10000)

            # ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì¶”ì¶œ (ë” ìœ ì—°í•œ ë°©ë²•)
            content = ""
            
            # ë°©ë²• 1: JavaScriptë¡œ ì§ì ‘ ë³¸ë¬¸ ì¶”ì¶œ
            try:
                content = await page.evaluate('''() => {
                    // ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì„ íƒìë“¤
                    const selectors = [
                        'section.article-body p',
                        'div#article-body p',
                        'div.article-body p',
                        'article.article-body p',
                        '.story-news p',
                        '.article-content p',
                        'main p',
                        'article p'
                    ];
                    
                    for (const selector of selectors) {
                        const paragraphs = document.querySelectorAll(selector);
                        if (paragraphs.length > 0) {
                            const texts = Array.from(paragraphs)
                                .map(p => p.textContent.trim())
                                .filter(text => text.length > 20)
                                .slice(0, 20); // ìµœëŒ€ 20ê°œ
                            
                            if (texts.length > 0) {
                                return texts.join('\\n\\n');
                            }
                        }
                    }
                    
                    // ëª¨ë“  p íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì°¾ê¸° (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
                    const allP = document.querySelectorAll('p');
                    const texts = Array.from(allP)
                        .map(p => p.textContent.trim())
                        .filter(text => text.length > 50) // ë” ê¸´ í…ìŠ¤íŠ¸ë§Œ
                        .slice(0, 15); // ìµœëŒ€ 15ê°œ
                    
                    return texts.join('\\n\\n');
                }''')
                
                if content and len(content.strip()) > 50:
                    return content.strip()
                    
            except Exception as e:
                console.print(f"âš ï¸ JavaScript ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:50]}")
            
            # ë°©ë²• 2: BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹± (ë°±ì—…)
            try:
                html = await page.content()
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ í›„ë³´ë“¤
                content_selectors = [
                    'section.article-body',
                    'div#article-body',
                    'div.article-body',
                    'article.article-body',
                    '.story-news',
                    '.article-content'
                ]
                
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        paragraphs = content_elem.find_all('p')
                        if paragraphs:
                            texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20]
                            if texts:
                                content = '\n\n'.join(texts[:20])
                                break
                                
            except Exception as e:
                console.print(f"âš ï¸ BeautifulSoup ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:50]}")

            return content.strip()
            
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url[:50]}...): {str(e)[:50]}")
            return ""
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass

    async def collect_contents(self):
        """ë³¸ë¬¸ ì „ë¬¸ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”)"""
        if not self.articles:
            return

        console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬ (ë°°ì¹˜ ì²˜ë¦¬)")
        
        # 3ê°œì”© ë°°ì¹˜ë¡œ ì²˜ë¦¬ (ë§¥ë¶ ì—ì–´ M2ì— ìµœì í™”)
        batch_size = 3
        success_count = 0
        
        for i in range(0, len(self.articles), batch_size):
            batch = self.articles[i:i + batch_size]
            console.print(f"ğŸ“„ ë°°ì¹˜ {i//batch_size + 1}/{(len(self.articles) + batch_size - 1)//batch_size} ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ìˆœì°¨ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
            for j, art in enumerate(batch):
                try:
                    content = await self._extract_content(art["url"])
                    if content:
                        self.articles[i + j]["content"] = content
                        success_count += 1
                        console.print(f"âœ… [{i + j + 1}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
                    else:
                        console.print(f"âš ï¸ [{i + j + 1}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
                    
                    # ê° ê¸°ì‚¬ ê°„ ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    console.print(f"âŒ [{i + j + 1}/{len(self.articles)}] ì˜¤ë¥˜: {str(e)[:50]}")
            
            # ë°°ì¹˜ ê°„ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œê°„)
            if i + batch_size < len(self.articles):
                console.print("â³ ë°°ì¹˜ ê°„ ëŒ€ê¸° ì¤‘... (ë©”ëª¨ë¦¬ ì •ë¦¬)")
                await asyncio.sleep(2)

        console.print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(self.articles)}ê°œ ì„±ê³µ")

    async def save_to_supabase(self):
        """DB ì €ì¥ (ì¤‘ë³µ ìë™ ì²˜ë¦¬)"""
        if not self.articles:
            console.print("âŒ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")

        # ì–¸ë¡ ì‚¬ í™•ì¸
        media = self.supabase_manager.get_media_outlet(self.media_name)
        if not media:
            media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
        else:
            media_id = media["id"]

        # ëª¨ë“  URLì„ í•œ ë²ˆì— ì¡°íšŒí•˜ì—¬ ì¤‘ë³µ ì²´í¬ (íš¨ìœ¨ì„± ê°œì„ )
        urls = [art["url"] for art in self.articles]
        console.print("ğŸ” ê¸°ì¡´ ê¸°ì‚¬ ì¤‘ë³µ ì²´í¬ ì¤‘...")
        
        try:
            existing_urls = set()
            # ë°°ì¹˜ë¡œ ì¤‘ë³µ ì²´í¬ (í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ì¡°íšŒí•˜ì§€ ì•Šë„ë¡)
            batch_size = 20
            for i in range(0, len(urls), batch_size):
                batch_urls = urls[i:i + batch_size]
                for url in batch_urls:
                    exists = self.supabase_manager.client.table("articles").select("url").eq("url", url).execute()
                    if exists.data:
                        existing_urls.add(url)
            
            console.print(f"ğŸ“Š ì¤‘ë³µ ì²´í¬ ì™„ë£Œ: {len(existing_urls)}ê°œ ì¤‘ë³µ ë°œê²¬")
            
        except Exception as e:
            console.print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
            existing_urls = set()

        success, failed, skipped = 0, 0, 0
        
        for i, art in enumerate(self.articles, 1):
            try:
                # ì¤‘ë³µ ì²´í¬
                if art["url"] in existing_urls:
                    console.print(f"âš ï¸ [{i}/{len(self.articles)}] ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: {art['title'][:30]}...")
                    skipped += 1
                    continue

                # í•œêµ­ì‹œê°„ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì‹œê°„ëŒ€ ì •ë³´ ì œê±°)
                published_at_str = None
                created_at_str = None
                
                if isinstance(art["published_at"], datetime):
                    # naive datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì‹œê°„ëŒ€ ì •ë³´ ì—†ì´)
                    published_at_str = art["published_at"].strftime('%Y-%m-%d %H:%M:%S')
                    created_at_str = art["created_at"].strftime('%Y-%m-%d %H:%M:%S') if art.get("created_at") else published_at_str
                elif art.get("published_at"):
                    # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš°
                    published_at_str = art["published_at"]
                    created_at_str = art.get("created_at", published_at_str)

                article_data = {
                    "media_id": media_id,
                    "title": art["title"],
                    "content": art["content"],
                    "url": art["url"],
                    "published_at": published_at_str,
                    "created_at": created_at_str,
                }

                if self.supabase_manager.insert_article(article_data):
                    success += 1
                    console.print(f"âœ… [{i}/{len(self.articles)}] ì €ì¥ ì„±ê³µ: {art['title'][:30]}...")
                else:
                    failed += 1
                    console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì‹¤íŒ¨: {art['title'][:30]}...")
                    
            except Exception as e:
                failed += 1
                console.print(f"âŒ [{i}/{len(self.articles)}] ì €ì¥ ì˜¤ë¥˜: {str(e)[:50]}")

        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"  âœ… ì„±ê³µ: {success}ê°œ")
        console.print(f"  âŒ ì‹¤íŒ¨: {failed}ê°œ") 
        console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skipped}ê°œ")
        console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(success / len(self.articles) * 100):.1f}%")

    async def cleanup(self):
        """Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self._browser:
                await self._browser.close()
                self._browser = None
            if self._playwright:
                await self._playwright.stop()
                self._playwright = None
            console.print("ğŸ§¹ Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            console.print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}")

    async def run(self):
        """ì‹¤í–‰ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
        try:
            console.print(f"ğŸš€ ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹  100ê°œ)")
            console.print("ğŸ’¡ ë§¥ë¶ ì—ì–´ M2 ìµœì í™” ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
            
            await self._collect_latest_articles()
            await self.collect_contents()
            await self.save_to_supabase()
            
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except KeyboardInterrupt:
            console.print("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            await self.cleanup()


async def main():
    collector = ChosunPoliticsCollector()
    await collector.run()

if __name__ == "__main__":
    asyncio.run(main())
