#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ìµœì í™” ë²„ì „)
- ìµœì‹  ì •ì¹˜ ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘
- í˜ì´ì§€ë„¤ì´ì…˜ í™œìš© (?page={page})
- 20ì´ˆ ë‚´ í¬ë¡¤ë§ ì™„ë£Œ ëª©í‘œ
- ë§¥ë¶ ì—ì–´ M2 ìµœì í™”
"""

import asyncio
import httpx
import time
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
import logging
from playwright.async_api import async_playwright

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.supabase_manager import SupabaseManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class JoongangPoliticsCrawler:
    """ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_articles: int = 100):
        self.max_articles = max_articles
        self.console = Console()
        self.supabase_manager = SupabaseManager()
        
        # ì¤‘ì•™ì¼ë³´ ì„¤ì •
        self.base_url = "https://www.joongang.co.kr"
        self.politics_url = "https://www.joongang.co.kr/politics"
        
        # ì¤‘ì•™ì¼ë³´ëŠ” ìš°í¸í–¥ ì„±í–¥
        self.media_name = "ì¤‘ì•™ì¼ë³´"
        self.media_bias = None  # ì´ˆê¸°í™” ì‹œì ì—ëŠ” Noneìœ¼ë¡œ ì„¤ì •
        
        # í†µê³„
        self.total_articles = 0
        self.successful_articles = 0
        self.failed_articles = 0
        self.start_time: Optional[datetime] = None
        
        # media_id ì´ˆê¸°í™”
        self.media_id = None
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •
        self.page_size = 25  # í˜ì´ì§€ë‹¹ ê¸°ì‚¬ ìˆ˜ (ì¶”ì •)
        self.max_pages = 10  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        
    async def _get_media_info(self) -> tuple[int, str]:
        """media_outlets í…Œì´ë¸”ì—ì„œ ì¤‘ì•™ì¼ë³´ì˜ IDì™€ biasë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # media_outlets í…Œì´ë¸”ì—ì„œ ì¤‘ì•™ì¼ë³´ ì°¾ê¸°
            result = self.supabase_manager.client.table('media_outlets').select('*').eq('name', self.media_name).execute()
            
            if result.data:
                media_id = result.data[0]['id']
                self.media_bias = result.data[0]['bias']  # media_outlets í…Œì´ë¸”ì—ì„œ bias ê°€ì ¸ì˜´
                logger.info(f"âœ… {self.media_name} media_id: {media_id}, bias: {self.media_bias}")
                return media_id, self.media_bias
            else:
                # ì—†ìœ¼ë©´ ìƒì„± (ê¸°ë³¸ê°’ìœ¼ë¡œ Right ì‚¬ìš©)
                media_id = self.supabase_manager.create_media_outlet(self.media_name, "Right")
                self.media_bias = "Right"
                logger.info(f"âœ… {self.media_name} ìƒì„±ë¨ - media_id: {media_id}, bias: {self.media_bias}")
                return media_id, self.media_bias
                
        except Exception as e:
            logger.error(f"âŒ media_info ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
            # ê¸°ë³¸ê°’ ì‚¬ìš© (media_outlets í…Œì´ë¸” ê¸°ì¤€)
            self.media_bias = "Right"
            return 2, self.media_bias  # ì¤‘ì•™ì¼ë³´

    async def _get_media_id(self) -> int:
        """media_outlets í…Œì´ë¸”ì—ì„œ ì¤‘ì•™ì¼ë³´ì˜ IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (í˜¸í™˜ì„± ìœ ì§€)"""
        media_id, _ = await self._get_media_info()
        return media_id
        
    async def _collect_latest_articles(self):
        """í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ ìµœì‹  100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì¤‘ë³µ ì œì™¸) - Playwright ì‚¬ìš©"""
        self.console.print("ğŸ”Œ í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ ìµœì‹  ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘ ì‹œì‘...")
        target_count = 100
        page = 1
        page_offset = 0
        max_attempts = 20  # ìµœëŒ€ 20í˜ì´ì§€ ì‹œë„

        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰ (ë§¥ë¶ ì—ì–´ M2 ìµœì í™”)
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--memory-pressure-off',
                    '--max_old_space_size=4096'
                ]
            )
            
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            collected_articles = []
            
            while len(collected_articles) < target_count and page <= max_attempts:
                try:
                    # í˜ì´ì§€ URL êµ¬ì„±
                    if page == 1:
                        url = self.politics_url
                    else:
                        url = f"{self.politics_url}?page={page}"
                    
                    self.console.print(f"ğŸ“¡ í˜ì´ì§€ {page}/{max_attempts} ì²˜ë¦¬ ì¤‘ (offset: {page_offset})")
                    
                    # Playwrightë¡œ í˜ì´ì§€ ë¡œë“œ
                    page_obj = await context.new_page()
                    await page_obj.goto(url, wait_until="domcontentloaded", timeout=10000)
                    
                    # HTML íŒŒì‹±
                    html = await page_obj.content()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ì¤‘ì•™ì¼ë³´ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                    article_elements = soup.select('.story_list .card .headline a')
                    
                    self.console.print(f"ğŸ“Š í˜ì´ì§€ {page} ì‘ë‹µ: {len(article_elements)}ê°œ ìš”ì†Œ ìˆ˜ì‹ ")
                    
                    page_articles = []
                    for element in article_elements:
                        href = element.get('href')
                        if href and '/article/' in href:
                            if href.startswith('/'):
                                full_url = f"{self.base_url}{href}"
                            else:
                                full_url = href
                            
                            # ì¤‘ë³µ ì²´í¬
                            if not any(article['url'] == full_url for article in collected_articles):
                                page_articles.append({
                                    'url': full_url,
                                    'title': element.get_text(strip=True) or f"ì¤‘ì•™ì¼ë³´ ê¸°ì‚¬ {len(collected_articles) + 1}",
                                    'published_at': datetime.now().isoformat()
                                })
                    
                    # í˜ì´ì§€ ê¸°ì‚¬ë“¤ì„ ì „ì²´ ëª©ë¡ì— ì¶”ê°€
                    collected_articles.extend(page_articles)
                    
                    self.console.print(f"ğŸ“ˆ í˜ì´ì§€ {page} íŒŒì‹± ì„±ê³µ: {len(page_articles)}ê°œ, ìµœì¢… ì¶”ê°€: {len(page_articles)}ê°œ")
                    self.console.print(f"ğŸ“Š í˜„ì¬ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(collected_articles)}ê°œ")
                    
                    await page_obj.close()
                    
                    # ëª©í‘œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
                    if len(collected_articles) >= target_count:
                        break
                    
                    page += 1
                    page_offset += len(page_articles)
                    
                    # í˜ì´ì§€ ê°„ ëŒ€ê¸° (ê³¼ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    self.console.print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    page += 1
                    continue
            
            await browser.close()
            
            # ëª©í‘œ ê°œìˆ˜ë§Œí¼ ìë¥´ê¸°
            final_articles = collected_articles[:target_count]
            self.console.print(f"ğŸ¯ ìˆ˜ì§‘ ì™„ë£Œ: {len(final_articles)}ê°œ ê¸°ì‚¬ (ëª©í‘œ: {target_count}ê°œ)")
            
            return final_articles

    async def collect_contents(self):
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì˜ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ - Playwright ì‚¬ìš©"""
        if not hasattr(self, 'articles') or not self.articles:
            self.console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        self.console.print(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {len(self.articles)}ê°œ ê¸°ì‚¬ (ë°°ì¹˜ ì²˜ë¦¬)")
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ ìµœì í™”)
        batch_size = 5  # PlaywrightëŠ” ë” ë¬´ê±°ìš°ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¤„ì„
        total_batches = (len(self.articles) + batch_size - 1) // batch_size
        
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰ (ë§¥ë¶ ì—ì–´ M2 ìµœì í™”)
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--memory-pressure-off',
                    '--max_old_space_size=4096'
                ]
            )
            
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(self.articles))
                batch_articles = self.articles[start_idx:end_idx]
                
                self.console.print(f"ğŸ“„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘...")
                
                # ë°°ì¹˜ ë‚´ ê¸°ì‚¬ë“¤ ìˆœì°¨ ì²˜ë¦¬ (PlaywrightëŠ” ë™ì‹œ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜)
                for i, article in enumerate(batch_articles):
                    try:
                        result = await self._fetch_article_content_playwright(context, article, start_idx + i + 1)
                        if result:
                            self.console.print(f"âœ… [{start_idx + i + 1}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì„±ê³µ")
                            self.successful_articles += 1
                        else:
                            self.console.print(f"âš ï¸ [{start_idx + i + 1}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (ë‚´ìš© ì—†ìŒ)")
                            self.failed_articles += 1
                    except Exception as e:
                        self.console.print(f"âŒ [{start_idx + i + 1}/{len(self.articles)}] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                        self.failed_articles += 1
                
                # ë°°ì¹˜ ê°„ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬)
                if batch_idx < total_batches - 1:
                    self.console.print("â³ ë°°ì¹˜ ê°„ ëŒ€ê¸° ì¤‘... (ë©”ëª¨ë¦¬ ì •ë¦¬)")
                    await asyncio.sleep(2)
            
            await browser.close()
        
        self.console.print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {self.successful_articles}/{len(self.articles)}ê°œ ì„±ê³µ")

    async def _fetch_article_content_playwright(self, context, article: Dict[str, Any], index: int) -> bool:
        """Playwrightë¥¼ ì‚¬ìš©í•œ ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            page = await context.new_page()
            await page.goto(article['url'], wait_until="domcontentloaded", timeout=10000)
            
            # HTML íŒŒì‹±
            html = await page.content()
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì¤‘ì•™ì¼ë³´ ë³¸ë¬¸ ì¶”ì¶œ
            content = None
            content_selectors = [
                '.article_body',  # ì¤‘ì•™ì¼ë³´ ì‹¤ì œ ë³¸ë¬¸
                '.article-content',  # ì¤‘ì•™ì¼ë³´ ë³¸ë¬¸ ì„¹ì…˜
                '.content',
                '.body',
                '.article-body',
                '.text',
                'article',
                '.desc'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for unwanted in content_elem.select('script, style, .ad, .advertisement, .related, .comment, .ui-control, .font-control, .share-control, .sidebar, .side-news, .related-news, .reading-mode, .dark-mode, .font-size, .bookmark, .print, .share, .reaction, .recommend, .like, .subscribe, .donation, .footer, .navigation, .menu, .header, .banner, .art_photo, .art_photo_wrap, .caption, .contbox-wrap, .footerFirst, .editor-wrap, .swiper-container, .area-replay-wrap, .replay-cont, .bottom-wrap, .m-pop, .wrap.category'):
                        unwanted.decompose()
                    
                    content = content_elem.get_text(strip=True, separator='\n')
                    if content and len(content) > 100:
                        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
                        unwanted_patterns = [
                            'ì½ê¸°ëª¨ë“œ', 'ë‹¤í¬ëª¨ë“œ', 'í°íŠ¸í¬ê¸°', 'ê°€ê°€ê°€ê°€ê°€ê°€', 'ë¶ë§ˆí¬', 'ê³µìœ í•˜ê¸°', 'í”„ë¦°íŠ¸',
                            'ê¸°ì‚¬ë°˜ì‘', 'ì¶”ì²œí•´ìš”', 'ì¢‹ì•„ìš”', 'ê°ë™ì´ì—ìš”', 'í™”ë‚˜ìš”', 'ìŠ¬í¼ìš”',
                            'My ì¶”ì²œ ê¸°ì‚¬', 'ê°€ì¥ ë§ì´ ì½ì€ ê¸°ì‚¬', 'ëŒ“ê¸€ ë§ì€ ê¸°ì‚¬', 'ì‹¤ì‹œê°„ ìµœì‹  ë‰´ìŠ¤',
                            'ì£¼ìš”ë‰´ìŠ¤', 'ì´ìŠˆNOW', 'ê´€ë ¨ê¸°ì‚¬', 'ë”ë³´ê¸°', 'ëª©ë¡', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€',
                            'êµ¬ë…', 'ê¸°ì‚¬ í›„ì›í•˜ê¸°', 'ì¹´ì¹´ì˜¤í†¡', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ë¼ì¸', 'ë§í¬ë³µì‚¬',
                            'í¼ì¹˜ê¸°/ì ‘ê¸°', 'ê¸°ì‚¬ ì½ê¸°', 'ìš”ì•½', 'ë‹«ê¸°', 'Please activate JavaScript',
                            'AD', 'ëŒ“ê¸€', 'ìƒˆë¡œê³ ì¹¨', 'ì£¼ìš” ê¸°ì‚¬', 'ë‰´ìŠ¤ë£¸ PICK', 'ì§€ê¸ˆ ë§ì´ ë³´ëŠ” ê¸°ì‚¬'
                        ]
                        
                        for pattern in unwanted_patterns:
                            content = content.replace(pattern, '')
                        
                        # ê¸°ì ì •ë³´ ì´í›„ì˜ ëª¨ë“  ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
                        lines = content.split('\n')
                        cleaned_lines = []
                        for line in lines:
                            # ê¸°ì ì •ë³´ë¥¼ ì°¾ìœ¼ë©´ ê·¸ ì´í›„ëŠ” ëª¨ë‘ ì œê±°
                            if 'ê¸°ì' in line and len(line.strip()) < 30:
                                cleaned_lines.append(line.strip())
                                break
                            cleaned_lines.append(line)
                        
                        content = '\n'.join(cleaned_lines)
                        
                        # ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ì¶”ê°€ ì •ë¦¬
                        import re
                        
                        # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
                        content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', content)
                        
                        # í•´ì‹œíƒœê·¸ ì œê±°
                        content = re.sub(r'#\s*\w+', '', content)
                        
                        # ìˆ«ìë§Œ ìˆëŠ” ë¼ì¸ ì œê±°
                        content = re.sub(r'^\d+$', '', content, flags=re.MULTILINE)
                        
                        # ì‹œê°„ í˜•ì‹ ì œê±° (12:10 ê°™ì€)
                        content = re.sub(r'^\d{1,2}:\d{2}$', '', content, flags=re.MULTILINE)
                        
                        # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
                        content = '\n'.join([line.strip() for line in content.split('\n') if line.strip()])
                        
                        if content and len(content) > 100:
                            break
            
            if not content or len(content) < 100:
                await page.close()
                return False
            
            # ì œëª© ì¶”ì¶œ (ë” ì •í™•í•œ ì œëª©)
            title = article.get('title', '')
            if not title or len(title) < 10:
                # og:title ë©”íƒ€íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
                meta_title = soup.find('meta', property='og:title')
                if meta_title and meta_title.get('content'):
                    title = meta_title.get('content').strip()
                
                # title íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
                if not title:
                    page_title = soup.find('title')
                    if page_title:
                        title = page_title.get_text(strip=True)
                        # ì‚¬ì´íŠ¸ëª… ì œê±° (ì˜ˆ: " - ì¤‘ì•™ì¼ë³´")
                        if ' - ' in title:
                            title = title.split(' - ')[0].strip()
                
                # ì¤‘ì•™ì¼ë³´ ì œëª© ì„ íƒì
                if not title:
                    title_selectors = [
                        'h1.headline',
                        '.headline h1',
                        'h1.title',
                        '.title h1',
                        'h1'
                    ]
                    
                    for selector in title_selectors:
                        title_elem = soup.select_one(selector)
                        if title_elem:
                            candidate_title = title_elem.get_text(strip=True)
                            if candidate_title and len(candidate_title) > 10:
                                title = candidate_title
                                break
            
            # ë°œí–‰ì¼ ì¶”ì¶œ
            published_at = article.get('published_at', datetime.now().isoformat())
            date_selectors = [
                'meta[name="article:published_time"]',
                '.date',
                '.published_date',
                '.article_date',
                '.time'
            ]
            
            for selector in date_selectors:
                if selector.startswith('meta'):
                    date_elem = soup.select_one(selector)
                    if date_elem:
                        date_str = date_elem.get('content')
                        if date_str:
                            try:
                                # ISO 8601 í˜•ì‹ íŒŒì‹±
                                published_at = datetime.fromisoformat(date_str.replace('Z', '+00:00')).isoformat()
                                break
                            except:
                                pass
                else:
                    date_elem = soup.select_one(selector)
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                        if date_text:
                            # ì¤‘ì•™ì¼ë³´ ë‚ ì§œ í˜•ì‹: "2025.08.20 22:48"
                            try:
                                date_obj = datetime.strptime(date_text, '%Y.%m.%d %H:%M')
                                published_at = date_obj.isoformat()
                                break
                            except:
                                pass
            
            # ê¸°ì‚¬ ì •ë³´ ì—…ë°ì´íŠ¸
            article.update({
                'title': title,
                'content': content,
                'published_at': published_at
            })
            
            await page.close()
            return True
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({article['url']}): {str(e)}")
            return False

    async def save_to_supabase(self):
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not hasattr(self, 'articles') or not self.articles:
            self.console.print("âŒ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        self.console.print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ê¸°ë³¸ ì´ìŠˆ ìƒì„± í™•ì¸
        await self._create_default_issue()
        
        # media_info ê°€ì ¸ì˜¤ê¸°
        media_id, bias = await self._get_media_info()
        
        success_count = 0
        failed_count = 0
        
        for article in self.articles:
            try:
                # ì¤‘ë³µ ì²´í¬
                existing = self.supabase_manager.client.table('articles').select('id').eq('url', article['url']).execute()
                if existing.data:
                    continue  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê¸°ì‚¬
                
                # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
                article_data = {
                    'title': article['title'],
                    'url': article['url'],
                    'content': article['content'],
                    'published_at': article['published_at'],
                    'media_id': media_id
                }
                
                # Supabaseì— ì €ì¥
                result = self.supabase_manager.insert_article(article_data)
                if result:
                    self.console.print(f"âœ… ê¸°ì‚¬ ì‚½ì… ì„±ê³µ: {article['title'][:50]}...")
                    success_count += 1
                else:
                    self.console.print(f"âŒ ê¸°ì‚¬ ì‚½ì… ì‹¤íŒ¨: {article['title'][:50]}...")
                    failed_count += 1
                    
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                failed_count += 1
        
        # ê²°ê³¼ í‘œì‹œ
        self.console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        self.console.print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
        self.console.print(f"  âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
        self.console.print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {len(self.articles) - success_count - failed_count}ê°œ")
        self.console.print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(success_count/len(self.articles)*100):.1f}%")

    async def _create_default_issue(self):
        """ê¸°ë³¸ ì´ìŠˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ ì´ìŠˆ í™•ì¸ (UUID í˜•ì‹ìœ¼ë¡œ ìˆ˜ì •)
            existing = self.supabase_manager.client.table('issues').select('id').limit(1).execute()
            
            if not existing.data:
                # ê¸°ë³¸ ì´ìŠˆ ìƒì„± (UUID ìë™ ìƒì„±, ê°„ë‹¨í•œ êµ¬ì¡°)
                issue_data = {
                    'title': 'ê¸°ë³¸ ì´ìŠˆ',
                    'subtitle': 'í¬ë¡¤ëŸ¬ë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ ìœ„í•œ ê¸°ë³¸ ì´ìŠˆ',
                    'summary': 'ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ìˆ˜ì§‘ëœ ì •ì¹˜ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ í¬í•¨í•˜ëŠ” ê¸°ë³¸ ì´ìŠˆì…ë‹ˆë‹¤.'
                }
                
                result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
                logger.info("ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì„±ê³µ")
            else:
                logger.info("ê¸°ë³¸ ì´ìŠˆê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
                
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    async def run(self):
        """ì‹¤í–‰ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
        try:
            self.console.print(f"ğŸš€ ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹  100ê°œ)")
            self.console.print("ğŸ’¡ ë§¥ë¶ ì—ì–´ M2 ìµœì í™” ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
            
            # 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            self.articles = await self._collect_latest_articles()
            
            if not self.articles:
                self.console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 2ë‹¨ê³„: ë³¸ë¬¸ ìˆ˜ì§‘
            await self.collect_contents()
            
            # 3ë‹¨ê³„: Supabase ì €ì¥
            await self.save_to_supabase()
            
            self.console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            try:
                pass  # httpx í´ë¼ì´ì–¸íŠ¸ëŠ” ìë™ìœ¼ë¡œ ì •ë¦¬ë¨
            except Exception as e:
                self.console.print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = JoongangPoliticsCrawler(max_articles=100)
    await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
