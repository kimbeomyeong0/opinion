#!/usr/bin/env python3
"""
UMAP + HDBSCANì„ ì´ìš©í•œ ì •ì¹˜ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§
"""

import sys
import os
import numpy as np
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any, Tuple
import json

# í´ëŸ¬ìŠ¤í„°ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬
import umap
import hdbscan
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.supabase_manager import get_supabase_client
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.panel import Panel

console = Console()

class PoliticalNewsClusterer:
    """ì •ì¹˜ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í´ëŸ¬ìŠ¤í„°ëŸ¬ ì´ˆê¸°í™”"""
        self.supabase = get_supabase_client()
        self.embeddings = None
        self.articles_data = None
        self.umap_reducer = None
        self.hdbscan_clusterer = None
        self.cluster_labels = None
        self.umap_embedding = None
        
        # ê²°ê³¼ ì €ì¥
        self.clusters_info = []
        self.cluster_stats = {}
        
    def load_embeddings_data(self) -> bool:
        """ì„ë² ë”© ë°ì´í„° ë¡œë“œ"""
        try:
            console.print("ğŸ“Š ì„ë² ë”© ë°ì´í„° ë¡œë“œ ì¤‘...")
            
            # ì„ë² ë”© ë°ì´í„° ì¡°íšŒ
            result = self.supabase.client.table('articles_embeddings').select(
                'cleaned_article_id, embedding_vector, model_name'
            ).eq('embedding_type', 'combined').execute()
            
            if not result.data:
                console.print("âŒ ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ê¸°ì‚¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ë°°ì¹˜ ì²˜ë¦¬)
            article_ids = [item['cleaned_article_id'] for item in result.data]
            
            # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ì„œ ì¡°íšŒ
            batch_size = 100
            articles_data_list = []
            
            for i in range(0, len(article_ids), batch_size):
                batch_ids = article_ids[i:i+batch_size]
                articles_result = self.supabase.client.table('articles_cleaned').select(
                    'id, title_cleaned, lead_paragraph, merged_content, original_article_id'
                ).in_('id', batch_ids).execute()
                
                if articles_result.data:
                    articles_data_list.extend(articles_result.data)
            
            articles_result = type('obj', (object,), {'data': articles_data_list})
            
            # ë°ì´í„° ì •ë¦¬
            embeddings_df = pd.DataFrame(result.data)
            articles_df = pd.DataFrame(articles_result.data)
            
            # ì„ë² ë”© ë²¡í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_list = []
            for embedding_str in embeddings_df['embedding_vector']:
                if isinstance(embedding_str, str):
                    embedding_vector = json.loads(embedding_str)
                else:
                    embedding_vector = embedding_str
                embeddings_list.append(embedding_vector)
            
            self.embeddings = np.array(embeddings_list)
            
            # ê¸°ì‚¬ ë°ì´í„°ì™€ ë§¤í•‘
            self.articles_data = articles_df.merge(
                embeddings_df[['cleaned_article_id']], 
                left_on='id', 
                right_on='cleaned_article_id'
            )
            
            console.print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.embeddings)}ê°œ ê¸°ì‚¬")
            console.print(f"   - ì„ë² ë”© ì°¨ì›: {self.embeddings.shape[1]}")
            console.print(f"   - ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°: {len(self.articles_data)}ê°œ")
            
            return True
            
        except Exception as e:
            console.print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def run_umap_reduction(self, n_components: int = 2, n_neighbors: int = 15, min_dist: float = 0.1) -> bool:
        """UMAP ì°¨ì› ì¶•ì†Œ ì‹¤í–‰"""
        try:
            console.print("ğŸ”„ UMAP ì°¨ì› ì¶•ì†Œ ì‹¤í–‰ ì¤‘...")
            
            # UMAP ë¦¬ë“€ì„œ ìƒì„±
            self.umap_reducer = umap.UMAP(
                n_components=n_components,
                n_neighbors=n_neighbors,
                min_dist=min_dist,
                random_state=42,
                verbose=True
            )
            
            # ì°¨ì› ì¶•ì†Œ ì‹¤í–‰
            self.umap_embedding = self.umap_reducer.fit_transform(self.embeddings)
            
            console.print(f"âœ… UMAP ì™„ë£Œ: {self.embeddings.shape[1]}D â†’ {n_components}D")
            console.print(f"   - íŒŒë¼ë¯¸í„°: n_neighbors={n_neighbors}, min_dist={min_dist}")
            
            return True
            
        except Exception as e:
            console.print(f"âŒ UMAP ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False
    
    def run_hdbscan_clustering(self, min_cluster_size: int = 5, min_samples: int = 3) -> bool:
        """HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"""
        try:
            console.print("ğŸ”„ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì¤‘...")
            
            # HDBSCAN í´ëŸ¬ìŠ¤í„°ëŸ¬ ìƒì„±
            self.hdbscan_clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                metric='euclidean'
            )
            
            # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
            self.cluster_labels = self.hdbscan_clusterer.fit_predict(self.umap_embedding)
            
            # í´ëŸ¬ìŠ¤í„° í†µê³„
            unique_labels = np.unique(self.cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            n_noise = list(self.cluster_labels).count(-1)
            
            console.print(f"âœ… HDBSCAN ì™„ë£Œ:")
            console.print(f"   - í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}ê°œ")
            console.print(f"   - ë…¸ì´ì¦ˆ ê¸°ì‚¬: {n_noise}ê°œ")
            console.print(f"   - íŒŒë¼ë¯¸í„°: min_cluster_size={min_cluster_size}, min_samples={min_samples}")
            
            return True
            
        except Exception as e:
            console.print(f"âŒ HDBSCAN ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False
    
    def evaluate_clusters(self) -> Dict[str, float]:
        """í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€"""
        try:
            console.print("ğŸ“Š í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€ ì¤‘...")
            
            # ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ ë°ì´í„°ë§Œ ì‚¬ìš©
            valid_mask = self.cluster_labels != -1
            if np.sum(valid_mask) < 2:
                console.print("âŒ ìœ íš¨í•œ í´ëŸ¬ìŠ¤í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return {}
            
            # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
            silhouette_avg = silhouette_score(
                self.umap_embedding[valid_mask], 
                self.cluster_labels[valid_mask]
            )
            
            # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
            cluster_stats = {}
            for label in np.unique(self.cluster_labels):
                if label == -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                    continue
                cluster_mask = self.cluster_labels == label
                cluster_size = np.sum(cluster_mask)
                cluster_stats[f'cluster_{label}'] = {
                    'size': cluster_size,
                    'percentage': cluster_size / len(self.cluster_labels) * 100
                }
            
            self.cluster_stats = {
                'silhouette_score': silhouette_avg,
                'n_clusters': len(np.unique(self.cluster_labels)) - 1,
                'n_noise': np.sum(self.cluster_labels == -1),
                'cluster_details': cluster_stats
            }
            
            console.print(f"âœ… í‰ê°€ ì™„ë£Œ:")
            console.print(f"   - ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.3f}")
            console.print(f"   - í´ëŸ¬ìŠ¤í„° ìˆ˜: {self.cluster_stats['n_clusters']}ê°œ")
            console.print(f"   - ë…¸ì´ì¦ˆ ë¹„ìœ¨: {self.cluster_stats['n_noise']/len(self.cluster_labels)*100:.1f}%")
            
            return self.cluster_stats
            
        except Exception as e:
            console.print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {}
    
    def visualize_clusters(self, save_path: str = None) -> bool:
        """í´ëŸ¬ìŠ¤í„° ì‹œê°í™”"""
        try:
            console.print("ğŸ¨ í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ìƒì„± ì¤‘...")
            
            # ë°ì´í„° ì¤€ë¹„
            df_viz = pd.DataFrame({
                'x': self.umap_embedding[:, 0],
                'y': self.umap_embedding[:, 1],
                'cluster': self.cluster_labels,
                'title': self.articles_data['title_cleaned'].values
            })
            
            # matplotlib ì‹œê°í™”
            plt.figure(figsize=(12, 8))
            scatter = plt.scatter(df_viz['x'], df_viz['y'], 
                               c=df_viz['cluster'], 
                               cmap='tab20', 
                               alpha=0.7,
                               s=50)
            plt.colorbar(scatter)
            plt.title('Political News Clusters (UMAP + HDBSCAN)')
            plt.xlabel('UMAP Dimension 1')
            plt.ylabel('UMAP Dimension 2')
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                console.print(f"âœ… ì‹œê°í™” ì €ì¥: {save_path}")
            
            plt.show()
            
            return True
            
        except Exception as e:
            console.print(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def analyze_clusters(self) -> List[Dict[str, Any]]:
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„ ë° ì •ë³´ ì¶”ì¶œ"""
        try:
            console.print("ğŸ” í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")
            
            clusters_info = []
            
            for label in np.unique(self.cluster_labels):
                if label == -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                    continue
                
                cluster_mask = self.cluster_labels == label
                cluster_articles = self.articles_data[cluster_mask]
                
                # í´ëŸ¬ìŠ¤í„° ì •ë³´ ìˆ˜ì§‘
                cluster_info = {
                    'cluster_id': int(label),
                    'size': int(np.sum(cluster_mask)),
                    'articles': cluster_articles.to_dict('records'),
                    'representative_article': self._find_representative_article(cluster_articles),
                    'keywords': self._extract_keywords(cluster_articles)
                }
                
                clusters_info.append(cluster_info)
            
            self.clusters_info = clusters_info
            
            console.print(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(clusters_info)}ê°œ í´ëŸ¬ìŠ¤í„°")
            
            return clusters_info
            
        except Exception as e:
            console.print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _find_representative_article(self, cluster_articles: pd.DataFrame) -> Dict[str, Any]:
        """ëŒ€í‘œ ê¸°ì‚¬ ì°¾ê¸° (í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ê¸°ì‚¬)"""
        # ê°„ë‹¨íˆ ì²« ë²ˆì§¸ ê¸°ì‚¬ë¥¼ ëŒ€í‘œë¡œ ì„ íƒ (ë‚˜ì¤‘ì— ê°œì„  ê°€ëŠ¥)
        if len(cluster_articles) > 0:
            return cluster_articles.iloc[0].to_dict()
        return {}
    
    def _extract_keywords(self, cluster_articles: pd.DataFrame) -> List[str]:
        """í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        # ì œëª©ì—ì„œ ê³µí†µ ë‹¨ì–´ ì¶”ì¶œ (ë‚˜ì¤‘ì— TF-IDFë¡œ ê°œì„ )
        titles = cluster_articles['title_cleaned'].dropna().tolist()
        if titles:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• í•„ìš”)
            all_words = ' '.join(titles).split()
            word_freq = pd.Series(all_words).value_counts()
            return word_freq.head(5).index.tolist()
        return []
    
    def save_to_database(self) -> bool:
        """í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            console.print("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
            
            # issues í…Œì´ë¸”ì— í´ëŸ¬ìŠ¤í„° ì €ì¥
            for cluster_info in self.clusters_info:
                issue_data = {
                    'title': f"í´ëŸ¬ìŠ¤í„° {cluster_info['cluster_id']}",
                    'subtitle': f"{cluster_info['size']}ê°œ ê¸°ì‚¬",
                    'summary': self._generate_cluster_summary(cluster_info),
                    'left_view': "ë³´ìˆ˜ ê´€ì  ë¶„ì„ í•„ìš”",
                    'center_view': "ì¤‘ë¦½ ê´€ì  ë¶„ì„ í•„ìš”", 
                    'right_view': "ì§„ë³´ ê´€ì  ë¶„ì„ í•„ìš”",
                    'source': "AI í´ëŸ¬ìŠ¤í„°ë§",
                    'date': datetime.now().date()
                }
                
                # issue ì €ì¥
                issue_result = self.supabase.client.table('issues').insert(issue_data).execute()
                if not issue_result.data:
                    console.print(f"âŒ ì´ìŠˆ ì €ì¥ ì‹¤íŒ¨: í´ëŸ¬ìŠ¤í„° {cluster_info['cluster_id']}")
                    continue
                
                issue_id = issue_result.data[0]['id']
                
                # issue_articles í…Œì´ë¸”ì— ë§¤í•‘ ì €ì¥
                for article in cluster_info['articles']:
                    article_mapping = {
                        'issue_id': issue_id,
                        'article_id': article['id'],
                        'stance': 'center'  # ê¸°ë³¸ê°’, ë‚˜ì¤‘ì— ê°œì„ 
                    }
                    
                    self.supabase.client.table('issue_articles').insert(article_mapping).execute()
            
            console.print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
            return True
            
        except Exception as e:
            console.print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _generate_cluster_summary(self, cluster_info: Dict[str, Any]) -> str:
        """í´ëŸ¬ìŠ¤í„° ìš”ì•½ ìƒì„±"""
        keywords = ', '.join(cluster_info['keywords'][:3])
        return f"ì´ í´ëŸ¬ìŠ¤í„°ëŠ” {cluster_info['size']}ê°œì˜ ê¸°ì‚¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì£¼ìš” í‚¤ì›Œë“œëŠ” {keywords}ì…ë‹ˆë‹¤."
    
    def run_full_pipeline(self) -> bool:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            console.print(Panel.fit(
                "[bold blue]ğŸš€ UMAP + HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘[/bold blue]",
                title="í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"
            ))
            
            # 1. ë°ì´í„° ë¡œë“œ
            if not self.load_embeddings_data():
                return False
            
            # 2. UMAP ì°¨ì› ì¶•ì†Œ
            if not self.run_umap_reduction():
                return False
            
            # 3. HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
            if not self.run_hdbscan_clustering():
                return False
            
            # 4. í’ˆì§ˆ í‰ê°€
            self.evaluate_clusters()
            
            # 5. ì‹œê°í™”
            self.visualize_clusters('clustering_results.png')
            
            # 6. í´ëŸ¬ìŠ¤í„° ë¶„ì„
            self.analyze_clusters()
            
            # 7. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            self.save_to_database()
            
            console.print(Panel.fit(
                "[bold green]âœ… í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ![/bold green]",
                title="ì™„ë£Œ"
            ))
            
            return True
            
        except Exception as e:
            console.print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    clusterer = PoliticalNewsClusterer()
    clusterer.run_full_pipeline()

if __name__ == "__main__":
    main()
