#!/usr/bin/env python3
"""
HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤í¬ë¦½íŠ¸
- articles_embeddings í…Œì´ë¸”ì˜ ë²¡í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
- issues í…Œì´ë¸”ì— ì´ìŠˆ ì €ì¥
- issue_articles í…Œì´ë¸”ì— ì—°ê²° ì €ì¥
"""

import sys
import os
import json
import numpy as np
from datetime import datetime, date
from typing import List, Dict, Any, Optional
from collections import Counter

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from utils.supabase_manager import SupabaseManager

# HDBSCAN ì„¤ì¹˜ í™•ì¸ ë° import
try:
    import hdbscan
except ImportError:
    print("âŒ HDBSCANì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹: pip install hdbscan")
    sys.exit(1)

class HDBSCANClusterer:
    """HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ì„¤ì • (í•˜ë“œì½”ë”©) - ë” í° í´ëŸ¬ìŠ¤í„°ë¥¼ ìœ„í•´ ì™„í™”
        self.MIN_CLUSTER_SIZE = 8        # ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (3 â†’ 8)
        self.MIN_SAMPLES = 4             # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (2 â†’ 4)
        self.CLUSTER_SELECTION_EPSILON = 0.3  # í´ëŸ¬ìŠ¤í„° ì„ íƒ ì„ê³„ê°’ (0.1 â†’ 0.3)
        self.METRIC = 'euclidean'        # ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ì •ê·œí™”ëœ ë²¡í„°ì—ì„œ ì½”ì‚¬ì¸ê³¼ ìœ ì‚¬)
        
        # Supabase ì—°ê²°
        self.supabase_manager = SupabaseManager()
        if not self.supabase_manager.client:
            raise Exception("Supabase ì—°ê²° ì‹¤íŒ¨")
    
    def fetch_embeddings_data(self) -> tuple:
        """
        ì„ë² ë”© ë°ì´í„° ì¡°íšŒ
        
        Returns:
            tuple: (embeddings, article_ids, article_metadata)
        """
        try:
            print("ğŸ“¡ ì„ë² ë”© ë°ì´í„° ì¡°íšŒ ì¤‘...")
            
            # articles_embeddingsì™€ articles_cleaned ì¡°ì¸í•˜ì—¬ ë°ì´í„° ì¡°íšŒ
            result = self.supabase_manager.client.table('articles_embeddings').select(
                'id, cleaned_article_id, article_id, media_id, embedding_vector, model_name'
            ).execute()
            
            if not result.data:
                print("âŒ ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None
            
            print(f"âœ… {len(result.data)}ê°œ ì„ë² ë”© ë°ì´í„° ì¡°íšŒ ì™„ë£Œ")
            
            # ë²¡í„° íŒŒì‹± ë° ì •ë¦¬
            embeddings = []
            article_ids = []
            article_metadata = []
            
            for item in result.data:
                try:
                    # JSON ë¬¸ìì—´ì„ íŒŒì‹±
                    vector = json.loads(item['embedding_vector'])
                    
                    if len(vector) == 1536:  # text-embedding-3-small ì°¨ì› í™•ì¸
                        embeddings.append(vector)
                        article_ids.append(item['article_id'])
                        article_metadata.append({
                            'id': item['id'],
                            'cleaned_article_id': item['cleaned_article_id'],
                            'article_id': item['article_id'],
                            'media_id': item['media_id']
                        })
                    else:
                        print(f"âš ï¸ ì˜ëª»ëœ ë²¡í„° ì°¨ì›: {len(vector)}ì°¨ì› (ê¸°ì‚¬ ID: {item['article_id']})")
                        
                except Exception as e:
                    print(f"âš ï¸ ë²¡í„° íŒŒì‹± ì‹¤íŒ¨ (ê¸°ì‚¬ ID: {item['article_id']}): {str(e)}")
                    continue
            
            print(f"âœ… {len(embeddings)}ê°œ ë²¡í„° íŒŒì‹± ì™„ë£Œ")
            
            # ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ íš¨ê³¼)
            embeddings_array = np.array(embeddings)
            norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)
            normalized_embeddings = embeddings_array / norms
            
            return normalized_embeddings, article_ids, article_metadata
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None, None, None
    
    def perform_clustering(self, embeddings: np.ndarray) -> tuple:
        """
        HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            embeddings: ì„ë² ë”© ë²¡í„° ë°°ì—´
            
        Returns:
            tuple: (cluster_labels, clusterer)
        """
        try:
            print("ğŸ”„ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
            
            # HDBSCAN í´ëŸ¬ìŠ¤í„°ëŸ¬ ì´ˆê¸°í™”
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=self.MIN_CLUSTER_SIZE,
                min_samples=self.MIN_SAMPLES,
                cluster_selection_epsilon=self.CLUSTER_SELECTION_EPSILON,
                metric=self.METRIC
            )
            
            # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            cluster_labels = clusterer.fit_predict(embeddings)
            
            # ê²°ê³¼ ë¶„ì„
            unique_labels = np.unique(cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            
            print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ:")
            print(f"  - í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}ê°œ")
            print(f"  - ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {n_noise}ê°œ")
            print(f"  - í´ëŸ¬ìŠ¤í„°ë§ ë¹„ìœ¨: {((len(cluster_labels) - n_noise) / len(cluster_labels) * 100):.1f}%")
            
            return cluster_labels, clusterer
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return None, None
    
    def clear_existing_data(self) -> bool:
        """
        ê¸°ì¡´ ì´ìŠˆ ë°ì´í„° ì´ˆê¸°í™”
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("ğŸ—‘ï¸ ê¸°ì¡´ ì´ìŠˆ ë°ì´í„° ì´ˆê¸°í™” ì¤‘...")
            
            # issue_articles í…Œì´ë¸” ì´ˆê¸°í™”
            self.supabase_manager.client.table('issue_articles').delete().neq('issue_id', '00000000-0000-0000-0000-000000000000').execute()
            
            # issues í…Œì´ë¸” ì´ˆê¸°í™”
            self.supabase_manager.client.table('issues').delete().neq('id', '00000000-0000-0000-0000-000000000000').execute()
            
            print("âœ… ê¸°ì¡´ ì´ìŠˆ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def analyze_media_bias(self, article_metadata: List[Dict]) -> Dict[str, str]:
        """
        ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°ì—ì„œ ì„±í–¥ë³„ ì–¸ë¡ ì‚¬ ì •ë³´ ë¶„ì„
        
        Args:
            article_metadata: ê¸°ì‚¬ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ì„±í–¥ë³„ ì–¸ë¡ ì‚¬ ì •ë³´
        """
        try:
            # ì–¸ë¡ ì‚¬ IDë³„ ì„±í–¥ ë§¤í•‘ (í•˜ë“œì½”ë”©)
            media_bias_map = {
                '755f5a31-507f-42d7-aa8c-587a9459896c': 'right',  # ì¡°ì„ ì¼ë³´
                '629d6050-76e2-466a-ae66-f2532d1f359c': 'right',  # ì¤‘ì•™ì¼ë³´
                'afcd9fc8-e4fd-44c7-8d9d-59722bb21b26': 'right',  # ë™ì•„ì¼ë³´
                'ea42a075-88e0-4c6e-a21b-8854ec10dec9': 'left',   # í•œê²¨ë ˆ
                '81324c3e-5f68-4356-bc91-bd5c7719f5c9': 'left',   # ì˜¤ë§ˆì´ë‰´ìŠ¤
                '3847c39d-bc90-44e5-8650-331e67cbe140': 'left',   # ê²½í–¥ì‹ ë¬¸
                '33e32516-abb5-46ca-b87d-306304a61c34': 'center', # ì—°í•©ë‰´ìŠ¤
                'a29afc6a-1764-43fe-8b0e-3a5962d90402': 'center', # ë‰´ì‹œìŠ¤
                'a8fecf98-41ac-4018-85e8-19bfeb702fe5': 'center'  # ë‰´ìŠ¤ì›
            }
            
            # ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„
            media_counts = {}
            for article in article_metadata:
                media_id = article['media_id']
                if media_id in media_bias_map:
                    bias = media_bias_map[media_id]
                    if bias not in media_counts:
                        media_counts[bias] = 0
                    media_counts[bias] += 1
            
            # ì„±í–¥ë³„ ì–¸ë¡ ì‚¬ ì •ë³´ ìƒì„±
            left_count = media_counts.get('left', 0)
            center_count = media_counts.get('center', 0)
            right_count = media_counts.get('right', 0)
            total_count = left_count + center_count + right_count
            
            return {
                'total_source': str(total_count),
                'left_source': str(left_count),
                'center_source': str(center_count),
                'right_source': str(right_count)
            }
            
        except Exception as e:
            print(f"âš ï¸ ì„±í–¥ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                'total_source': '0',
                'left_source': '0',
                'center_source': '0',
                'right_source': '0'
            }
    
    def create_issue(self, cluster_id: int, article_ids: List[str], article_metadata: List[Dict]) -> Optional[str]:
        """
        ì´ìŠˆ ìƒì„± ë° ì €ì¥
        
        Args:
            cluster_id: í´ëŸ¬ìŠ¤í„° ID
            article_ids: í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ì‚¬ IDë“¤
            article_metadata: ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°
            
        Returns:
            str: ìƒì„±ëœ ì´ìŠˆ ID ë˜ëŠ” None
        """
        try:
            # ì„±í–¥ë³„ ì–¸ë¡ ì‚¬ ì •ë³´ ë¶„ì„
            source_info = self.analyze_media_bias(article_metadata)
            
            # ê¸°ë³¸ ì´ìŠˆ ë°ì´í„° ìƒì„±
            issue_data = {
                'date': date.today().isoformat(),
                'title': f'ì´ìŠˆ {cluster_id + 1}',
                'summary': f'{len(article_ids)}ê°œ ê¸°ì‚¬ë¡œ êµ¬ì„±ëœ ì´ìŠˆ',
                'subtitle': f'í´ëŸ¬ìŠ¤í„° {cluster_id + 1}',
                'importance': 'medium',
                'source': source_info['total_source'],
                'left_source': source_info['left_source'],
                'center_source': source_info['center_source'],
                'right_source': source_info['right_source'],
                'created_at': datetime.now().isoformat()
            }
            
            # ì´ìŠˆ ì €ì¥
            result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
            
            if result.data:
                issue_id = result.data[0]['id']
                print(f"âœ… ì´ìŠˆ {cluster_id + 1} ìƒì„± ì™„ë£Œ (ID: {issue_id})")
                return issue_id
            else:
                print(f"âŒ ì´ìŠˆ {cluster_id + 1} ìƒì„± ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            print(f"âŒ ì´ìŠˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None
    
    def create_issue_articles(self, issue_id: str, article_ids: List[str], article_metadata: List[Dict]) -> bool:
        """
        ì´ìŠˆ-ê¸°ì‚¬ ì—°ê²° ìƒì„±
        
        Args:
            issue_id: ì´ìŠˆ ID
            article_ids: ê¸°ì‚¬ IDë“¤
            article_metadata: ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°
            
        Returns:
            bool: ìƒì„± ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì—°ê²° ë°ì´í„° ìƒì„±
            connections = []
            for i, article_id in enumerate(article_ids):
                # í•´ë‹¹ article_idì˜ cleaned_article_id ì°¾ê¸°
                cleaned_article_id = None
                for metadata in article_metadata:
                    if metadata['article_id'] == article_id:
                        cleaned_article_id = metadata['cleaned_article_id']
                        break
                
                connections.append({
                    'issue_id': issue_id,
                    'article_id': article_id,
                    'cleaned_article_id': cleaned_article_id,
                    'stance': 'center'  # ê¸°ë³¸ê°’
                })
            
            # ì¼ê´„ ì €ì¥
            result = self.supabase_manager.client.table('issue_articles').insert(connections).execute()
            
            if result.data:
                print(f"âœ… {len(connections)}ê°œ ì—°ê²° ìƒì„± ì™„ë£Œ")
                return True
            else:
                print(f"âŒ ì—°ê²° ìƒì„± ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì—°ê²° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def run_clustering(self) -> bool:
        """
        í´ëŸ¬ìŠ¤í„°ë§ ë©”ì¸ í”„ë¡œì„¸ìŠ¤
        
        Returns:
            bool: ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("ğŸš€ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
            
            # 1. ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™”
            if not self.clear_existing_data():
                return False
            
            # 2. ì„ë² ë”© ë°ì´í„° ì¡°íšŒ
            embeddings, article_ids, article_metadata = self.fetch_embeddings_data()
            if embeddings is None:
                return False
            
            # 3. í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            cluster_labels, clusterer = self.perform_clustering(embeddings)
            if cluster_labels is None:
                return False
            
            # 4. í´ëŸ¬ìŠ¤í„°ë³„ ì´ìŠˆ ìƒì„±
            unique_labels = np.unique(cluster_labels)
            created_issues = 0
            failed_issues = 0
            
            for label in unique_labels:
                if label == -1:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ê±´ë„ˆë›°ê¸°
                    continue
                
                # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ì‚¬ë“¤ ì°¾ê¸°
                cluster_mask = cluster_labels == label
                cluster_article_ids = [article_ids[i] for i in range(len(article_ids)) if cluster_mask[i]]
                cluster_metadata = [article_metadata[i] for i in range(len(article_metadata)) if cluster_mask[i]]
                
                # ì´ìŠˆ ìƒì„±
                issue_id = self.create_issue(label, cluster_article_ids, cluster_metadata)
                
                if issue_id:
                    # ì´ìŠˆ-ê¸°ì‚¬ ì—°ê²° ìƒì„±
                    if self.create_issue_articles(issue_id, cluster_article_ids, cluster_metadata):
                        created_issues += 1
                    else:
                        failed_issues += 1
                else:
                    failed_issues += 1
            
            print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
            print(f"  - ìƒì„±ëœ ì´ìŠˆ: {created_issues}ê°œ")
            print(f"  - ì‹¤íŒ¨í•œ ì´ìŠˆ: {failed_issues}ê°œ")
            print(f"  - ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {list(cluster_labels).count(-1)}ê°œ")
            
            return created_issues > 0
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {str(e)}")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ”® HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    
    try:
        # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        clusterer = HDBSCANClusterer()
        success = clusterer.run_clustering()
        
        if success:
            print("\nâœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        else:
            print("\nâŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
